{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import gc\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import warnings\n",
    "from math import sqrt\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import ml_insights as mli\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import joblib\n",
    "import h5py\n",
    "# Data processing\n",
    "import pandas as pd\n",
    "import neurokit2 as nk\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Machine learning\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()  \n",
    "\n",
    "from sklearn.metrics import (\n",
    "    average_precision_score, roc_curve, \n",
    "    roc_auc_score, accuracy_score, f1_score\n",
    ")\n",
    "from sklearn.model_selection import GroupKFold, train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.calibration import IsotonicRegression\n",
    "from sklearn.metrics import brier_score_loss\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import confusion_matrix,roc_curve, auc\n",
    "from scipy.stats import chi2\n",
    "from scipy.signal import resample\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# ML\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Visualization\n",
    "import shap\n",
    "import optuna\n",
    "from BorutaShap import BorutaShap\n",
    "# Custom imports\n",
    "from model_code import *\n",
    "from model.blocks import FinalModel\n",
    "from team_code import *\n",
    "from helper_code import *\n",
    "from plot_model import *\n",
    "from delong import *\n",
    "\n",
    "SEED = 1\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "shap.initjs()\n",
    "# ==================================================\n",
    "N_TRIAL                 = 50\n",
    "N_FOLD                  = 5\n",
    "N_EPOCH                 = 100\n",
    "\n",
    "# Train hyperparams\n",
    "BATCH_SIZE_LIST_GLOBAL  = [8,16]\n",
    "\n",
    "LR_INITIAL_LO           = 5e-4\n",
    "LR_INITIAL_HI           = 5e-3\n",
    "\n",
    "LR_STEP_SIZE_LO         = 2\n",
    "LR_STEP_SIZE_HI         = 3\n",
    "LR_STEP_GAMMA_LO        = 0.05\n",
    "LR_STEP_GAMMA_HI        = 0.3\n",
    "\n",
    "EARLY_STOP_PATIENCE_LO  = 3\n",
    "EARLY_STOP_PATIENCE_HI  = 4\n",
    "\n",
    "# Architecture hyperparams\n",
    "BLOCK_SIZE_GLOBAL = [12,16,24]\n",
    "BLOCK_DEPTH_GLOBAL = [2,3]\n",
    "BLOCK_LAYERS_GLOBAL = [3,4]\n",
    "HIDDEN_SIZE_GLOBAL = [32,64,128]\n",
    "KERNEL_NUM_GLOBAL = [5,7,9]\n",
    "# ==================================================\n",
    "# Modify CustomCNN to take dropout_rate as an argument\n",
    "def collate_fn(batch):\n",
    "    inputs = torch.stack([torch.tensor(i[0], dtype=torch.float32) for i in batch])\n",
    "    targets = torch.stack([torch.tensor(i[1], dtype=torch.float32) for i in batch])\n",
    "    age = torch.tensor([[i[3]] for i in batch], dtype=torch.float32)\n",
    "    gender = torch.tensor([[i[4]] for i in batch], dtype=torch.float32)\n",
    "\n",
    "    return inputs, targets, age, gender\n",
    "\n",
    "## Youden index\n",
    "def youden(y_true, y_score):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
    "    idx = np.argmax(tpr - fpr)\n",
    "    return thresholds[idx]\n",
    "\n",
    "# Function to train the model (speed optimized, no prefetch, batch-by-batch)\n",
    "def train_model(train_loader, model, criterion, optimizer, scheduler, device=DEVICE):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    n_batch = len(train_loader)\n",
    "    pbar = tqdm(train_loader, total=n_batch, leave=False)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    for batch in pbar:\n",
    "        # Unpack and move to device efficiently\n",
    "        inputs, labels, age, gender = batch\n",
    "        inputs = inputs.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "        age = age.to(device, non_blocking=True)\n",
    "        gender = gender.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)  # set_to_none is faster than zeroing\n",
    "        outputs = model(inputs, age, gender)\n",
    "        labels_ = labels[:, 1].unsqueeze(1)\n",
    "        loss = criterion(outputs, labels_)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        # Explicitly delete to free memory\n",
    "        del inputs, labels, age, gender, outputs, loss, labels_\n",
    "    torch.backends.cudnn.benchmark = False  # Reset if needed\n",
    "    del pbar  # Remove after use\n",
    "    return running_loss / n_batch\n",
    "\n",
    "def evaluate_model(valid_loader, model, device=DEVICE):\n",
    "    model.eval()\n",
    "    model.return_hidden = True  # Enable hidden state return\n",
    "    y_true, y_pred, hidden_states = None, None, None\n",
    "    n_batch = len(valid_loader)\n",
    "    pbar = tqdm(enumerate(valid_loader), total=n_batch, leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i_batch, (X_batch, y_true_batch, age_batch, gender_batch) in pbar:\n",
    "            X_batch, y_true_batch = X_batch.to(device), y_true_batch.to(device)\n",
    "            age_batch, gender_batch = age_batch.to(device), gender_batch.to(device)\n",
    "            y_true_batch = y_true_batch[:, 1].unsqueeze(1)\n",
    "            \n",
    "            y_pred_batch, hidden_batch = model(X_batch, age_batch, gender_batch)\n",
    "            y_pred_batch = F.sigmoid(y_pred_batch)\n",
    "            \n",
    "            # Convert to numpy arrays\n",
    "            y_pred_batch = y_pred_batch.cpu().numpy().reshape((-1, 1))\n",
    "            y_true_batch = y_true_batch.cpu().numpy().reshape((-1, 1))\n",
    "            hidden_batch = hidden_batch.cpu().numpy()\n",
    "\n",
    "            # Concatenate results\n",
    "            if y_pred is None:\n",
    "                y_pred = y_pred_batch\n",
    "                y_true = y_true_batch\n",
    "                hidden_states = hidden_batch\n",
    "            else:\n",
    "                y_pred = np.r_[y_pred, y_pred_batch]\n",
    "                y_true = np.r_[y_true, y_true_batch]\n",
    "                hidden_states = np.r_[hidden_states, hidden_batch]\n",
    "            \n",
    "            # Remove variables after use\n",
    "            del X_batch, y_true_batch, age_batch, gender_batch, y_pred_batch, hidden_batch\n",
    "\n",
    "            pbar.set_description(f'Evaluating ... {1 + i_batch}/{n_batch}')\n",
    "    \n",
    "    model.return_hidden = False  # Reset hidden state return\n",
    "    del pbar  # Remove after use\n",
    "    return y_true, y_pred, hidden_states\n",
    "\n",
    "def loguniform(low, high, size=None):\n",
    "    return np.exp(np.random.uniform(np.log(low), np.log(high), size))\n",
    "\n",
    "def calc_unreliability(y_true, y_prob, g=10):\n",
    "    \"\"\"\n",
    "    Calculates a Hosmer-Lemeshow type statistic and p-value, \n",
    "    with some defensive checks against empty bins or p=0/1.\n",
    "    \"\"\"\n",
    "    # Clip probabilities away from exact 0 or 1\n",
    "    y_prob = np.clip(y_prob, 1e-12, 1 - 1e-12)\n",
    "    \n",
    "    # Put data in a DataFrame so we can bin easily\n",
    "    df = pd.DataFrame({'y': y_true, 'p': y_prob})\n",
    "    df['bin'] = pd.qcut(df['p'], q=g, duplicates='drop')  # drop duplicate edges\n",
    "    \n",
    "    # Group by bin to compute sums and means\n",
    "    bin_data = df.groupby('bin', dropna=True).agg({'y': ['sum','count'], 'p': 'mean'})\n",
    "    bin_data.columns = ['y_sum', 'y_count', 'p_mean']\n",
    "    \n",
    "    # Drop any rows that might be empty or cause invalid calculations\n",
    "    bin_data = bin_data[bin_data['y_count'] > 0]\n",
    "    \n",
    "    # Observed vs. expected\n",
    "    O = bin_data['y_sum']         # observed events\n",
    "    N = bin_data['y_count']       # bin size\n",
    "    P = bin_data['p_mean']        # avg predicted p in bin\n",
    "    E = N * P                     # expected # events\n",
    "    \n",
    "    # HL statistic\n",
    "    hl_stat = np.sum((O - E)**2 / (N * P * (1 - P)))\n",
    "    \n",
    "    # Degrees of freedom: (number_of_nonempty_bins - 2)\n",
    "    dof = len(bin_data) - 2\n",
    "    if dof > 0:\n",
    "        p_value = 1 - chi2.cdf(hl_stat, dof)\n",
    "    else:\n",
    "        # If dof <= 0, cannot calculate a valid p-value\n",
    "        p_value = np.nan\n",
    "    \n",
    "    return hl_stat, p_value\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "def create_objective_function(model_type, model_class, param_grid_fn, features, model_name):\n",
    "    def objective(trial, df_train, cv=N_FOLD):\n",
    "        param_grid = param_grid_fn(trial)\n",
    "        h = df_train['hid'].unique()\n",
    "        groups = np.array(range(len(h)))\n",
    "        h_shuffled, groups_shuffled = shuffle(h, groups, random_state=SEED)\n",
    "        group_kf = GroupKFold(n_splits=cv)\n",
    "        cv_scores = np.empty(cv)\n",
    "        for idx, (train_h, val_h) in enumerate(group_kf.split(h_shuffled, groups=groups_shuffled)):\n",
    "            X_train = df_train[df_train['hid'].isin(h_shuffled[train_h])][features]\n",
    "            X_valid = df_train[df_train['hid'].isin(h_shuffled[val_h])][features]\n",
    "            y_train = df_train.loc[df_train['hid'].isin(h_shuffled[train_h]), 'label'].values\n",
    "            y_valid = df_train.loc[df_train['hid'].isin(h_shuffled[val_h]), 'label'].values\n",
    "            \n",
    "            model = model_class(**param_grid)\n",
    "            if model_type == 'xgb':\n",
    "                model.fit(X_train, y_train, verbose=False)\n",
    "            else:\n",
    "                model.fit(X_train, y_train)   \n",
    "            cv_scores[idx] = roc_auc_score(y_valid, model.predict_proba(X_valid)[:, 1])\n",
    "        mean_cv_score = np.mean(cv_scores)      \n",
    "        try:\n",
    "            if mean_cv_score > trial.study.best_value: \n",
    "                with open(f'best_params_{model_type}_{model_name}.json', 'w') as f:\n",
    "                    json.dump(param_grid, f, indent=4)\n",
    "        except ValueError:\n",
    "            with open(f'best_params_{model_type}_{model_name}.json', 'w') as f:\n",
    "                json.dump(param_grid, f, indent=4)\n",
    "        return mean_cv_score\n",
    "    return objective\n",
    "\n",
    "def xgb_params(trial):\n",
    "    return {\n",
    "        'objective': 'binary:logistic',\n",
    "        'tree_method': 'gpu_hist',\n",
    "        'predictor': 'gpu_predictor',\n",
    "        'tweedie_variance_power': trial.suggest_discrete_uniform('tweedie_variance_power', 1.0, 2.0, 0.1),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 8),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 200, 1600, 200),\n",
    "        'eta': trial.suggest_float('eta', 0.005, 0.05),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 0.9),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 0.9),\n",
    "        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 0.9),\n",
    "        'min_child_weight': trial.suggest_loguniform('min_child_weight', 1, 1e3),\n",
    "        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1, 1e3),\n",
    "        'reg_alpha': trial.suggest_loguniform('reg_alpha', 0.1, 1e3),\n",
    "        'gamma': trial.suggest_loguniform('gamma', 0.01, 1e2),\n",
    "        'random_state': SEED,\n",
    "        'enable_categorical': True\n",
    "    }\n",
    "\n",
    "def rf_params(trial):\n",
    "    return {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000, 100),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 20),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 20),\n",
    "        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
    "        'bootstrap': trial.suggest_categorical('bootstrap', [True, False]),\n",
    "        'random_state': SEED\n",
    "    }\n",
    "\n",
    "def svm_params(trial):\n",
    "    return {\n",
    "        'C': trial.suggest_loguniform('C', 1e-3, 1e3),\n",
    "        'kernel': trial.suggest_categorical('kernel', ['linear', 'rbf', 'poly', 'sigmoid']),\n",
    "        'gamma': trial.suggest_categorical('gamma', ['scale', 'auto']),\n",
    "        'probability': True,\n",
    "        'random_state': SEED\n",
    "    }\n",
    "\n",
    "def logreg_params(trial):\n",
    "    return {\n",
    "        'C': trial.suggest_loguniform('C', 1e-4, 1e2),\n",
    "        'penalty': trial.suggest_categorical('penalty', ['l2', 'none']),\n",
    "        'solver': trial.suggest_categorical('solver', ['lbfgs', 'saga']),\n",
    "        'max_iter': 1000,\n",
    "        'random_state': SEED\n",
    "    }\n",
    "def compute_saliency_maps(input_data, input_age, input_gender, model):\n",
    "    input_tensor = torch.FloatTensor(input_data).to(DEVICE)\n",
    "    input_age_tensor = torch.FloatTensor(input_age).to(DEVICE)\n",
    "    input_gender_tensor = torch.FloatTensor(input_gender).to(DEVICE)\n",
    "    input_tensor.requires_grad_()\n",
    "    # Forward pass\n",
    "    model.eval()\n",
    "    output = model(input_tensor, input_age_tensor, input_gender_tensor)\n",
    "    # Compute gradients with respect to input\n",
    "    output.backward(torch.ones_like(output))\n",
    "    # Get gradients\n",
    "    saliency = input_tensor.grad.abs().cpu().numpy()\n",
    "    return saliency\n",
    "\n",
    "def extract_date(filename):\n",
    "    parts = filename.split('_')\n",
    "    # Case 1: ..._YYYYMMDD.xml\n",
    "    if len(parts) >= 3 and parts[-1].endswith('.xml') and len(parts[-1].replace('.xml','')) == 8:\n",
    "        date_str = parts[-1].replace('.xml','')\n",
    "        return f\"{date_str[:4]}/{date_str[4:6]}/{date_str[6:8]}\"\n",
    "    # Case 2: ..._YYYY_MM_DD_...\n",
    "    elif len(parts) >= 4 and parts[1].isdigit() and parts[2].isdigit() and parts[3].isdigit():\n",
    "        # e.g. 74079672_2010_05_11_07_20_37.xml\n",
    "        return f\"{parts[1]}/{parts[2]}/{parts[3]}\"\n",
    "    # Fallback: try to find 8-digit date in any part\n",
    "    for p in parts:\n",
    "        if len(p) == 8 and p.isdigit():\n",
    "            return f\"{p[:4]}/{p[4:6]}/{p[6:8]}\"\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_labels_M_N.csv')\n",
    "\n",
    "# Randomly split into develop and test sets, ensuring no overlap in 'hid'\n",
    "unique_hids = df['hid'].unique()\n",
    "np.random.seed(SEED)\n",
    "np.random.shuffle(unique_hids)\n",
    "\n",
    "n_test_hids = int(np.ceil(len(unique_hids) * 0.10))\n",
    "test_hids = set(unique_hids[:n_test_hids])\n",
    "develop_hids = set(unique_hids[n_test_hids:])\n",
    "\n",
    "test_df = df[df['hid'].isin(test_hids)].copy()\n",
    "develop_df = df[df['hid'].isin(develop_hids)].copy()\n",
    "\n",
    "# Double-check no overlap in 'hid'\n",
    "assert len(set(test_df['hid']) & set(develop_df['hid'])) == 0, \"Overlap in 'hid' between test and develop sets!\"\n",
    "\n",
    "# Now, split develop set into 5 folds, grouped by 'hid'\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "folds = []\n",
    "develop_idx = develop_df.index.to_numpy()\n",
    "for fold_idx, (_, val_idx) in enumerate(gkf.split(develop_df, groups=develop_df['hid'])):\n",
    "    fold = develop_df.iloc[val_idx]\n",
    "    folds.append(fold)\n",
    "\n",
    "# Double-check no overlap of 'hid' between any fold and test set\n",
    "for i, fold in enumerate(folds):\n",
    "    overlap = set(fold['hid']) & test_hids\n",
    "    assert len(overlap) == 0, f\"Fold {i+1} and test set have overlapping hids: {overlap}\"\n",
    "\n",
    "# Save filenames for each fold and test set\n",
    "for i, fold in enumerate(folds):\n",
    "    np.save(f'fold_{i+1}_filenames.npy', fold['filename'].to_numpy())\n",
    "np.save('test_filenames.npy', test_df['filename'].to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(SEED)\n",
    "\n",
    "study_date = datetime.now().strftime('%y%m%d')\n",
    "study_folder_prefix = f'{study_date}'\n",
    "study_num = 0\n",
    "for folder in sorted(os.listdir('model'), reverse=True):\n",
    "    if folder.startswith(study_folder_prefix):\n",
    "        ch_idx = len(study_folder_prefix) + 1\n",
    "        study_num = int(folder[ch_idx:ch_idx + 2]) + 1\n",
    "        break\n",
    "STUDY_DIR = f'model/{study_folder_prefix}_{study_num:02d}'\n",
    "os.makedirs(STUDY_DIR, exist_ok=True)\n",
    "search_space_train = {\n",
    "    'BATCH_SIZE_LIST'       : BATCH_SIZE_LIST_GLOBAL,\n",
    "    'LR_INITIAL_LO'         : LR_INITIAL_LO,\n",
    "    'LR_INITIAL_HI'         : LR_INITIAL_HI,\n",
    "    'LR_STEP_SIZE_LO'       : LR_STEP_SIZE_LO,\n",
    "    'LR_STEP_SIZE_HI'       : LR_STEP_SIZE_HI,\n",
    "    'LR_STEP_GAMMA_LO'      : LR_STEP_GAMMA_LO,\n",
    "    'LR_STEP_GAMMA_HI'      : LR_STEP_GAMMA_HI,\n",
    "    'EARLY_STOP_PATIENCE_LO'  : EARLY_STOP_PATIENCE_LO,\n",
    "    'EARLY_STOP_PATIENCE_HI'  : EARLY_STOP_PATIENCE_HI\n",
    "}\n",
    "search_space_arch = {\n",
    "    'BLOCK_SIZE'          : BLOCK_SIZE_GLOBAL,\n",
    "    'BLOCK_DEPTH'        : BLOCK_DEPTH_GLOBAL,\n",
    "    'BLOCK_LAYERS'       : BLOCK_LAYERS_GLOBAL,\n",
    "    'HIDDEN_SIZE'        : HIDDEN_SIZE_GLOBAL,\n",
    "    'KERNEL_NUM'         : KERNEL_NUM_GLOBAL,  \n",
    "}\n",
    "SEARCH_SPACE_ARCH_FILENAME  = f'{STUDY_DIR}/search_space_arch.pkl'\n",
    "SEARCH_SPACE_TRAIN_FILENAME = f'{STUDY_DIR}/search_space_train.pkl'\n",
    "pickle.dump(search_space_arch, open(SEARCH_SPACE_ARCH_FILENAME, 'wb'))\n",
    "pickle.dump(search_space_train, open(SEARCH_SPACE_TRAIN_FILENAME, 'wb'))\n",
    "\n",
    "fold_filenames = [np.load(f'fold_{i}_filenames.npy', allow_pickle=True) for i in range(1, 6)]\n",
    "\n",
    "for i_trial in range(N_TRIAL):\n",
    "    search_space_arch = pickle.load(open(SEARCH_SPACE_ARCH_FILENAME, 'rb'))\n",
    "    search_space_train = pickle.load(open(SEARCH_SPACE_TRAIN_FILENAME, 'rb'))\n",
    "\n",
    "    batch_size          = np.random.choice(search_space_train['BATCH_SIZE_LIST'], size=1)[0]\n",
    "    lr_initial          = loguniform(low=search_space_train['LR_INITIAL_LO'], high=search_space_train['LR_INITIAL_HI'], size=1)[0]\n",
    "    lr_step_size        = np.random.randint(low=search_space_train['LR_STEP_SIZE_LO'], high=search_space_train['LR_STEP_SIZE_HI'], size=1)[0]\n",
    "    lr_step_gamma       = np.random.uniform(low=search_space_train['LR_STEP_GAMMA_LO'], high=search_space_train['LR_STEP_GAMMA_HI'], size=1)[0]\n",
    "    early_stop_pat      = np.random.randint(low=search_space_train['EARLY_STOP_PATIENCE_LO'], high=search_space_train['EARLY_STOP_PATIENCE_HI'], size=1)[0]\n",
    "\n",
    "    # Remove search_space_train after use\n",
    "    del search_space_train\n",
    "\n",
    "    block_size             = np.random.choice(search_space_arch['BLOCK_SIZE'], size=1)[0]\n",
    "    block_depth            = np.random.choice(search_space_arch['BLOCK_DEPTH'], size=1)[0]\n",
    "    block_layers           = np.random.choice(search_space_arch['BLOCK_LAYERS'], size=1)[0]\n",
    "    hidden_size            = np.random.choice(search_space_arch['HIDDEN_SIZE'], size=1)[0]\n",
    "    kernel_num             = np.random.choice(search_space_arch['KERNEL_NUM'], size=1)[0]\n",
    "\n",
    "    # Remove search_space_arch after use\n",
    "    del search_space_arch\n",
    "\n",
    "    trial_folder = ''\n",
    "    trial_folder += f'batch={batch_size}_'\n",
    "    trial_folder += f'lr={lr_initial:.5f}_step={lr_step_size}_gam={lr_step_gamma:.3f}_pat={early_stop_pat}_'\n",
    "    trial_folder += f'block_size={block_size}_block_depth={block_depth}_hidden_size={hidden_size}_block_layers={block_layers}_kernel_num={kernel_num}'\n",
    "    trial_dir = f'{STUDY_DIR}/{trial_folder}'\n",
    "    os.makedirs(trial_dir, exist_ok=True)\n",
    "    hparams_train = {\n",
    "        'N_FOLD'            : N_FOLD,\n",
    "        'N_EPOCH'           : N_EPOCH,\n",
    "        'BATCH_SIZE'        : batch_size,\n",
    "        'LR_INITIAL'        : lr_initial,\n",
    "        'LR_STEP_SIZE'      : lr_step_size,\n",
    "        'LR_STEP_GAMMA'     : lr_step_gamma,\n",
    "        'EARLY_STOP_PAT'    : early_stop_pat,\n",
    "    }\n",
    "    hparams_arch = {\n",
    "        'BLOCK_SIZE'        : block_size,\n",
    "        'BLOCK_DEPTH'       : block_depth,\n",
    "        'BLOCK_LAYERS'      : block_layers,\n",
    "        'HIDDEN_SIZE'       : hidden_size,\n",
    "        'KERNEL_NUM'        : kernel_num,\n",
    "    }\n",
    "    # Remove all single-use hparam variables after use\n",
    "    del batch_size, lr_initial, lr_step_size, lr_step_gamma, early_stop_pat\n",
    "    del block_size, block_depth, block_layers, hidden_size, kernel_num\n",
    "\n",
    "    print(f'RANDOM SEARCH TRIAL {1 + i_trial}/{N_TRIAL}')\n",
    "    HPARAMS_TRAIN_FILENAME  = f'{trial_dir}/hparams_train.pkl'\n",
    "    HPARAMS_ARCH_FILENAME   = f'{trial_dir}/hparams_arch.pkl'\n",
    "    pickle.dump(hparams_train, open(HPARAMS_TRAIN_FILENAME, 'wb'))\n",
    "    pickle.dump(hparams_arch, open(HPARAMS_ARCH_FILENAME, 'wb'))\n",
    "\n",
    "    TRIAL_DIR                   = trial_dir\n",
    "    TRIAL_FOLDER                = TRIAL_DIR.split('/')[-1]\n",
    "    STUDY_DIR                   = TRIAL_DIR[:-(len(TRIAL_FOLDER) + 1)]\n",
    "    HPARAMS_ARCH_FILENAME       = f'{TRIAL_DIR}/hparams_arch.pkl'\n",
    "    HPARAMS_TRAIN_FILENAME      = f'{TRIAL_DIR}/hparams_train.pkl'\n",
    "    hparams_arch    = pickle.load(open(HPARAMS_ARCH_FILENAME, 'rb'))\n",
    "    BLOCK_SIZE      = int(hparams_arch['BLOCK_SIZE'])\n",
    "    BLOCK_DEPTH     = int(hparams_arch['BLOCK_DEPTH'])\n",
    "    BLOCK_LAYERS    = int(hparams_arch['BLOCK_LAYERS'])\n",
    "    HIDDEN_SIZE     = int(hparams_arch['HIDDEN_SIZE'])\n",
    "    KERNEL_NUM      = int(hparams_arch['KERNEL_NUM'])\n",
    "    # Remove hparams_arch after use\n",
    "    del hparams_arch\n",
    "\n",
    "    hparams_train   = pickle.load(open(HPARAMS_TRAIN_FILENAME, 'rb'))\n",
    "    N_FOLD          = int(hparams_train['N_FOLD'])\n",
    "    N_EPOCH         = int(hparams_train['N_EPOCH'])\n",
    "    BATCH_SIZE      = int(hparams_train['BATCH_SIZE'])\n",
    "    LR_INITIAL      = hparams_train['LR_INITIAL']\n",
    "    LR_STEP_SIZE    = hparams_train['LR_STEP_SIZE']\n",
    "    LR_STEP_GAMMA   = hparams_train['LR_STEP_GAMMA']\n",
    "    EARLY_STOP_PAT  = hparams_train['EARLY_STOP_PAT']\n",
    "    # Remove hparams_train after use\n",
    "    del hparams_train\n",
    "\n",
    "    model = FinalModel(block_size=BLOCK_SIZE, block_depth=BLOCK_DEPTH, block_layers=BLOCK_LAYERS, hidden_size=HIDDEN_SIZE, kernel_num=KERNEL_NUM).to(DEVICE)\n",
    "    torch.save(model.state_dict(), f'{TRIAL_DIR}/initial_weights.pth')\n",
    "    best_val_auroc_list = []\n",
    "\n",
    "    # --- Cross-validation to determine early stopping points ---\n",
    "    early_stop_epochs = []\n",
    "    for i_fold in range(N_FOLD):\n",
    "        train_filenames = np.concatenate(fold_filenames[:i_fold] + fold_filenames[i_fold+1:])\n",
    "        valid_filenames = fold_filenames[i_fold]\n",
    "        train = dataset(header_files=train_filenames)\n",
    "        train.num_leads = 12\n",
    "        train.sample = True\n",
    "        valid = dataset(header_files=valid_filenames)\n",
    "        valid.num_leads = 12\n",
    "        valid.sample = False\n",
    "        valid.files.reset_index(drop=True, inplace=True)\n",
    "        train_loader = DataLoader(train, collate_fn=collate_fn, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        valid_loader = DataLoader(valid, collate_fn=collate_fn, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        model.load_state_dict(torch.load(f'{TRIAL_DIR}/initial_weights.pth', weights_only=True))\n",
    "        weight_cache = f'{TRIAL_DIR}/weights_fold_{i_fold+1}.pth'\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        optimizer = Adam(model.parameters(), lr=LR_INITIAL, weight_decay=1e-4)\n",
    "        scheduler = ReduceLROnPlateau(optimizer, patience=LR_STEP_SIZE, factor=LR_STEP_GAMMA, min_lr=1e-5, mode='max')\n",
    "        best_val_auroc = 0.0\n",
    "        early_stop_count = 0\n",
    "        best_epoch = 0\n",
    "        for epoch in range(N_EPOCH):\n",
    "            print(f'FOLD {i_fold+1} - EPOCH {1 + epoch}/{N_EPOCH}')        \n",
    "            train_loss = train_model(train_loader, model, criterion, optimizer, scheduler)\n",
    "            y_true_valid, y_pred_valid, _ = evaluate_model(valid_loader, model)\n",
    "            valid_loss = F.binary_cross_entropy(torch.FloatTensor(y_pred_valid), torch.FloatTensor(y_true_valid))\n",
    "            valid_auroc = roc_auc_score(y_true_valid, y_pred_valid)\n",
    "            print(f'val_auroc: {valid_auroc:.4f}')\n",
    "            scheduler.step(valid_auroc)\n",
    "            if valid_auroc > best_val_auroc:\n",
    "                print(f'>> val_auroc increased from {best_val_auroc:.4f} to {valid_auroc:.4f}>> Saving weights to [{weight_cache}]')\n",
    "                torch.save(model.state_dict(), weight_cache)\n",
    "                best_val_auroc = valid_auroc\n",
    "                early_stop_count = 0\n",
    "                best_epoch = epoch + 1  # 1-based epoch\n",
    "            else:\n",
    "                early_stop_count += 1\n",
    "            if early_stop_count >= EARLY_STOP_PAT:\n",
    "                break\n",
    "        best_val_auroc_list.append(best_val_auroc)\n",
    "        early_stop_epochs.append(best_epoch if best_epoch > 0 else epoch + 1)\n",
    "        # Remove per-fold variables after use\n",
    "        del train_filenames, valid_filenames, train, valid, train_loader, valid_loader, weight_cache, criterion, optimizer, scheduler\n",
    "\n",
    "    val_auroc_mean = np.mean(np.array(best_val_auroc_list), axis=0)\n",
    "\n",
    "    print('*' * 100)\n",
    "    for i in range(N_FOLD):\n",
    "        print(f'Fold #{i}  : {best_val_auroc_list[i]:.4f} (early stop at epoch {early_stop_epochs[i]})')\n",
    "    print(f'Mean : {val_auroc_mean:.4f}')\n",
    "    print('*' * 100)\n",
    "\n",
    "    # --- Fit final model on develop set for average early stopping steps ---\n",
    "    weight_cache = f'{TRIAL_DIR}/weights.pth'\n",
    "    # Only create develop dataset/loader once per trial\n",
    "    develop_filenames = np.concatenate(fold_filenames)\n",
    "    develop = dataset(header_files=develop_filenames)\n",
    "    develop.num_leads = 12\n",
    "    develop.sample = True\n",
    "    develop_loader = DataLoader(develop, collate_fn=collate_fn, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    avg_early_stop_epoch = int(np.round(np.mean(early_stop_epochs)))\n",
    "    print(f\"Training final model on develop set for {avg_early_stop_epoch} epochs (average early stopping point)\")\n",
    "    model.load_state_dict(torch.load(f'{TRIAL_DIR}/initial_weights.pth', weights_only=True))\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=LR_INITIAL)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, patience=LR_STEP_SIZE, factor=LR_STEP_GAMMA, min_lr=1e-5, mode='max')\n",
    "    for epoch in range(int((avg_early_stop_epoch)*4/5)):\n",
    "        print(f'DEVELOP FINAL - EPOCH {1 + epoch}/{avg_early_stop_epoch}')\n",
    "        train_loss = train_model(develop_loader, model, criterion, optimizer, scheduler)\n",
    "    torch.save(model.state_dict(), weight_cache)\n",
    "    # Remove develop set variables after use\n",
    "    del develop_filenames, develop, develop_loader, criterion, optimizer, scheduler, weight_cache\n",
    "\n",
    "    TRIAL_FOLDER_NEW = f'auc={val_auroc_mean:.4f}_{TRIAL_FOLDER}'\n",
    "    TRIAL_DIR_NEW = f'{STUDY_DIR}/{TRIAL_FOLDER_NEW}'\n",
    "    os.rename(TRIAL_DIR, TRIAL_DIR_NEW)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_dir = \"model/auc=0.8772_batch=64_lr=0.00086_step=2_gam=0.164_pat=4_block_size=16_block_depth=3_hidden_size=32_block_layers=3_kernel_num=5\"\n",
    "TRIAL_DIR                   = trial_dir\n",
    "TRIAL_FOLDER                = TRIAL_DIR.split('/')[-1]\n",
    "STUDY_DIR                   = TRIAL_DIR[:-(len(TRIAL_FOLDER) + 1)]\n",
    "HPARAMS_ARCH_FILENAME       = f'{TRIAL_DIR}/hparams_arch.pkl'\n",
    "HPARAMS_TRAIN_FILENAME      = f'{TRIAL_DIR}/hparams_train.pkl'\n",
    "hparams_arch    = pickle.load(open(HPARAMS_ARCH_FILENAME, 'rb'))\n",
    "BLOCK_SIZE      = int(hparams_arch['BLOCK_SIZE'])\n",
    "BLOCK_DEPTH     = int(hparams_arch['BLOCK_DEPTH'])\n",
    "BLOCK_LAYERS    = int(hparams_arch['BLOCK_LAYERS'])\n",
    "HIDDEN_SIZE     = int(hparams_arch['HIDDEN_SIZE'])\n",
    "KERNEL_NUM      = int(hparams_arch['KERNEL_NUM'])\n",
    "hparams_train   = pickle.load(open(HPARAMS_TRAIN_FILENAME, 'rb'))\n",
    "N_FOLD          = int(hparams_train['N_FOLD'])\n",
    "N_EPOCH         = int(hparams_train['N_EPOCH'])\n",
    "BATCH_SIZE      = int(hparams_train['BATCH_SIZE'])\n",
    "LR_INITIAL      = hparams_train['LR_INITIAL']\n",
    "LR_STEP_SIZE    = hparams_train['LR_STEP_SIZE']\n",
    "LR_STEP_GAMMA   = hparams_train['LR_STEP_GAMMA']\n",
    "EARLY_STOP_PAT  = hparams_train['EARLY_STOP_PAT']\n",
    "\n",
    "test_filenames = np.load('test_filenames.npy', allow_pickle=True)\n",
    "test= dataset(header_files=test_filenames)\n",
    "test.num_leads = 12\n",
    "test.sample = False\n",
    "test.files.reset_index(drop=True, inplace=True)\n",
    "test_loader = DataLoader(test, collate_fn=collate_fn, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "fold_filenames = [np.load(f'fold_{i}_filenames.npy', allow_pickle=True) for i in range(1, 6)]\n",
    "develop_filenames = np.concatenate(fold_filenames)\n",
    "develop = dataset(header_files=develop_filenames)\n",
    "develop.num_leads = 12\n",
    "develop.sample = False\n",
    "develop.files.reset_index(drop=True, inplace=True)\n",
    "develop_loader = DataLoader(develop, collate_fn=collate_fn, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_num_threads(os.cpu_count())\n",
    "\n",
    "model = FinalModel(\n",
    "    block_size=BLOCK_SIZE,\n",
    "    block_depth=BLOCK_DEPTH,\n",
    "    block_layers=BLOCK_LAYERS,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    kernel_num=KERNEL_NUM\n",
    ").to(DEVICE)\n",
    "weight_cache = f'{TRIAL_DIR}/weights.pth'\n",
    "model.load_state_dict(torch.load(weight_cache, weights_only=True))\n",
    "\n",
    "with open('y_test_proba_cnn.pkl', 'rb') as f:\n",
    "    y_pred_test = pickle.load(f)\n",
    "\n",
    "DEVICE = next(model.parameters()).device\n",
    "SAMPLING_RATE = 500\n",
    "BEAT_LEN = 400\n",
    "TOP_K = 1000\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 1. Grab raw signals, ages, genders, predictions (one pass only)\n",
    "# ----------------------------------------------------------------\n",
    "ecgs, ages, genders, preds = [], [], [], []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for X, _, a, g in tqdm(test_loader, desc='Collecting signals'):\n",
    "        ecgs.extend(X.cpu().numpy())\n",
    "        ages.extend(a.cpu().numpy())\n",
    "        genders.extend(g.cpu().numpy())\n",
    "preds = y_pred_test.ravel()\n",
    "ecgs = np.asarray(ecgs)\n",
    "ages = np.asarray(ages)\n",
    "genders = np.asarray(genders)\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 2. Patient-level median waveform & ROI  (Lead-II reference)\n",
    "# ----------------------------------------------------------------\n",
    "def patient_median_and_roi(signal_12xN, *, sampling_rate=SAMPLING_RATE, beat_len=BEAT_LEN):\n",
    "    \"\"\"\n",
    "    signal_12xN : np.ndarray (12, N)\n",
    "    Returns\n",
    "    -------\n",
    "    median_wave : np.ndarray (12, beat_len)\n",
    "    roi_indices_per_lead : list[ list[(start, end)] ]   # per lead\n",
    "    \"\"\"\n",
    "    # --- 1) detect R-peaks once on Lead-II (index 1) -----------------\n",
    "    lead_ref = signal_12xN[1]\n",
    "    try:\n",
    "        _, rpeaks = nk.ecg_peaks(lead_ref, sampling_rate=sampling_rate)\n",
    "        r_idx = rpeaks[\"ECG_R_Peaks\"]\n",
    "    except Exception:\n",
    "        return np.full((12, beat_len), np.nan, np.float32), [[] for _ in range(12)]\n",
    "\n",
    "    if len(r_idx) < 3:\n",
    "        return np.full((12, beat_len), np.nan, np.float32), [[] for _ in range(12)]\n",
    "\n",
    "    # --- 2) build per-lead epochs and resample -----------------------\n",
    "    median_wave = np.full((12, beat_len), np.nan, np.float32)\n",
    "    roi_indices_perlead = []\n",
    "\n",
    "    for lead in signal_12xN:\n",
    "        try:\n",
    "            epochs = nk.epochs_create(\n",
    "                lead, events=r_idx,\n",
    "                sampling_rate=sampling_rate,\n",
    "                epochs_start=-0.3, epochs_end=0.5\n",
    "            )\n",
    "        except Exception:\n",
    "            roi_indices_perlead.append([])\n",
    "            continue\n",
    "\n",
    "        keys = list(epochs.keys())[1:-1]\n",
    "        if not keys:\n",
    "            roi_indices_perlead.append([])\n",
    "            continue\n",
    "\n",
    "        roi_this_lead = [epochs[k][\"Index\"] for k in keys]\n",
    "        roi_indices_perlead.append(roi_this_lead)\n",
    "\n",
    "        try:\n",
    "            beats_rs = resample(\n",
    "                np.stack([epochs[k][\"Signal\"] for k in keys]),\n",
    "                beat_len, axis=1\n",
    "            )\n",
    "            median_wave[len(roi_indices_perlead)-1] = np.nanmedian(beats_rs, axis=0)\n",
    "        except Exception:\n",
    "            median_wave[len(roi_indices_perlead)-1] = np.full((beat_len,), np.nan, np.float32)\n",
    "\n",
    "    return median_wave, roi_indices_perlead\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 3. Group median waveforms (high-risk vs low-risk) and ROI indices\n",
    "# ----------------------------------------------------------------\n",
    "hi_idx = np.argsort(preds)[-TOP_K:]\n",
    "lo_idx = np.argsort(preds)[:TOP_K:]\n",
    "del preds\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "def group_median_and_roi(indices):\n",
    "    results = Parallel(n_jobs=-1, backend=\"loky\")(\n",
    "        delayed(patient_median_and_roi)(ecgs[i]) for i in indices\n",
    "    )\n",
    "    patient_meds, patient_rois = zip(*results)\n",
    "    patient_meds = np.stack(patient_meds)\n",
    "    return np.nanmedian(patient_meds, axis=0), list(patient_rois)\n",
    "\n",
    "try:\n",
    "    median_hi, roi_hi = group_median_and_roi(hi_idx)\n",
    "except Exception as e:\n",
    "    print(f\"Error in group_median_and_roi(hi_idx): {e}\")\n",
    "    median_hi, roi_hi = None, None\n",
    "\n",
    "try:\n",
    "    median_lo, roi_lo = group_median_and_roi(lo_idx)\n",
    "except Exception as e:\n",
    "    print(f\"Error in group_median_and_roi(lo_idx): {e}\")\n",
    "    median_lo, roi_lo = None, None\n",
    "\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 4. Plot median waveforms (4x3 grid, custom lead order and names)\n",
    "#    Also overlay original top_K waveforms in brighter, slimmer lines\n",
    "# ----------------------------------------------------------------\n",
    "lead_names = ['I', 'II', 'III', 'aVR', 'aVL', 'aVF','V1', 'V2', 'V3', 'V4', 'V5', 'V6']\n",
    "lead_indices = [0, 1,2,3,4,5,6,7,8,9,10,11]\n",
    "\n",
    "time_ms = np.arange(BEAT_LEN) * 1000 / SAMPLING_RATE  # x-axis in ms\n",
    "\n",
    "fig, axes = plt.subplots(4, 3, figsize=(14, 16), sharex=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for plot_idx, lead_idx in enumerate(lead_indices):\n",
    "    # Shift high risk plot by 10 frames\n",
    "    shifted_hi = np.roll(median_hi[lead_idx], 10)\n",
    "    # Plot the group median waveforms only\n",
    "    axes[plot_idx].plot(time_ms, shifted_hi, color='red', label='High MACCE risk', linewidth=2.2, zorder=2)\n",
    "    axes[plot_idx].plot(time_ms, median_lo[lead_idx], color='green', label='Low MACCE risk', linewidth=2.2, zorder=2)\n",
    "    axes[plot_idx].set_ylabel(f\"mV\")\n",
    "    axes[plot_idx].set_title(lead_names[lead_idx])\n",
    "    axes[plot_idx].set_xlim([time_ms[0], time_ms[-1]])\n",
    "    axes[plot_idx].set_ylim([median_hi[lead_idx].min()-5, median_hi[lead_idx].max()+3])\n",
    "    axes[plot_idx].set_xlabel('ms')\n",
    "    # Place legend inside the plot area at the bottom center, not overlapping with timeseries\n",
    "    axes[plot_idx].legend(\n",
    "        loc='lower center',\n",
    "        bbox_to_anchor=(0.5, 0.05),\n",
    "        borderaxespad=0.,\n",
    "        fontsize='small',\n",
    "        frameon=True\n",
    "    )\n",
    "for ax in axes[:-1]:\n",
    "    ax.set_xlabel('')\n",
    "# Hide any unused subplots (if any)\n",
    "for ax in axes[len(lead_indices):]:\n",
    "    ax.axis('off')\n",
    "# Add more empty space above and below the plots\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])  \n",
    "plt.show()\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 5. SHAP deep explainer on high-risk ECGs\n",
    "# ----------------------------------------------------------------\n",
    "del test_loader\n",
    "\n",
    "class ModelWrapper(torch.nn.Module):\n",
    "    \"\"\"Wrap FinalModel to handle concatenated ECG, age, and gender inputs.\"\"\"\n",
    "    def __init__(self, base):\n",
    "        super().__init__()\n",
    "        self.base = base.eval()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Split input into ECG, age, and gender components\n",
    "        ecg = x[:, :12*5000]  # First 12*5000 elements are ECG channels\n",
    "        age = x[:, 12*5000:12*5000+1]  # Next element is age\n",
    "        gender = x[:, 12*5000+1:]  # Last element is gender\n",
    "        # Reshape ECG to (batch, 12, 5000)\n",
    "        ecg = ecg.reshape(-1, 12, 5000)\n",
    "        out = self.base(ecg, age, gender)\n",
    "        return F.sigmoid(out)\n",
    "\n",
    "# Use GPU for SHAP computation if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Convert ages and genders to tensors (on GPU)\n",
    "ages_torch = torch.tensor(ages, dtype=torch.float32, device=device)\n",
    "genders_torch = torch.tensor(genders, dtype=torch.float32, device=device)\n",
    "\n",
    "# Create background data with ECG, age, and gender concatenated (on GPU)\n",
    "background_ecgs = torch.tensor(ecgs[lo_idx[:10]], dtype=torch.float32, device=device)\n",
    "background_ecgs_flat = background_ecgs.reshape(background_ecgs.shape[0], -1)\n",
    "background_ages = ages_torch[lo_idx[:10]]\n",
    "background_genders = genders_torch[lo_idx[:10]]\n",
    "background = torch.cat([background_ecgs_flat, background_ages, background_genders], dim=1)\n",
    "\n",
    "SHAP_BATCH = 1\n",
    "DTYPE = torch.float32\n",
    "\n",
    "def make_activations_safe(module):\n",
    "    for child in module.children():\n",
    "        if hasattr(child, \"inplace\") and child.inplace:\n",
    "            child.inplace = False\n",
    "        make_activations_safe(child)\n",
    "\n",
    "wrapped_model = ModelWrapper(model).to(device)\n",
    "make_activations_safe(wrapped_model)\n",
    "wrapped_model_gpu = wrapped_model.float().to(device)\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "explainer = shap.DeepExplainer(wrapped_model_gpu, background)\n",
    "del background, background_ecgs, background_ecgs_flat, background_ages, background_genders\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Compute SHAP values batch-wise (on GPU)\n",
    "shap_vals_chunks = []\n",
    "for start in tqdm(range(0, len(hi_idx), SHAP_BATCH), desc=\"SHAP batches\"):\n",
    "    batch_idx = hi_idx[start:start+SHAP_BATCH]\n",
    "    batch_ecgs = torch.tensor(ecgs[batch_idx], dtype=DTYPE, device=device)\n",
    "    batch_ages = ages_torch[batch_idx]\n",
    "    batch_genders = genders_torch[batch_idx]\n",
    "    batch_flat = batch_ecgs.reshape(batch_ecgs.shape[0], -1)\n",
    "    batch = torch.cat([batch_flat, batch_ages, batch_genders], dim=1)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # Calculate SHAP values\n",
    "    shap_batch = explainer.shap_values(batch, check_additivity=False)[0]\n",
    "    shap_batch = shap_batch.astype(np.float32)[:12*5000]\n",
    "    shap_vals_chunks.append(shap_batch)\n",
    "    \n",
    "    # Clear memory\n",
    "    del batch, batch_ecgs, batch_ages, batch_genders, batch_flat, shap_batch\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "shap_vals = np.array(shap_vals_chunks)    # (K, 12*5000)\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 6. Aggregate saliency using region-of-interest (ROI) SHAP values\n",
    "#    (EXCLUDE first and last detected beat in both waveform and shap)\n",
    "# ----------------------------------------------------------------\n",
    "# For each high-risk sample, for each lead, extract the SHAP values corresponding to the same ROI as used for the median waveform\n",
    "# and resample/average to BEAT_LEN, then median across patients\n",
    "\n",
    "shap_abs = np.abs(shap_vals)  # (K, 12*5000)\n",
    "del shap_vals\n",
    "gc.collect()\n",
    "\n",
    "from scipy.signal import resample\n",
    "def extract_roi_shap_per_lead(shap_12x5000, roi_indices_per_lead, beat_len=BEAT_LEN):\n",
    "    beats_out = np.full((12, beat_len), np.nan, dtype=np.float32)\n",
    "    for lead, (lead_shap, lead_rois) in enumerate(zip(shap_12x5000, roi_indices_per_lead)):\n",
    "        if len(lead_rois) < 3:       # no middle beats → already nan\n",
    "            continue\n",
    "        use_rois = lead_rois[1:-1]\n",
    "        if not use_rois:\n",
    "            continue\n",
    "        # Vectorised resampling of all beats in one shot\n",
    "        beats = np.vstack([\n",
    "            resample(lead_shap[roi.loc[-0.3]:roi.loc[0.5]+1], beat_len)\n",
    "            for roi in use_rois\n",
    "        ])\n",
    "        beats_out[lead] = np.nanmedian(beats, axis=0)\n",
    "    return beats_out\n",
    "\n",
    "# For each high-risk sample, extract ROI-aligned SHAP\n",
    "saliency_hi = []\n",
    "for i in range(len(hi_idx)):\n",
    "    # SHAP for this sample: (12*5000,) -> (12, 5000)\n",
    "    sample_shap = shap_abs[i].reshape(12, 5000)\n",
    "    # ROI indices for this sample: roi_hi[i] (list of 12 lists of (start, end))\n",
    "    med_sample = extract_roi_shap_per_lead(sample_shap, roi_hi[i], beat_len=BEAT_LEN)\n",
    "    saliency_hi.append(med_sample)\n",
    "saliency_hi = np.stack(saliency_hi)  # (K, 12, beat_len)\n",
    "\n",
    "del shap_abs\n",
    "gc.collect()\n",
    "saliency_hi = np.nanmedian(saliency_hi, axis=0)        # (12, beat_len)\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 7. Plot saliency map OVER high-risk median waveform (4x3 grid, custom lead order and names)\n",
    "# ----------------------------------------------------------------\n",
    "# Desired order: I, II, III // aVR, aVL, aVF // V1, V2, V3 // V4, V5, V6\n",
    "fig, axes = plt.subplots(4, 3, figsize=(14, 16), sharex=True)\n",
    "axes = axes.flatten()\n",
    "im = None\n",
    "time_ms = np.arange(BEAT_LEN) * 1000 / SAMPLING_RATE  # x-axis in ms\n",
    "\n",
    "# Set a simple white-to-blue colormap for SHAP\n",
    "white_blue_cmap = LinearSegmentedColormap.from_list(\n",
    "    \"white_blue\", [\"#ffffff\", \"#3b4cc0\"]\n",
    ")\n",
    "saliency_cmap = white_blue_cmap\n",
    "\n",
    "for plot_idx, lead_idx in enumerate(lead_indices):\n",
    "    y_min = np.nanmin(median_hi[lead_idx])\n",
    "    y_max = np.nanmax(median_hi[lead_idx])\n",
    "    y_range = y_max - y_min\n",
    "    margin = 0.25 * y_range if y_range > 0 else 0.5\n",
    "    plot_ymin = y_min - margin\n",
    "    plot_ymax = y_max + margin\n",
    "    axes[plot_idx].set_ylabel(f\"mV\")\n",
    "    axes[plot_idx].set_title(lead_names[plot_idx])\n",
    "    axes[plot_idx].set_xlim([time_ms[0], time_ms[-1]])\n",
    "    axes[plot_idx].set_ylim([median_hi[lead_idx].min()-5, median_hi[lead_idx].max()+3]) \n",
    "    axes[plot_idx].set_xlabel('ms')\n",
    "\n",
    "    axes[plot_idx].plot(time_ms, median_hi[lead_idx], color='red', zorder=2)\n",
    "    im = axes[plot_idx].imshow(\n",
    "        saliency_hi[lead_idx][None, :], aspect='auto',\n",
    "        cmap=saliency_cmap, interpolation='nearest',\n",
    "        extent=[time_ms[0], time_ms[-1], median_hi[lead_idx].min()-5, median_hi[lead_idx].max()+3],\n",
    "        alpha=0.85,  # slightly less transparent for more contrast\n",
    "        origin='lower', zorder=1\n",
    "    )\n",
    "\n",
    "for ax in axes[:-1]:\n",
    "    ax.set_xlabel('')\n",
    "\n",
    "# Hide any unused subplots (if any)\n",
    "for ax in axes[len(lead_indices):]:\n",
    "    ax.axis('off')\n",
    "\n",
    "# Place colorbars on the rightmost column (i.e., axes 2, 5, 8, 11)\n",
    "rightmost_indices = [2, 5, 8, 11]\n",
    "for i in rightmost_indices:\n",
    "    cbar = fig.colorbar(im, ax=axes[i], orientation='vertical',\n",
    "                       fraction=0.04, pad=0.02)\n",
    "    cbar.set_ticks([cbar.vmin, cbar.vmax])\n",
    "    cbar.set_ticklabels(['Low\\nsalience', 'High\\nsalience'])\n",
    "    cbar.ax.yaxis.set_ticks_position('right')\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])  \n",
    "plt.show()\n",
    "del fig, axes, im, saliency_hi, time_ms\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(SEED)\n",
    "df = pd.read_csv('data_labels_M_N.csv')\n",
    "df2 = pd.read_csv('ecg_labels_w_features_lab_asa_3mo_N.csv')\n",
    "df['andur'] = df['filename'].map(df2.set_index('filename')['andur'])\n",
    "df['asa'] = df['filename'].map(df2.set_index('filename')['final_asa'])\n",
    "df.to_csv('data_labels_gbm.csv', index=False)\n",
    "\n",
    "all= dataset(header_files=df['filename'].to_list())\n",
    "all.num_leads = 12\n",
    "all.sample = False\n",
    "all.files.reset_index(drop=True, inplace=True)\n",
    "all_loader = DataLoader(all, collate_fn=collate_fn, batch_size=BATCH_SIZE, shuffle=False)\n",
    "model = FinalModel(block_size =BLOCK_SIZE, block_depth =BLOCK_DEPTH, block_layers=BLOCK_LAYERS, hidden_size=HIDDEN_SIZE, kernel_num=KERNEL_NUM).to(DEVICE)\n",
    "weight_cache = f'{TRIAL_DIR}/weights.pth'\n",
    "model.load_state_dict(torch.load(weight_cache, weights_only=True))\n",
    "y_true_test, y_pred_test, hidden_states = evaluate_model(all_loader, model)\n",
    "hidden_states_df = pd.DataFrame(hidden_states, columns=[f'hidden_{i}' for i in range(HIDDEN_SIZE)])\n",
    "df = pd.concat([df, hidden_states_df], axis=1)\n",
    "df.to_csv('data_labels_gbm_hidden.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run on tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "SEED= 1\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, droprate):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.droprate = droprate\n",
    "        self.ff_dim = ff_dim\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential([layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),])\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(droprate)\n",
    "        self.dropout2 = layers.Dropout(droprate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {'embed_dim': self.embed_dim, 'num_heads': self.num_heads, 'ff_dim':self.ff_dim, 'droprate':self.droprate}\n",
    "\n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.maxlen = maxlen\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "\n",
    "    def get_config(self):\n",
    "        return {'maxlen': self.maxlen, 'vocab_size': self.vocab_size, 'embed_dim': self.embed_dim}\n",
    "\n",
    "df_temp = pd.read_csv('transformer/icd10_mapping_hclee_corrected_manual_01.csv', usecols=['opname', 'p', 'o', 'a'], dtype=str)\n",
    "df_temp = df_temp.dropna(subset=['p', 'o', 'a'])\n",
    "df_temp = df_temp.drop_duplicates(subset=['opname'], keep='first')\n",
    "\n",
    "df = pd.read_csv('data_labels_gbm.csv', dtype=str)\n",
    "df['src'] = 'snuh'\n",
    "df = df.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "opname_set = set(df_temp['opname'])\n",
    "mask = df['opname'].isin(opname_set)\n",
    "df.loc[mask, 'opname_final'] = df.loc[mask, 'opname']\n",
    "\n",
    "df['opname_final'] = df['opname_final'].fillna(df['opname'])\n",
    "\n",
    "x_raw = np.copy(df['opname_final'].values.astype(str))\n",
    "vocab_size = 4000\n",
    "\n",
    "#t_x = Tokenizer(vocab_size)\n",
    "#t_x.fit_on_texts(x_raw)\n",
    "#with open('transformer/tokenizer_x.pkl', 'wb') as f:\n",
    "#    pickle.dump(t_x, f)\n",
    "\n",
    "with open('transformer/tokenizer_x.pkl', 'rb') as f:\n",
    "    t_x = pickle.load(f)\n",
    "x_seq = t_x.texts_to_sequences(x_raw)\n",
    "maxlen = 158  \n",
    "x_pad = keras.preprocessing.sequence.pad_sequences(x_seq, maxlen=maxlen)\n",
    "\n",
    "# To ensure consistent predictions for the same input, cache predictions for unique input sequences\n",
    "# Map from tuple(sequence) -> prediction\n",
    "x_pad_tuples = [tuple(row) for row in x_pad]\n",
    "unique_x_pad, unique_indices, inverse_indices = np.unique(x_pad, axis=0, return_index=True, return_inverse=True)\n",
    "unique_x_pad_tuples = [tuple(row) for row in unique_x_pad]\n",
    "\n",
    "prefix = 'transformer_res'\n",
    "for target in ('p', 'o', 'a'):\n",
    "    print(f'Processing {target}...')\n",
    "    opath = f'transformer/{prefix}_{target}.csv'\n",
    "    t_y = Tokenizer()\n",
    "    tokenizer_config = json.loads(open(f'transformer/tokenizer_y_{target}.json').read())\n",
    "    tokenizer_config = tokenizer_config['config']\n",
    "    t_y.word_index = json.loads(tokenizer_config['word_index']) if isinstance(tokenizer_config['word_index'], str) else tokenizer_config['word_index']\n",
    "    t_y.index_word = json.loads(tokenizer_config['index_word']) if isinstance(tokenizer_config['index_word'], str) else tokenizer_config['index_word']\n",
    "    t_y.word_counts = json.loads(tokenizer_config['word_counts']) if isinstance(tokenizer_config['word_counts'], str) else tokenizer_config['word_counts']\n",
    "    t_y.index_word = {int(k): v for k, v in t_y.index_word.items()}\n",
    "    t_y.document_count = tokenizer_config['document_count']\n",
    "    custom_objects = {\n",
    "        'TokenAndPositionEmbedding': TokenAndPositionEmbedding,\n",
    "        'TransformerBlock': TransformerBlock\n",
    "    }\n",
    "    model = keras.models.model_from_json(\n",
    "        open(f'transformer/model_{target}.json').read(),\n",
    "        custom_objects=custom_objects\n",
    "    )\n",
    "    model.load_weights(f'transformer/tuned_weights_{target}.h5')\n",
    "\n",
    "    pred_unique = model.predict(unique_x_pad, verbose=1)\n",
    "    pred = pred_unique[inverse_indices]\n",
    "\n",
    "    df['pred'] = pd.Series(t_y.sequences_to_texts(np.argmax(pred, axis=1)[...,None] + 1)).str.upper()\n",
    "    df['conf'] = pred.max(axis=1)\n",
    "    df[df['src'] == 'snuh'].drop(columns='src').to_csv(opath, index=False, encoding='utf-8-sig')\n",
    "    print(f'Finished processing {target}\\n')\n",
    "\n",
    "df = pd.read_csv(f'transformer/{prefix}_p.csv', dtype=str, usecols=['opname_final', 'pred']).rename(columns={'pred':'p'})\n",
    "df = df.drop_duplicates(subset=['opname_final'], keep='first')\n",
    "df_o = pd.read_csv(f'transformer/{prefix}_o.csv', dtype=str, usecols=['opname_final', 'pred']).rename(columns={'pred':'o'})\n",
    "df_o = df_o.drop_duplicates(subset=['opname_final'], keep='first')\n",
    "df['o'] = df['opname_final'].map(df_o.set_index('opname_final')['o'])\n",
    "df_a = pd.read_csv(f'transformer/{prefix}_a.csv', dtype=str, usecols=['opname_final', 'pred']).rename(columns={'pred':'a'})\n",
    "df_a = df_a.drop_duplicates(subset=['opname_final'], keep='first')\n",
    "df['a'] = df['opname_final'].map(df_a.set_index('opname_final')['a'])\n",
    "\n",
    "df_final = pd.read_csv('data_labels_gbm_hidden_icd.csv', dtype=str)\n",
    "\n",
    "# Set opname_final to opname if opname is in df_temp['opname'] (even if not NA)\n",
    "mask = df_final['opname'].isin(opname_set)\n",
    "df_final.loc[mask, 'opname_final'] = df_final.loc[mask, 'opname']\n",
    "\n",
    "df_final['opname_final'] = df_final['opname_final'].fillna(df_final['opname'])\n",
    "\n",
    "mask2 = df_final['opname_final'].isin(opname_set)\n",
    "for col in ['p','o','a']:\n",
    "    df_final.loc[mask2, col] = df_final.loc[mask2, 'opname_final'].map(df_temp.set_index('opname')[col])\n",
    "for col in ['p', 'o', 'a']:\n",
    "    df_final.loc[~mask2, col] = df_final.loc[~mask2, 'opname_final'].map(df.set_index('opname_final')[col])\n",
    "\n",
    "df_final.to_csv('data_labels_gbm_hidden_icd.csv', index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_labels_gbm_hidden_icd.csv')\n",
    "xml_fields = {\n",
    "    'mach_ventricular_rate': 'VentricularRate',\n",
    "    'mach_atrial_rate': 'AtrialRate',\n",
    "    'mach_pr_interval': 'PRInterval',\n",
    "    'mach_qrs_duration': 'QRSDuration',\n",
    "    'mach_qt_interval': 'QTInterval',\n",
    "    'mach_qtc_interval': 'QTCorrected',\n",
    "    'mach_p_axis': 'PAxis',\n",
    "    'mach_r_axis': 'RAxis',\n",
    "    'mach_t_axis': 'TAxis'\n",
    "}\n",
    "for col in xml_fields:\n",
    "    df[col] = None\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    xml_path = 'C:/rsrch/240801_ecg_mace/data/'+row['filename']\n",
    "    try:\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "        # Find the RestingECGMeasurements section\n",
    "        measurements = root.find('RestingECGMeasurements')\n",
    "        if measurements is not None:\n",
    "            for col, tag in xml_fields.items():\n",
    "                elem = measurements.find(tag)\n",
    "                df.at[idx, col] = elem.text if elem is not None else None\n",
    "        else:\n",
    "            continue\n",
    "    except Exception as e:\n",
    "        continue\n",
    "df.to_csv('data_labels_gbm_hidden_icd.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE=32\n",
    "df = pd.read_csv('data_labels_gbm_hidden_icd.csv')\n",
    "df_col =[]\n",
    "\n",
    "labels = 'label'\n",
    "model_name = 'age_gender_p_o_a' # wo_cnn_, mach_/ age,andur, asa, gender, p_o_a\n",
    "\n",
    "df['gender'] = df['gender'].astype(float)\n",
    "for x in ['gender','asa','age','andur']:\n",
    "   if x in model_name:\n",
    "      df_col.extend([x])\n",
    "if 'wo_cnn' not in model_name:\n",
    "   if 'mach_' in model_name:\n",
    "      df_col.extend([col for col in df.columns if 'mach_' in col])\n",
    "   else:\n",
    "      df_col.extend([f'hidden_{i}' for i in range(HIDDEN_SIZE)])\n",
    "if 'p_o_a' in model_name:\n",
    "   onehot_cols = ['p', 'o', 'a']\n",
    "   onehot_df = pd.get_dummies(df[onehot_cols], prefix=onehot_cols).astype(float)\n",
    "   df = df.drop(columns=onehot_cols)\n",
    "   df = pd.concat([df, onehot_df], axis=1)\n",
    "   df = df[df_col + list(onehot_df.columns) + ['label', 'hid', 'filename']]\n",
    "else:\n",
    "   df = df[df_col + ['label', 'hid', 'filename']]\n",
    "df['label'] = df['label'].replace({3: 1})\n",
    "\n",
    "test_filenames = np.load('test_filenames.npy', allow_pickle=True)\n",
    "fold_filenames = [np.load(f'fold_{i}_filenames.npy', allow_pickle=True) for i in range(1, 6)]\n",
    "develop_filenames = np.concatenate(fold_filenames)\n",
    "df_train = df[df['filename'].isin(develop_filenames)]\n",
    "df_test = df[df['filename'].isin(test_filenames)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'xgb': create_objective_function('xgb', xgb.XGBClassifier, xgb_params, df_train.drop(columns=['hid', 'label', 'filename']).columns, model_name),\n",
    "    'rf': create_objective_function('rf', RandomForestClassifier, rf_params, df_train.drop(columns=['hid', 'label', 'filename']).columns, model_name),\n",
    "    'svm': create_objective_function('svm', SVC, svm_params, df_train.drop(columns=['hid', 'label', 'filename']).columns, model_name),\n",
    "    'logreg': create_objective_function('logreg', LogisticRegression, logreg_params, df_train.drop(columns=['hid', 'label', 'filename']).columns, model_name)\n",
    "}\n",
    "for model_type, objective_func in models.items():\n",
    "    print(f\"...{model_type.upper()}...\")\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(lambda trial: objective_func(trial, df_train.drop(columns=['filename'])), n_trials=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'xgb'\n",
    "with open(f\"best_params_{model_type}_{model_name}.json\", 'r') as f:\n",
    "    params = json.load(f)\n",
    "if model_type =='xgb':\n",
    "    base_model = xgb.XGBClassifier(**params)\n",
    "elif model_type =='svm':\n",
    "    base_model = SVC(**params)\n",
    "elif model_type == 'logreg':\n",
    "    base_model = LogisticRegression(**params)\n",
    "elif model_type == 'rf':\n",
    "    base_model = RandomForestClassifier(**params)\n",
    "\n",
    "# Prepare X_train and y_train\n",
    "X_train_full = df_train.drop(['label', 'hid', 'filename'], axis=1)\n",
    "y_train = df_train['label'].ravel()\n",
    "\n",
    "# --- BorutaShap feature selection only if model_name is 'age_gender_p_o_a' ---\n",
    "if model_name == 'age_gender_p_o_a':\n",
    "    if os.path.exists('selected_features.npy'):\n",
    "        selected_features = np.load('selected_features.npy', allow_pickle=True).tolist()\n",
    "        X_train = X_train_full[selected_features]\n",
    "    else:\n",
    "        boruta_model = BorutaShap(model=base_model, importance_measure='shap', classification=True)\n",
    "        boruta_model.fit(X=X_train_full, y=y_train, n_trials=30, sample=False, train_or_test='test', verbose=False)\n",
    "        # Get the selected features\n",
    "        selected_features = boruta_model.Subset().columns.tolist()\n",
    "        # Save the selected features\n",
    "        np.save('selected_features.npy', np.array(selected_features))\n",
    "        # Reduce X_train to selected features\n",
    "        X_train = X_train_full[selected_features]\n",
    "else:\n",
    "    selected_features = X_train_full.columns.tolist()\n",
    "    if 'p_o_a' in model_name:\n",
    "        selected_features_orig = np.load('selected_features.npy', allow_pickle=True).tolist()\n",
    "        selected_features = [col for col in selected_features_orig if col in selected_features]\n",
    "        if 'asa' in model_name:\n",
    "            selected_features.extend(['asa'])\n",
    "        if 'andur' in model_name:\n",
    "            selected_features.extend(['andur'])\n",
    "    X_train = X_train_full[selected_features]\n",
    "\n",
    "# Re-instantiate the model to avoid any contamination from BorutaShap\n",
    "if model_type =='xgb':\n",
    "    best_model = xgb.XGBClassifier(**params)\n",
    "elif model_type =='svm':\n",
    "    best_model = SVC(**params)\n",
    "elif model_type == 'logreg':\n",
    "    best_model = LogisticRegression(**params)\n",
    "elif model_type == 'rf':\n",
    "    best_model = RandomForestClassifier(**params)\n",
    "\n",
    "if not os.path.exists(f\"fitted_{model_type}_{model_name}.joblib\"):\n",
    "    best_model.fit(X_train, y_train)\n",
    "    joblib.dump(best_model, f\"fitted_{model_type}_{model_name}.joblib\")\n",
    "else:\n",
    "    best_model = joblib.load(f\"fitted_{model_type}_{model_name}.joblib\")\n",
    "y_train_proba = best_model.predict_proba(X_train)[:,1].ravel()\n",
    "\n",
    "# Read anesthesia type and emop\n",
    "sg = pd.read_csv('data_labels_gbm_hidden_icd.csv')\n",
    "df_test['anetype'] = df_test['filename'].map(sg.set_index('filename')['anetype'])\n",
    "df_test['emop'] = df_test['filename'].map(sg.set_index('filename')['emop'])\n",
    "df_test['new_dept'] = df_test['filename'].map(sg.set_index('filename')['new_dept'])\n",
    "df_test['new_dept'] = df_test['new_dept'].replace(['OG', 'UR'], 'OG+UR')\n",
    "df_test['new_dept'] = df_test['new_dept'].replace(['OL', 'OT', 'PS', 'Others'], 'Others')\n",
    "\n",
    "# Add opid to df_test for subgroup counting\n",
    "if 'opid' not in df_test.columns:\n",
    "    if 'opid' in sg.columns:\n",
    "        df_test['opid'] = df_test['filename'].map(sg.set_index('filename')['opid'])\n",
    "    else:\n",
    "        # fallback: try to load from opid_icd_matching.csv if available\n",
    "        try:\n",
    "            opid_map = pd.read_csv('opid_icd_matching.csv')\n",
    "            df_test['opid'] = df_test['filename'].map(dict(zip(opid_map['filename'], opid_map['opid'])))\n",
    "        except Exception:\n",
    "            df_test['opid'] = None\n",
    "\n",
    "# Prepare test sets for all, general (1), and regional (0) anesthesia\n",
    "test_groups = {\n",
    "    'all': df_test.drop(columns=['anetype']),\n",
    "    'gen': df_test[df_test['anetype'] == 1].drop(columns=['anetype']),\n",
    "    'reg': df_test[df_test['anetype'] == 0].drop(columns=['anetype'])\n",
    "}\n",
    "# Add emop subgroups\n",
    "test_groups['emop1'] = df_test[df_test['emop'] == 1].drop(columns=['anetype'])\n",
    "test_groups['emop0'] = df_test[df_test['emop'] == 0].drop(columns=['anetype'])\n",
    "\n",
    "# Add gender subgroups only if 'gender' exists in df_test\n",
    "if 'gender' in df_test.columns:\n",
    "    test_groups['gender1'] = df_test[df_test['gender'] == 1].drop(columns=['anetype'])\n",
    "    test_groups['gender0'] = df_test[df_test['gender'] == 0].drop(columns=['anetype'])\n",
    "\n",
    "# Add elderly subgroups (age > 60 and age <= 60) only if 'age' exists in df_test\n",
    "if 'age' in df_test.columns:\n",
    "    test_groups['elderly1'] = df_test[df_test['age'] > 60].drop(columns=['anetype'])\n",
    "    test_groups['elderly0'] = df_test[df_test['age'] <= 60].drop(columns=['anetype'])\n",
    "\n",
    "# Add new_dept subgroups\n",
    "new_dept_categories = ['GS', 'NS', 'OG+UR', 'OS', 'TS', 'Others']\n",
    "for dept in new_dept_categories:\n",
    "    test_groups[f'new_dept_{dept}'] = df_test[df_test['new_dept'] == dept].drop(columns=['anetype'])\n",
    "\n",
    "# Predict probabilities for each group using only selected features\n",
    "X_test = {k: v.drop(['label', 'hid','filename'], axis=1)[selected_features] for k, v in test_groups.items()}\n",
    "y_test = {k: v['label'].ravel() for k, v in test_groups.items()}\n",
    "y_test_proba = {k: best_model.predict_proba(X_test[k])[:, 1].ravel() for k in test_groups}\n",
    "\n",
    "# Helper function to get n filenames, n unique hids, and n unique opids for a group\n",
    "def get_n_filename_hid_opid(df):\n",
    "    n_filename = len(df)\n",
    "    n_hid = len(df['hid'].unique()) if 'hid' in df.columns else 0\n",
    "    n_opid = len(df['opid'].unique()) if 'opid' in df.columns and df['opid'].notna().any() else 0\n",
    "    return n_filename, n_hid, n_opid\n",
    "\n",
    "# SHAP summary for the full test set (only if model_type is 'xgb')\n",
    "if model_type == 'xgb':\n",
    "    # Define mapping for feature names\n",
    "    feature_name_map = {\n",
    "        'age': 'Age',\n",
    "        'gender': 'Gender',\n",
    "        'o_J': 'Operation (Inspection)',\n",
    "        'o_Q': 'Operation (Repair)',\n",
    "        'o_Y': 'Operation (Transplantation)',\n",
    "        'p_WG': 'Body Part (Peritoneal Cavity)',\n",
    "        'p_BK': 'Body Part (Right Lung)',\n",
    "        'p_D5': 'Body Part (Esophagus)',\n",
    "        'p_T0': 'Body Part (Right Kidney)',\n",
    "        'a_3': 'Approach (Percutaneous)',\n",
    "        **{f'hidden_{i}': f'Hidden unit {i}' for i in range(33)},\n",
    "    }\n",
    "\n",
    "    # Create a list of display names for the features, using the mapping where available\n",
    "    display_feature_names = [\n",
    "        feature_name_map.get(col, col) for col in X_test['all'].columns\n",
    "    ]\n",
    "\n",
    "    explainer = shap.TreeExplainer(best_model)\n",
    "    shap_values = explainer.shap_values(X_test['all'])\n",
    "    shap.summary_plot(shap_values, X_test['all'], feature_names=display_feature_names, max_display =60)\n",
    "\n",
    "    # --- Draw SHAP force plot for a random patient in X_test['all'] ---\n",
    "    rng = random.Random(6)\n",
    "    random_idx = rng.randint(0, X_test['all'].shape[0] - 1)\n",
    "    patient_data = X_test['all'].iloc[random_idx:random_idx+1]\n",
    "    patient_shap_values = explainer.shap_values(patient_data)\n",
    "    print(f\"SHAP force plot for random patient at index {random_idx}:\")\n",
    "    # Format feature values and SHAP values to .3f for display\n",
    "    formatted_feature_values = [f\"{v:.1f}\" if isinstance(v, (float, int)) else str(v) \n",
    "                               for v in patient_data.values[0]]\n",
    "    formatted_shap_values = [f\"{v:.2f}\" for v in patient_shap_values[0]]\n",
    "    # Compose custom feature names with value and SHAP value\n",
    "    custom_feature_names = [\n",
    "        f\"{name}\\n({val}, SHAP={shapv})\"  # {name}\\n({val}, SHAP={shapv})\n",
    "        for name, val, shapv in zip(display_feature_names, formatted_feature_values, formatted_shap_values)\n",
    "    ]\n",
    "    shap.initjs()\n",
    "    # Use matplotlib force plot with custom feature names, adjust figsize and text position\n",
    "    fig = plt.figure(figsize=(max(8, len(custom_feature_names) * 0.7), 2.5))\n",
    "    # Draw force plot\n",
    "    force_plot = shap.force_plot(\n",
    "        explainer.expected_value, \n",
    "        patient_shap_values, \n",
    "        #patient_data, \n",
    "        feature_names=custom_feature_names, \n",
    "        matplotlib=True,\n",
    "        show=False\n",
    "    )\n",
    "    # Move x-tick labels (feature names) lower for readability\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=9)\n",
    "    ax = plt.gca()\n",
    "    texts_to_remove = []\n",
    "    for txt in ax.texts:\n",
    "        if txt.get_text() in [\"base value\", \"f(x)\"]:\n",
    "            texts_to_remove.append(txt)\n",
    "    for txt in texts_to_remove:\n",
    "        txt.remove()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Calibration before and after (only for 'all')\n",
    "cal_model = LinearRegression().fit(y_test_proba['all'].reshape(-1,1), y_test['all'])\n",
    "cal_intercept, cal_slope = cal_model.intercept_, cal_model.coef_[0]\n",
    "brier_before = brier_score_loss(y_test['all'], y_test_proba['all'])\n",
    "U_before, p_before = calc_unreliability(y_test['all'], y_test_proba['all'])\n",
    "\n",
    "# Spline calibration\n",
    "calib_filename = f\"calib_{model_type}_{model_name}.pkl\"\n",
    "if os.path.exists(calib_filename):\n",
    "    with open(calib_filename, \"rb\") as f:\n",
    "        calib = pickle.load(f)\n",
    "else:\n",
    "    calib = mli.SplineCalib(unity_prior=False, unity_prior_weight=100, random_state=42, max_iter=500)\n",
    "    calib.fit(y_train_proba, y_train)\n",
    "    with open(calib_filename, \"wb\") as f:\n",
    "        pickle.dump(calib, f)\n",
    "y_test_proba_cal = {k: calib.calibrate(y_test_proba[k]) for k in test_groups}\n",
    "cal_model_after = LinearRegression().fit(y_test_proba_cal['all'].reshape(-1,1), y_test['all'])\n",
    "cal_intercept_after, cal_slope_after = cal_model_after.intercept_, cal_model_after.coef_[0]\n",
    "brier_after = brier_score_loss(y_test['all'],y_test_proba_cal['all'])\n",
    "U_after, p_after = calc_unreliability(y_test['all'], y_test_proba_cal['all'])\n",
    "\n",
    "# Isotonic calibration (only for 'all') \n",
    "iso_calib_filename = f\"iso_{model_type}_{model_name}.pkl\"\n",
    "if os.path.exists(iso_calib_filename):\n",
    "    with open(iso_calib_filename, \"rb\") as f:\n",
    "        iso_calib = pickle.load(f)\n",
    "else:\n",
    "    iso_calib = IsotonicRegression(out_of_bounds='clip').fit(y_train_proba, y_train)\n",
    "    with open(iso_calib_filename, \"wb\") as f:\n",
    "        pickle.dump(iso_calib, f)\n",
    "y_test_proba_iso = iso_calib.predict(y_test_proba['all'])\n",
    "cal_model_iso = LinearRegression().fit(y_test_proba_iso.reshape(-1,1), y_test['all'])\n",
    "cal_intercept_iso, cal_slope_iso = cal_model_iso.intercept_, cal_model_iso.coef_[0]\n",
    "brier_iso = brier_score_loss(y_test['all'], y_test_proba_iso)\n",
    "U_iso, p_iso = calc_unreliability(y_test['all'], y_test_proba_iso)\n",
    "\n",
    "print(f\"Before Calibration:\\nCalibration Intercept: {cal_intercept:.4f}\\nCalibration Slope: {cal_slope:.4f}\\nBrier Score: {brier_before:.4f}\\nUnreliability Index: {U_before:.4f}\\nUnreliability p-value: {p_before:.4f}\")\n",
    "print(f\"\\nAfter Spline Calibration:\\nCalibration Intercept: {cal_intercept_after:.4f}\\nCalibration Slope: {cal_slope_after:.4f}\\nBrier Score: {brier_after:.4f}\\nUnreliability Index: {U_after:.4f}\\nUnreliability p-value: {p_after:.4f}\")\n",
    "print(f\"\\nAfter Isotonic Regression:\\nCalibration Intercept: {cal_intercept_iso:.4f}\\nCalibration Slope: {cal_slope_iso:.4f}\\nBrier Score: {brier_iso:.4f}\\nUnreliability Index: {U_iso:.4f}\\nUnreliability p-value: {p_iso:.4f}\\n\")\n",
    "\n",
    "# Youden threshold and predictions for all groups\n",
    "youden_filename = f\"Youden_{model_type}_{model_name}.pkl\"\n",
    "if os.path.exists(youden_filename):\n",
    "    with open(youden_filename, \"rb\") as f:\n",
    "        Youden = pickle.load(f)\n",
    "else:\n",
    "    Youden = youden(y_train, y_train_proba)\n",
    "    with open(youden_filename, \"wb\") as f:\n",
    "        pickle.dump(Youden, f)\n",
    "Youden = youden(y_train, y_train_proba)\n",
    "y_pred = {k: (y_test_proba_cal[k] > Youden).astype(int) for k in test_groups}\n",
    "\n",
    "print(f\"Youden: {Youden:.3f}\")\n",
    "# Print n filenames, n unique hids, and n unique opids for each group in the following outputs\n",
    "\n",
    "# For 'all'\n",
    "n_filename, n_hid, n_opid = get_n_filename_hid_opid(test_groups['all'])\n",
    "print(f\"<Without Calibration> (n={n_filename}, unique hids={n_hid}, unique opids={n_opid})\")\n",
    "draw_model_evaluation_plots(y_test['all'], y_test_proba['all'], y_pred['all'])\n",
    "print(f\"<With Isotonic Calibration> (n={n_filename}, unique hids={n_hid}, unique opids={n_opid})\")\n",
    "draw_model_evaluation_plots(y_test['all'], y_test_proba_iso, y_pred['all'])\n",
    "print(f\"<With Spline Calibration> (n={n_filename}, unique hids={n_hid}, unique opids={n_opid})\")\n",
    "draw_model_evaluation_plots(y_test['all'], y_test_proba_cal['all'], y_pred['all'])\n",
    "\n",
    "# For subgroups\n",
    "def print_subgroup_with_counts(label, key):\n",
    "    n_filename, n_hid, n_opid = get_n_filename_hid_opid(test_groups[key])\n",
    "    print(f\"<Subgroup: {label} (n={n_filename}, unique hids={n_hid}, unique opids={n_opid})>\")\n",
    "    draw_model_evaluation_plots(y_test[key], y_test_proba_cal[key], y_pred[key])\n",
    "\n",
    "print_subgroup_with_counts(\"General Anesthesia\", \"gen\")\n",
    "print_subgroup_with_counts(\"Regional Anesthesia\", \"reg\")\n",
    "print_subgroup_with_counts(\"EMOP=1\", \"emop1\")\n",
    "print_subgroup_with_counts(\"EMOP=0\", \"emop0\")\n",
    "\n",
    "# Only print and plot gender subgroups if 'gender' in df_test\n",
    "if 'gender' in df_test.columns:\n",
    "    print_subgroup_with_counts(\"Gender=1\", \"gender1\")\n",
    "    print_subgroup_with_counts(\"Gender=0\", \"gender0\")\n",
    "\n",
    "# Only print and plot elderly subgroups if 'age' in df_test\n",
    "if 'age' in df_test.columns:\n",
    "    print_subgroup_with_counts(\"Elderly (age > 60)\", \"elderly1\")\n",
    "    print_subgroup_with_counts(\"Not Elderly (age <= 60)\", \"elderly0\")\n",
    "\n",
    "# --- Subgroup analysis for new_dept ---\n",
    "from sklearn.utils import resample\n",
    "from scipy.stats import ttest_ind, f_oneway\n",
    "import itertools\n",
    "import statsmodels.stats.multitest as smm\n",
    "\n",
    "def bootstrap_auroc(y, y_proba, n_bootstrap=4000, random_state=42):\n",
    "    np.random.seed(random_state)\n",
    "    aucs = []\n",
    "    for _ in range(n_bootstrap):\n",
    "        idx = np.random.choice(len(y), len(y), replace=True)\n",
    "        try:\n",
    "            auc = roc_auc_score(y[idx], y_proba[idx])\n",
    "        except ValueError:\n",
    "            auc = np.nan\n",
    "        aucs.append(auc)\n",
    "    return np.array(aucs)\n",
    "\n",
    "# Compute bootstrapped AUROC for each new_dept subgroup\n",
    "auroc_bootstrap = {}\n",
    "for dept in new_dept_categories:\n",
    "    key = f'new_dept_{dept}'\n",
    "    y = y_test[key]\n",
    "    y_proba = y_test_proba_cal[key]\n",
    "    if len(np.unique(y)) < 2 or len(y) < 10:\n",
    "        print(f\"Skipping {dept} (not enough samples or only one class present)\")\n",
    "        auroc_bootstrap[dept] = np.array([np.nan]*4000)\n",
    "        continue\n",
    "    auroc_bootstrap[dept] = bootstrap_auroc(y, y_proba, n_bootstrap=4000)\n",
    "\n",
    "# Draw model evaluation for each new_dept subgroup\n",
    "print(\"\\n[NEW_DEPT SUBGROUP MODEL EVALUATION]\")\n",
    "for dept in new_dept_categories:\n",
    "    key = f'new_dept_{dept}'\n",
    "    y = y_test[key]\n",
    "    y_proba = y_test_proba_cal[key]\n",
    "    y_pred_dept = y_pred[key]\n",
    "    n_filename, n_hid, n_opid = get_n_filename_hid_opid(test_groups[key])\n",
    "    if len(np.unique(y)) < 2 or len(y) < 10:\n",
    "        print(f\"Skipping {dept} (not enough samples or only one class present)\")\n",
    "        continue\n",
    "    print(f\"<Subgroup: new_dept={dept} (n={n_filename}, unique hids={n_hid}, unique opids={n_opid})>\")\n",
    "    draw_model_evaluation_plots(y, y_proba, y_pred_dept)\n",
    "\n",
    "# One-way ANOVA across all new_dept subgroups (only those with valid AUROC)\n",
    "valid_depts = [dept for dept in new_dept_categories if not np.isnan(auroc_bootstrap[dept]).all()]\n",
    "anova_data = [auroc_bootstrap[dept] for dept in valid_depts]\n",
    "anova_stat, anova_p = f_oneway(*anova_data)\n",
    "print(\"\\n[NEW_DEPT SUBGROUP AUROC BOOTSTRAP]\")\n",
    "for dept in valid_depts:\n",
    "    print(f\"{dept}: mean AUROC={np.nanmean(auroc_bootstrap[dept]):.3f} (n={len(auroc_bootstrap[dept])})\")\n",
    "print(f\"One-way ANOVA p-value: {anova_p:.4f}\")\n",
    "\n",
    "# If ANOVA p < 0.05, do pairwise t-tests with Bonferroni correction\n",
    "if anova_p < 0.05:\n",
    "    print(\"ANOVA significant, performing pairwise t-tests (Bonferroni corrected):\")\n",
    "    pairs = list(itertools.combinations(valid_depts, 2))\n",
    "    ttest_pvals = []\n",
    "    ttest_results = []\n",
    "    for d1, d2 in pairs:\n",
    "        # Remove nan values for t-test\n",
    "        a1 = auroc_bootstrap[d1][~np.isnan(auroc_bootstrap[d1])]\n",
    "        a2 = auroc_bootstrap[d2][~np.isnan(auroc_bootstrap[d2])]\n",
    "        # If either group is empty, skip\n",
    "        if len(a1) == 0 or len(a2) == 0:\n",
    "            ttest_pvals.append(np.nan)\n",
    "            ttest_results.append((d1, d2, np.nan, np.nan))\n",
    "            continue\n",
    "        t_stat, p_val = ttest_ind(a1, a2, equal_var=False)\n",
    "        ttest_pvals.append(p_val)\n",
    "        ttest_results.append((d1, d2, t_stat, p_val))\n",
    "    # Bonferroni correction\n",
    "    reject, pvals_corrected, _, _ = smm.multipletests(ttest_pvals, alpha=0.05, method='bonferroni')\n",
    "    for i, (d1, d2, t_stat, p_val) in enumerate(ttest_results):\n",
    "        print(f\"{d1} vs {d2}: t={t_stat:.3f}, raw p={p_val:.4g}, corrected p={pvals_corrected[i]:.4g}, significant={reject[i]}\")\n",
    "else:\n",
    "    print(\"ANOVA not significant, no pairwise t-tests performed.\")\n",
    "\n",
    "# --- Additional: t-test between bootstrapped AUROC in other subgroup categories (anetype, emop, age, sex, ...) ---\n",
    "\n",
    "def print_ttest_bootstrap_auroc(subgroup1, subgroup2, label1, label2):\n",
    "    # Only run if both subgroups have at least 2 classes and enough samples\n",
    "    y1, y2 = y_test[subgroup1], y_test[subgroup2]\n",
    "    proba1, proba2 = y_test_proba_cal[subgroup1], y_test_proba_cal[subgroup2]\n",
    "    if len(np.unique(y1)) < 2 or len(y1) < 10 or len(np.unique(y2)) < 2 or len(y2) < 10:\n",
    "        print(f\"Skipping {label1} vs {label2} (not enough samples or only one class present)\")\n",
    "        return\n",
    "    aucs1 = bootstrap_auroc(y1, proba1, n_bootstrap=4000)\n",
    "    aucs2 = bootstrap_auroc(y2, proba2, n_bootstrap=4000)\n",
    "    t_stat, p_val = ttest_ind(aucs1, aucs2, equal_var=False)\n",
    "    print(f\"AUROC bootstrap t-test: {label1} vs {label2}: t={t_stat:.3f}, p={p_val:.4g}, mean1={np.nanmean(aucs1):.3f}, mean2={np.nanmean(aucs2):.3f}\")\n",
    "\n",
    "print(\"\\n[SUBGROUP AUROC BOOTSTRAP T-TESTS]\")\n",
    "\n",
    "# Anesthesia type\n",
    "print_ttest_bootstrap_auroc('gen', 'reg', 'General', 'Regional')\n",
    "\n",
    "# EMOP\n",
    "print_ttest_bootstrap_auroc('emop1', 'emop0', 'EMOP=1', 'EMOP=0')\n",
    "\n",
    "# Gender\n",
    "if 'gender1' in y_test and 'gender0' in y_test:\n",
    "    print_ttest_bootstrap_auroc('gender1', 'gender0', 'Gender=1', 'Gender=0')\n",
    "\n",
    "# Elderly\n",
    "if 'elderly1' in y_test and 'elderly0' in y_test:\n",
    "    print_ttest_bootstrap_auroc('elderly1', 'elderly0', 'Elderly (age>60)', 'Not Elderly (age<=60)')\n",
    "\n",
    "with open(f'Youden_{model_name}.pkl', 'wb') as f:\n",
    "    pickle.dump(Youden, f)\n",
    "with open(f'y_test_{model_type}_{model_name}.pkl', 'wb') as f:\n",
    "    pickle.dump(y_test['all'], f)\n",
    "with open(f'y_test_proba_{model_type}_{model_name}.pkl', 'wb') as f:\n",
    "    pickle.dump(y_test_proba_cal['all'], f)\n",
    "\n",
    "with open(f'y_test_proba_xgb_age_gender_p_o_a.pkl', 'rb') as f:\n",
    "    y_test_proba_xgb_age_gender_p_o_a = pickle.load(f)\n",
    "\n",
    "print(f\"\\nDelong's P: {delong_roc_test(y_test['all'], y_test_proba_xgb_age_gender_p_o_a, y_test_proba_cal['all'])[0][0]:.3f}\")\n",
    "print(f\"AUPRC (Paired T-test): {auprc_test(y_test['all'], y_test_proba_xgb_age_gender_p_o_a, y_test_proba_cal['all']):.3f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multimodal GBM, Baseline GBM, ECG GBM, ASA, RCRI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_labels_gbm_hidden_icd.csv')\n",
    "df_catcol = ['gender','p','o','a'] #'gender','asa','p','o','a'\n",
    "df_numcol = ['age'] #'age','andur'\n",
    "df_numcol.extend([f'hidden_{i}' for i in range(HIDDEN_SIZE)])\n",
    "labels = 'label'\n",
    "model_name = 'age_gender_p_o_a'\n",
    "\n",
    "onehot_cols = ['p', 'o', 'a']\n",
    "onehot_df = pd.get_dummies(df[onehot_cols], prefix=onehot_cols).astype(float)\n",
    "df = df.drop(columns=onehot_cols)\n",
    "df = pd.concat([df, onehot_df], axis=1)\n",
    "df['gender'] = df['gender'].astype(float)\n",
    "df = df[['gender'] + list(onehot_df.columns) + df_numcol + ['label', 'hid', 'filename','asa']]\n",
    "df['label'] = df['label'].replace({3: 1})\n",
    "\n",
    "rcri = pd.read_csv('rcri_missing_2.csv')\n",
    "opid = pd.read_csv('opid_icd_matching.csv')\n",
    "df['opid'] = df['filename'].map(dict(zip(opid['filename'], opid['opid'])))\n",
    "df['rcri'] = df['opid'].map(dict(zip(rcri['opid'], rcri['rcri_score'])))\n",
    "df = df[df['rcri'].notna()]\n",
    "\n",
    "test_filenames = np.load('test_filenames.npy', allow_pickle=True)\n",
    "fold_filenames = [np.load(f'fold_{i}_filenames.npy', allow_pickle=True) for i in range(1, 6)]\n",
    "develop_filenames = np.concatenate(fold_filenames)\n",
    "df_train = df[df['filename'].isin(develop_filenames)]\n",
    "df_test = df[df['filename'].isin(test_filenames)]\n",
    "df_train.drop(columns=['filename'], inplace=True)\n",
    "df_test.drop(columns=['filename'], inplace=True)\n",
    "\n",
    "with open(f'y_test_proba_xgb_age_gender_p_o_a.pkl', 'rb') as f:\n",
    "    df_test['y_test_proba_xgb_age_gender_p_o_a'] = pickle.load(f)\n",
    "with open(f'y_test_proba_xgb_wo_cnn_age_gender_p_o_a.pkl', 'rb') as f:\n",
    "    df_test['y_test_proba_xgb_wo_cnn_age_gender_p_o_a'] = pickle.load(f)\n",
    "\n",
    "#ASA\n",
    "print(\"<ASA>\")\n",
    "Youden_ASA = youden(df_train['label'], df_train['asa'])\n",
    "print(f\"Youden ASA: {Youden_ASA}\")\n",
    "df_test['asa_pred'] = [1 if value >= Youden_ASA else 0 for value in df_test['asa']]\n",
    "draw_model_evaluation_plots(df_test['label'].ravel(),df_test['asa'].ravel() , df_test['asa_pred'].ravel())\n",
    "print(f\"ASA (Delong's P): {delong_roc_test(df_test['label'], df_test['y_test_proba_xgb_age_gender_p_o_a'], df_test['asa'])[0][0]:.3f}\")\n",
    "print(f\"ASA (AUPRC): {auprc_test(df_test['label'], df_test['y_test_proba_xgb_age_gender_p_o_a'], df_test['asa']):.3f}\\n\")\n",
    "#RCRI\n",
    "print(\"<RCRI>\")\n",
    "Youden_RCRI = youden(df_train['label'], df_train['rcri'])\n",
    "print(f\"Youden RCRI: {Youden_RCRI}\")\n",
    "df_test['rcri_pred'] = [1 if value >= Youden_RCRI else 0 for value in df_test['rcri']]\n",
    "draw_model_evaluation_plots(df_test['label'].ravel(),df_test['rcri'].ravel() , df_test['rcri_pred'].ravel())\n",
    "print(f\"RCRI (Delong's P): {delong_roc_test(df_test['label'], df_test['y_test_proba_xgb_age_gender_p_o_a'], df_test['rcri'])[0][0]:.3f}\")\n",
    "print(f\"RCRI (AUPRC): {auprc_test(df_test['label'], df_test['y_test_proba_xgb_age_gender_p_o_a'], df_test['rcri']):.3f}\\n\")\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "def plot_roc_with_ci(y_true, y_score, color, label, n_bootstraps=1000, seed=42):\n",
    "    from plot_model import normal_ci  # assumes normal_ci is available as in your context\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    # Calculate 95% CI for AUROC using normal approximation\n",
    "    ci_metrics, _ = normal_ci(np.array(y_true), np.array(y_score), (np.array(y_score) >= 0.5).astype(int))\n",
    "    auroc_ci = ci_metrics.get(\"AUROC\", (np.nan, np.nan))\n",
    "    plt.plot(\n",
    "        fpr, tpr, color=color, lw=2,\n",
    "        label=f'{label} (AUC = {roc_auc:.3f}, 95% CI: {auroc_ci[0]:.3f}-{auroc_ci[1]:.3f})'\n",
    "    )\n",
    "\n",
    "    # Bootstrap 95% CI for ROC curve\n",
    "    rng = np.random.RandomState(seed)\n",
    "    bootstrapped_tprs = []\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    for i in range(n_bootstraps):\n",
    "        indices = rng.randint(0, len(y_true), len(y_true))\n",
    "        if len(np.unique(y_true.iloc[indices] if hasattr(y_true, \"iloc\") else y_true[indices])) < 2:\n",
    "            continue\n",
    "        fpr_boot, tpr_boot, _ = roc_curve(\n",
    "            y_true.iloc[indices] if hasattr(y_true, \"iloc\") else y_true[indices],\n",
    "            y_score.iloc[indices] if hasattr(y_score, \"iloc\") else y_score[indices]\n",
    "        )\n",
    "        tpr_interp = np.interp(mean_fpr, fpr_boot, tpr_boot)\n",
    "        tpr_interp[0] = 0.0\n",
    "        bootstrapped_tprs.append(tpr_interp)\n",
    "    if len(bootstrapped_tprs) > 0:\n",
    "        bootstrapped_tprs = np.array(bootstrapped_tprs)\n",
    "        tpr_lower = np.percentile(bootstrapped_tprs, 2.5, axis=0)\n",
    "        tpr_upper = np.percentile(bootstrapped_tprs, 97.5, axis=0)\n",
    "        plt.fill_between(mean_fpr, tpr_lower, tpr_upper, color=color, alpha=0.2)\n",
    "\n",
    "# Multimodal GBM\n",
    "plot_roc_with_ci(df_test['label'], df_test['y_test_proba_xgb_age_gender_p_o_a'], color='blue', label='Multimodal GBM')\n",
    "# Baseline GBM\n",
    "plot_roc_with_ci(df_test['label'], df_test['y_test_proba_xgb_wo_cnn_age_gender_p_o_a'], color='orange', label='Baseline GBM')\n",
    "# RCRI\n",
    "plot_roc_with_ci(df_test['label'], df_test['rcri'], color='red', label='RCRI')\n",
    "# ASA\n",
    "plot_roc_with_ci(df_test['label'], df_test['asa'], color='purple', label='ASA')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=1)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multimodal GBM, TnI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_labels_gbm_hidden_icd.csv')\n",
    "df = df.loc[df['tni'].notna()]\n",
    "df_col = []\n",
    "\n",
    "labels = 'label'\n",
    "model_name = 'age_gender_p_o_a'  # wo_cnn_, mach_/ age,andur, asa, gender, p_o_a\n",
    "df['gender'] = df['gender'].astype(float)\n",
    "for x in ['gender', 'asa', 'age', 'andur']:\n",
    "    if x in model_name:\n",
    "        df_col.extend([x])\n",
    "if 'wo_cnn' not in model_name:\n",
    "    if 'mach_' in model_name:\n",
    "        df_col.extend([col for col in df.columns if 'mach_' in col])\n",
    "    else:\n",
    "        df_col.extend([f'hidden_{i}' for i in range(HIDDEN_SIZE)])\n",
    "if 'p_o_a' in model_name:\n",
    "    onehot_cols = ['p', 'o', 'a']\n",
    "    onehot_df = pd.get_dummies(df[onehot_cols], prefix=onehot_cols).astype(float)\n",
    "    df = df.drop(columns=onehot_cols)\n",
    "    df = pd.concat([df, onehot_df], axis=1)\n",
    "    df = df[df_col + list(onehot_df.columns) + ['label', 'hid', 'filename']]\n",
    "else:\n",
    "    df = df[df_col + ['label', 'hid', 'filename']]\n",
    "df['label'] = df['label'].replace({3: 1})\n",
    "\n",
    "tni = pd.read_csv('opid_troponin_orig.csv')\n",
    "opid = pd.read_csv('opid_icd_matching.csv')\n",
    "df['opid'] = df['filename'].map(dict(zip(opid['filename'], opid['opid'])))\n",
    "df['tni'] = df['opid'].map(dict(zip(tni['opid'], tni['pre_troponin_I'])))\n",
    "\n",
    "unique_hids = df['hid'].unique()\n",
    "train_hids, test_hids = train_test_split(unique_hids, test_size=0.1, random_state=23, shuffle=True)\n",
    "\n",
    "df_train = df[df['hid'].isin(train_hids)].copy()\n",
    "df_test = df[df['hid'].isin(test_hids)].copy()\n",
    "\n",
    "test_filenames = np.array(df_test['filename'].unique())\n",
    "\n",
    "df = df[df['tni'].notna()]\n",
    "df_train_tni = df.loc[~df['filename'].isin(test_filenames)]\n",
    "df_test = df.loc[df['filename'].isin(test_filenames)]\n",
    "\n",
    "selected_features = np.load('selected_features.npy', allow_pickle=True)\n",
    "with open(f\"best_params_xgb_{model_name}.json\", 'r') as f:\n",
    "    params = json.load(f)\n",
    "\n",
    "# Model 1: GBM-TnI (without tni)\n",
    "best_model = xgb.XGBClassifier(**params)\n",
    "X_train = df_train.drop(['label', 'hid', 'filename', 'tni', 'opid'], axis=1)\n",
    "X_train = X_train[selected_features]\n",
    "y_train = df_train['label'].ravel()\n",
    "best_model.fit(X_train, y_train)\n",
    "y_train_proba = best_model.predict_proba(X_train)[:,1].ravel()\n",
    "\n",
    "# Model 2: GBM-TnI + tni as input\n",
    "best_model_tni = xgb.XGBClassifier(**params)\n",
    "X_train_tni = df_train.drop(['label', 'hid', 'filename', 'opid'], axis=1)  # keep tni\n",
    "X_train_tni = X_train_tni[np.append(selected_features, 'tni')]\n",
    "best_model_tni.fit(X_train_tni, y_train)\n",
    "y_train_proba_tni = best_model_tni.predict_proba(X_train_tni)[:,1].ravel()\n",
    "\n",
    "# Model 3: TnI only\n",
    "y_train_tni = df_train_tni['label'].ravel()\n",
    "df_train.drop(columns=['filename'], inplace=True)\n",
    "df_test.drop(columns=['filename'], inplace=True)\n",
    "\n",
    "print(f\"Number of test set: {len(df_test)}\")\n",
    "\n",
    "# Prepare test sets\n",
    "X_test = df_test.drop(['label', 'hid','tni','opid'], axis=1)\n",
    "X_test_tni = df_test.drop(['label', 'hid','opid'], axis=1)  # keep tni\n",
    "X_test = X_test[selected_features]\n",
    "X_test_tni = X_test_tni[np.append(selected_features, 'tni')]\n",
    "y_test = df_test['label'].ravel()\n",
    "y_test_proba = best_model.predict_proba(X_test)[:,1].ravel()\n",
    "y_test_proba_tni = best_model_tni.predict_proba(X_test_tni)[:,1].ravel()\n",
    "\n",
    "# TnI only\n",
    "calib_tni = mli.SplineCalib(unity_prior=False, unity_prior_weight=100, random_state=42, max_iter=500)\n",
    "calib_tni.fit(df_train_tni['tni'].ravel(), y_train_tni)\n",
    "y_test_proba_cal_tni = calib_tni.calibrate(df_test['tni'])\n",
    "\n",
    "# Calibrate both GBM models\n",
    "calib = mli.SplineCalib(unity_prior=False, unity_prior_weight=100, random_state=42, max_iter=500)\n",
    "calib.fit(y_train_proba, y_train)\n",
    "y_test_proba_cal = calib.calibrate(y_test_proba)\n",
    "\n",
    "calib_gbm_tni = mli.SplineCalib(unity_prior=False, unity_prior_weight=100, random_state=42, max_iter=500)\n",
    "calib_gbm_tni.fit(y_train_proba_tni, y_train)\n",
    "y_test_proba_cal_gbm_tni = calib_gbm_tni.calibrate(y_test_proba_tni)\n",
    "\n",
    "# Youden thresholds\n",
    "Youden = youden(y_train, y_train_proba)\n",
    "Youden_gbm_tni = youden(y_train, y_train_proba_tni)\n",
    "Youden_tni = youden(y_train_tni, df_train_tni['tni'])\n",
    "\n",
    "y_pred = (y_test_proba_cal > Youden).astype(int)\n",
    "y_pred_gbm_tni = (y_test_proba_cal_gbm_tni > Youden_gbm_tni).astype(int)\n",
    "y_pred_tni = (y_test_proba_cal_tni > Youden_tni).astype(int)\n",
    "\n",
    "print(f\"Youden (GBM-TS): {Youden:.3f}\")\n",
    "print(f\"Youden (GBM-TS+TnI): {Youden_gbm_tni:.3f}\")\n",
    "print(f\"Youden (TnI): {Youden_tni:.3f}\")\n",
    "\n",
    "print(\"<GBM-TS>\")\n",
    "draw_model_evaluation_plots(y_test, y_test_proba_cal, y_pred)\n",
    "print(\"<GBM-TS+TnI>\")\n",
    "draw_model_evaluation_plots(y_test, y_test_proba_cal_gbm_tni, y_pred_gbm_tni)\n",
    "print(\"<Troponin I>\")\n",
    "draw_model_evaluation_plots(y_test, y_test_proba_cal_tni, y_pred_tni, draw=False)\n",
    "\n",
    "# --- Decision Curve Analysis (DCA) for GBM-TS, GBM-TS+TnI, and TnI only ---\n",
    "\n",
    "# Net Benefit calculation function\n",
    "def decision_curve_analysis(y_true, prob_model, thresholds=np.linspace(0.001, 0.5, 1000)):\n",
    "    net_benefit = []\n",
    "    n = len(y_true)\n",
    "    for thresh in thresholds:\n",
    "        pred = (prob_model >= thresh).astype(int)\n",
    "        tp = np.sum((pred == 1) & (y_true == 1))\n",
    "        fp = np.sum((pred == 1) & (y_true == 0))\n",
    "        nb = (tp / n) - (fp / n) * (thresh / (1 - thresh))\n",
    "        net_benefit.append(nb)\n",
    "    return thresholds, np.array(net_benefit)\n",
    "\n",
    "# Treat All Net Benefit calculation (skip values below -0.005)\n",
    "def treat_all_net_benefit(y_true, thresholds):\n",
    "    prevalence = np.mean(y_true)\n",
    "    nb = prevalence - (1 - prevalence) * (thresholds / (1 - thresholds))\n",
    "    nb = np.where(nb >= -0.005, nb, np.nan)\n",
    "    return nb\n",
    "\n",
    "# DCA for GBM-TS, GBM-TS+TnI, and TnI only\n",
    "thresholds = np.linspace(0.001, 0.5, 1000)\n",
    "y_true = y_test\n",
    "\n",
    "# Use calibrated probabilities\n",
    "probs_gbm = y_test_proba_cal\n",
    "probs_gbm_tni = y_test_proba_cal_gbm_tni\n",
    "probs_tni = y_test_proba_cal_tni\n",
    "\n",
    "# Calculate net benefit\n",
    "thresholds, nb_gbm = decision_curve_analysis(y_true, probs_gbm, thresholds)\n",
    "_, nb_gbm_tni = decision_curve_analysis(y_true, probs_gbm_tni, thresholds)\n",
    "_, nb_tni = decision_curve_analysis(y_true, probs_tni, thresholds)\n",
    "nb_treat_all = treat_all_net_benefit(y_true, thresholds)\n",
    "nb_treat_none = np.zeros_like(thresholds)\n",
    "\n",
    "# Plot DCA\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(thresholds, nb_gbm, color='#0044cc', linestyle='-', linewidth=2.5, label='GBM-TS')\n",
    "plt.plot(thresholds, nb_gbm_tni, color='#009933', linestyle='--', linewidth=2.5, label='GBM-TS+TnI')\n",
    "plt.plot(thresholds, nb_tni, color='#ff9900', linestyle='-.', linewidth=2.5, label='TnI only')\n",
    "plt.plot(thresholds, nb_treat_all, color='black', linestyle='-', linewidth=2.5, label='Treat All')\n",
    "plt.plot(thresholds, nb_treat_none, color='dimgray', linestyle=(0, (3, 3, 1, 3)), linewidth=2.5, label='Treat None')\n",
    "\n",
    "plt.xlabel('Threshold Probability', fontsize=13)\n",
    "plt.ylabel('Net Benefit', fontsize=13)\n",
    "plt.title('Decision Curve Analysis', fontsize=15)\n",
    "plt.legend(loc='lower right', fontsize=12, frameon=True, facecolor='white', edgecolor='black')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim([0, 0.5])\n",
    "plt.ylim(bottom=np.nanmin([nb_gbm, nb_gbm_tni, nb_tni, nb_treat_all])-0.01)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- End DCA ---\n",
    "\n",
    "def bootstrap_roc_ci(y_true, y_score, n_bootstraps=1000, seed=42, fpr_grid=None):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    bootstrapped_tprs = []\n",
    "    bootstrapped_aucs = []\n",
    "    if fpr_grid is None:\n",
    "        fpr_grid = np.linspace(0, 1, 100)\n",
    "    for i in range(n_bootstraps):\n",
    "        # bootstrap by sampling with replacement\n",
    "        indices = rng.randint(0, len(y_true), len(y_true))\n",
    "        if len(np.unique(y_true[indices])) < 2:\n",
    "            # We need at least one positive and one negative sample for ROC\n",
    "            continue\n",
    "        fpr, tpr, _ = roc_curve(y_true[indices], y_score[indices])\n",
    "        auc_score = auc(fpr, tpr)\n",
    "        bootstrapped_aucs.append(auc_score)\n",
    "        # Interpolate tpr at fpr_grid\n",
    "        tpr_interp = np.interp(fpr_grid, fpr, tpr)\n",
    "        bootstrapped_tprs.append(tpr_interp)\n",
    "    bootstrapped_tprs = np.array(bootstrapped_tprs)\n",
    "    tpr_mean = np.mean(bootstrapped_tprs, axis=0)\n",
    "    tpr_lower = np.percentile(bootstrapped_tprs, 2.5, axis=0)\n",
    "    tpr_upper = np.percentile(bootstrapped_tprs, 97.5, axis=0)\n",
    "    auc_mean = np.mean(bootstrapped_aucs)\n",
    "    auc_lower = np.percentile(bootstrapped_aucs, 2.5)\n",
    "    auc_upper = np.percentile(bootstrapped_aucs, 97.5)\n",
    "    return fpr_grid, tpr_mean, tpr_lower, tpr_upper, auc_mean, auc_lower, auc_upper\n",
    "\n",
    "# Compute ROC and bootstrap CIs for all three models\n",
    "fpr_grid = np.linspace(0, 1, 100)\n",
    "fpr_gbm, tpr_gbm, _ = roc_curve(y_test, y_test_proba_cal)\n",
    "roc_auc_gbm = auc(fpr_gbm, tpr_gbm)\n",
    "fpr_gbm_tni, tpr_gbm_tni, _ = roc_curve(y_test, y_test_proba_cal_gbm_tni)\n",
    "roc_auc_gbm_tni = auc(fpr_gbm_tni, tpr_gbm_tni)\n",
    "fpr_tni, tpr_tni, _ = roc_curve(y_test, y_test_proba_cal_tni)\n",
    "roc_auc_tni = auc(fpr_tni, tpr_tni)\n",
    "\n",
    "# Bootstrapping for 95% CI\n",
    "fpr_grid, tpr_gbm_mean, tpr_gbm_lower, tpr_gbm_upper, auc_gbm_mean, auc_gbm_lower, auc_gbm_upper = bootstrap_roc_ci(\n",
    "    np.array(y_test), np.array(y_test_proba_cal), n_bootstraps=1000, fpr_grid=fpr_grid\n",
    ")\n",
    "_, tpr_gbm_tni_mean, tpr_gbm_tni_lower, tpr_gbm_tni_upper, auc_gbm_tni_mean, auc_gbm_tni_lower, auc_gbm_tni_upper = bootstrap_roc_ci(\n",
    "    np.array(y_test), np.array(y_test_proba_cal_gbm_tni), n_bootstraps=1000, fpr_grid=fpr_grid\n",
    ")\n",
    "_, tpr_tni_mean, tpr_tni_lower, tpr_tni_upper, auc_tni_mean, auc_tni_lower, auc_tni_upper = bootstrap_roc_ci(\n",
    "    np.array(y_test), np.array(y_test_proba_cal_tni), n_bootstraps=1000, fpr_grid=fpr_grid\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(\n",
    "    fpr_grid, tpr_gbm_mean, color='blue', lw=2,\n",
    "    label=f\"GBM-TS (AUROC = {roc_auc_gbm:.3f}, 95% CI: {auc_gbm_lower:.3f}-{auc_gbm_upper:.3f})\"\n",
    ")\n",
    "plt.fill_between(fpr_grid, tpr_gbm_lower, tpr_gbm_upper, color='blue', alpha=0.2)\n",
    "plt.plot(\n",
    "    fpr_grid, tpr_gbm_tni_mean, color='green', lw=2,\n",
    "    label=f\"GBM-TS+TnI (AUROC = {roc_auc_gbm_tni:.3f}, 95% CI: {auc_gbm_tni_lower:.3f}-{auc_gbm_tni_upper:.3f})\"\n",
    ")\n",
    "plt.fill_between(fpr_grid, tpr_gbm_tni_lower, tpr_gbm_tni_upper, color='green', alpha=0.2)\n",
    "plt.plot(\n",
    "    fpr_grid, tpr_tni_mean, color='orange', lw=2,\n",
    "    label=f\"TnI (AUROC = {roc_auc_tni:.3f}, 95% CI: {auc_tni_lower:.3f}-{auc_tni_upper:.3f})\"\n",
    ")\n",
    "plt.fill_between(fpr_grid, tpr_tni_lower, tpr_tni_upper, color='orange', alpha=0.2)\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "print(f\"Delong's P (GBM-TS vs GBM-TS+TnI): {delong_roc_test(y_test, y_test_proba_cal, y_test_proba_cal_gbm_tni)[0][0]:.3f}\")\n",
    "print(f\"Delong's P (GBM-TS vs TnI): {delong_roc_test(y_test, y_test_proba_cal, y_test_proba_cal_tni)[0][0]:.3f}\")\n",
    "print(f\"Delong's P (GBM-TS+TnI vs TnI): {delong_roc_test(y_test, y_test_proba_cal_gbm_tni, y_test_proba_cal_tni)[0][0]:.3f}\")\n",
    "print(f\"AUPRC P (GBM-TS vs GBM-TS+TnI): {auprc_test(y_test, y_test_proba_cal, y_test_proba_cal_gbm_tni):.3f}\")\n",
    "print(f\"AUPRC P (GBM-TS vs TnI): {auprc_test(y_test, y_test_proba_cal, y_test_proba_cal_tni):.3f}\")\n",
    "print(f\"AUPRC P (GBM-TS+TnI vs TnI): {auprc_test(y_test, y_test_proba_cal_gbm_tni, y_test_proba_cal_tni):.3f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "240305",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
