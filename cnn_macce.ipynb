{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import gc\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import warnings\n",
    "from math import sqrt\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import ml_insights as mli\n",
    "\n",
    "# Data processing\n",
    "import pandas as pd\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.metrics import (\n",
    "    average_precision_score, roc_curve, \n",
    "    roc_auc_score, accuracy_score, f1_score\n",
    ")\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.calibration import IsotonicRegression\n",
    "from sklearn.metrics import brier_score_loss\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy.stats import chi2\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "\n",
    "# Visualization\n",
    "import shap\n",
    "import optuna\n",
    "\n",
    "# Custom imports\n",
    "from model_code import *\n",
    "from model.blocks import FinalModel\n",
    "from team_code import *\n",
    "from helper_code import *\n",
    "from plot_model import *\n",
    "from delong import *\n",
    "\n",
    "# Settings\n",
    "np.random.seed(0)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "shap.initjs()\n",
    "\n",
    "'''\n",
    "# Imports for ICD 10 code extraction\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import json\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify CustomCNN to take dropout_rate as an argument\n",
    "def collate_fn(batch):\n",
    "    inputs = torch.stack([torch.tensor(i[0], dtype=torch.float32) for i in batch])\n",
    "    targets = torch.stack([torch.tensor(i[1], dtype=torch.float32) for i in batch])\n",
    "    age = torch.tensor([[i[3]] for i in batch], dtype=torch.float32)\n",
    "    gender = torch.tensor([[i[4]] for i in batch], dtype=torch.float32)\n",
    "\n",
    "    return inputs, targets, age, gender\n",
    "\n",
    "## Youden index\n",
    "def youden(y_true, y_score):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
    "    idx = np.argmax(tpr - fpr)\n",
    "    return thresholds[idx]\n",
    "\n",
    "def roc_auc_ci(y_true, y_score, positive=1):\n",
    "    AUC = roc_auc_score(y_true, y_score)\n",
    "    N1 = sum(y_true == positive)\n",
    "    N2 = sum(y_true != positive)\n",
    "    Q1 = AUC / (2 - AUC)\n",
    "    Q2 = 2*AUC**2 / (1 + AUC)\n",
    "    SE_AUC = sqrt((AUC*(1 - AUC) + (N1 - 1)*(Q1 - AUC**2) + (N2 - 1)*(Q2 - AUC**2)) / (N1*N2))\n",
    "    lower = AUC - 1.96*SE_AUC\n",
    "    upper = AUC + 1.96*SE_AUC\n",
    "    if lower < 0:\n",
    "        lower = 0\n",
    "    if upper > 1:\n",
    "        upper = 1\n",
    "    return [lower, upper]\n",
    "\n",
    "# Function to train the model\n",
    "def train_model(train_loader, model, criterion, optimizer, scheduler, device=DEVICE):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    n_batch = len(train_loader)\n",
    "    pbar = tqdm(enumerate(train_loader), total=n_batch)\n",
    "\n",
    "    for i_batch, (inputs, labels, age, gender) in pbar:\n",
    "        inputs, labels, age, gender = inputs.to(device), labels.to(device), age.to(device), gender.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs,age,gender)\n",
    "        labels = labels[:, 1].unsqueeze(1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        pbar.set_description(f'Training ... {1 + i_batch}/{n_batch}')\n",
    "        \n",
    "    #scheduler.step()\n",
    "    return running_loss / len(train_loader)\n",
    "\n",
    "def evaluate_model(valid_loader, model, device=DEVICE):\n",
    "    model.eval()\n",
    "    model.return_hidden = True  # Enable hidden state return\n",
    "    y_true, y_pred, hidden_states = None, None, None\n",
    "    n_batch = len(valid_loader)\n",
    "    pbar = tqdm(enumerate(valid_loader), total=n_batch)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i_batch, (X_batch, y_true_batch, age_batch, gender_batch) in pbar:\n",
    "            X_batch, y_true_batch = X_batch.to(device), y_true_batch.to(device)\n",
    "            age_batch, gender_batch = age_batch.to(device), gender_batch.to(device)\n",
    "            y_true_batch = y_true_batch[:, 1].unsqueeze(1)\n",
    "            \n",
    "            y_pred_batch, hidden_batch = model(X_batch, age_batch, gender_batch)\n",
    "            y_pred_batch = F.sigmoid(y_pred_batch)\n",
    "            \n",
    "            # Convert to numpy arrays\n",
    "            y_pred_batch = y_pred_batch.cpu().numpy().reshape((-1, 1))\n",
    "            y_true_batch = y_true_batch.cpu().numpy().reshape((-1, 1))\n",
    "            hidden_batch = hidden_batch.cpu().numpy()\n",
    "\n",
    "            # Concatenate results\n",
    "            if y_pred is None:\n",
    "                y_pred = y_pred_batch\n",
    "                y_true = y_true_batch\n",
    "                hidden_states = hidden_batch\n",
    "            else:\n",
    "                y_pred = np.r_[y_pred, y_pred_batch]\n",
    "                y_true = np.r_[y_true, y_true_batch]\n",
    "                hidden_states = np.r_[hidden_states, hidden_batch]\n",
    "                \n",
    "            pbar.set_description(f'Evaluating ... {1 + i_batch}/{n_batch}')\n",
    "    \n",
    "    model.return_hidden = False  # Reset hidden state return\n",
    "    return y_true, y_pred, hidden_states\n",
    "\n",
    "# Random search\n",
    "def loguniform(low, high, size=None):\n",
    "    return np.exp(np.random.uniform(np.log(low), np.log(high), size))\n",
    "\n",
    "def calc_unreliability(y_true, y_pred):\n",
    "    # Add small epsilon to avoid log(0)\n",
    "    eps = 1e-15\n",
    "    y_pred = np.clip(y_pred, eps, 1-eps)\n",
    "    \n",
    "    # Log likelihood of uncalibrated predictions\n",
    "    ll_uncal = np.sum(y_true * np.log(y_pred) + (1-y_true) * np.log(1-y_pred))\n",
    "    \n",
    "    # Fit calibration model\n",
    "    cal_model = LinearRegression()\n",
    "    cal_model.fit(y_pred.reshape(-1,1), y_true)\n",
    "    y_cal = cal_model.predict(y_pred.reshape(-1,1))\n",
    "    y_cal = np.clip(y_cal, eps, 1-eps)\n",
    "    \n",
    "    # Log likelihood of calibrated predictions\n",
    "    ll_cal = np.sum(y_true * np.log(y_cal) + (1-y_true) * np.log(1-y_cal))\n",
    "    \n",
    "    # Calculate U statistic\n",
    "    U = -2 * (ll_uncal - ll_cal) / len(y_true)\n",
    "    \n",
    "    # Calculate p-value using chi-square distribution with 2 df\n",
    "\n",
    "    p_value = 1 - chi2.cdf(U * len(y_true), df=2)\n",
    "    \n",
    "    return U, p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "RANDOM_SEED = 98\n",
    "def seed_everything(random_seed):\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(random_seed)\n",
    "\n",
    "seed_everything(RANDOM_SEED)\n",
    "# ==================================================\n",
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "N_TRIAL                 = 50\n",
    "N_FOLD                  = 5\n",
    "N_EPOCH                 = 100\n",
    "\n",
    "# Train hyperparams\n",
    "BATCH_SIZE_LIST_GLOBAL  = [32,64,128]\n",
    "\n",
    "LR_INITIAL_LO           = 5e-4\n",
    "LR_INITIAL_HI           = 5e-3\n",
    "\n",
    "LR_STEP_SIZE_LO         = 2\n",
    "LR_STEP_SIZE_HI         = 3\n",
    "LR_STEP_GAMMA_LO        = 0.05\n",
    "LR_STEP_GAMMA_HI        = 0.3\n",
    "\n",
    "EARLY_STOP_PATIENCE_LO  = 3\n",
    "EARLY_STOP_PATIENCE_HI  = 4\n",
    "\n",
    "# Architecture hyperparams\n",
    "BLOCK_SIZE_GLOBAL = [12,16,24]\n",
    "BLOCK_DEPTH_GLOBAL = [3,4]\n",
    "BLOCK_LAYERS_GLOBAL = [3,4]\n",
    "HIDDEN_SIZE_GLOBAL = [32,64,128]\n",
    "KERNEL_NUM_GLOBAL = [5,7,9]\n",
    "# =================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, df_train, df_test, model_name, cv=N_FOLD):\n",
    "    param_grid = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'tree_method': 'gpu_hist',\n",
    "        'predictor': 'gpu_predictor',\n",
    "        'scale_pos_weight': trial.suggest_int('scale_pos_weight', 1,70,3), #trial.suggest_int('scale_pos_weight', 1,70,3) \n",
    "        'tweedie_variance_power': trial.suggest_discrete_uniform('tweedie_variance_power', 1.0, 2.0, 0.1),\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 10), # Extremely prone to overfitting! \n",
    "        'n_estimators': trial.suggest_int('n_estimators', 400, 2000, 200), # Extremely prone to overfitting!\n",
    "        'eta': trial.suggest_float('eta', 0.007, 0.013), # Most important parameter. \n",
    "        'subsample': trial.suggest_discrete_uniform('subsample', 0.2, 0.9, 0.1), \n",
    "        'colsample_bytree': trial.suggest_discrete_uniform('colsample_bytree', 0.2, 0.9, 0.1),\n",
    "        'colsample_bylevel': trial.suggest_discrete_uniform('colsample_bylevel', 0.2, 0.9, 0.1),\n",
    "        'min_child_weight': trial.suggest_loguniform('min_child_weight', 1e-4, 1e4), # I've had trouble with LB score until tuning this.\n",
    "        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-4, 1e4), # L2 regularization\n",
    "        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-4, 1e4), # L1 regularization\n",
    "        #'max_delta_step': trial.suggest_float('max_delta_step', 0, 1e1), \n",
    "        'gamma': trial.suggest_loguniform('gamma', 1e-4, 1e4),\n",
    "        'random_state': RANDOM_SEED,\n",
    "        'enable_categorical': True\n",
    "    }\n",
    "\n",
    "    # Split by patient\n",
    "    h = df_train['hid'].unique()\n",
    "    groups = np.array(range(len(h)))\n",
    "    h_shuffled, groups_shuffled = shuffle(h, groups, random_state=RANDOM_SEED)\n",
    "\n",
    "    #X_shuffled, y_shuffled, groups_shuffled = shuffle(X, y, groups, random_state=SEED)\n",
    "    group_kf = GroupKFold(n_splits=cv)\n",
    "    cv_scores = np.empty(cv)\n",
    "    best_models = []\n",
    "\n",
    "    X_test, y_test = df_test.drop(columns=['hid','label']), df_test['label'].values\n",
    "    y_pred_proba = []\n",
    "\n",
    "    for idx, (train_h, val_h) in enumerate(group_kf.split(h_shuffled, groups=groups_shuffled)):\n",
    "        X_train, X_valid = df_train[df_train['hid'].isin(h_shuffled[train_h])].drop(columns=['hid','label']), df_train[df_train['hid'].isin(h_shuffled[val_h])].drop(columns=['hid','label'])\n",
    "        y_train, y_valid = df_train.loc[df_train['hid'].isin(h_shuffled[train_h]),'label'].values, df_train.loc[df_train['hid'].isin(h_shuffled[val_h]),'label'].values\n",
    "\n",
    "        model = xgb.XGBClassifier(**param_grid,\n",
    "                                  eval_metric='auc', early_stopping_rounds=20,callbacks=[optuna.integration.XGBoostPruningCallback(trial, \"validation_0-auc\")])\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=[(X_valid, y_valid)]\n",
    "        )\n",
    "        best_models.append(model)\n",
    "        y_pred_proba.append(model.predict_proba(X_test)[:, 1])\n",
    "\n",
    "    y_pred_proba_mean = np.mean(y_pred_proba, axis=0)\n",
    "    mean_cv_score = roc_auc_score(y_test, y_pred_proba_mean)\n",
    "    \n",
    "    # Save models if this is the best trial so far\n",
    "    try:\n",
    "        if mean_cv_score > trial.study.best_value:\n",
    "            # Save the mean CV score along with the models\n",
    "            with open(f'model/{model_name}/best_model_score_{model_name}.txt', 'w') as f:\n",
    "                f.write(str(mean_cv_score))\n",
    "            for idx, model in enumerate(best_models):\n",
    "                model.save_model(f'model/{model_name}/best_model_fold_{idx}_{model_name}.ubj')\n",
    "    except ValueError:  # First trial\n",
    "        with open(f'model/{model_name}/best_model_score_{model_name}.txt', 'w') as f:\n",
    "            f.write(str(mean_cv_score))\n",
    "        for idx, model in enumerate(best_models):\n",
    "            model.save_model(f'model/{model_name}/best_model_fold_{idx}_{model_name}.ubj')\n",
    "    \n",
    "    return mean_cv_score\n",
    "\n",
    "def get_metrics(y_true, y_pred_proba, threshold):\n",
    "    y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    precision = tp / (tp + fp)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn) * 100\n",
    "    auroc = roc_auc_score(y_true, y_pred_proba)\n",
    "    auprc = average_precision_score(y_true, y_pred_proba)\n",
    "    \n",
    "    # Calculate 95% CI using Wilson score interval\n",
    "    def wilson_ci(p, n):\n",
    "        z = 1.96 # 95% CI\n",
    "        denominator = 1 + z**2/n\n",
    "        centre_adjusted = p + z**2/(2*n)\n",
    "        spread = z * np.sqrt(p*(1-p)/n + z**2/(4*n**2))\n",
    "        return (centre_adjusted - spread)/denominator, (centre_adjusted + spread)/denominator\n",
    "    \n",
    "    n_sens = tp + fn\n",
    "    n_spec = tn + fp\n",
    "    n_prec = tp + fp\n",
    "    n_total = len(y_true)\n",
    "    \n",
    "    sens_ci = wilson_ci(sensitivity, n_sens)\n",
    "    spec_ci = wilson_ci(specificity, n_spec)\n",
    "    prec_ci = wilson_ci(precision, n_prec)\n",
    "    f1_ci = wilson_ci(f1, n_total)\n",
    "    auroc_ci = roc_auc_ci(y_true, y_pred_proba)\n",
    "    auprc_ci = wilson_ci(auprc, n_total)\n",
    "    \n",
    "    return {\n",
    "        'Threshold': threshold,\n",
    "        'F1-Score': f1,\n",
    "        'F1-Score CI': f1_ci,\n",
    "        'Sensitivity': sensitivity,\n",
    "        'Sensitivity CI': sens_ci,\n",
    "        'Specificity': specificity, \n",
    "        'Specificity CI': spec_ci,\n",
    "        'Precision': precision,\n",
    "        'Precision CI': prec_ci,\n",
    "        'AUROC': auroc,\n",
    "        'AUROC CI': auroc_ci,\n",
    "        'AUPRC': auprc,\n",
    "        'AUPRC CI': auprc_ci,\n",
    "        'TN': tn,\n",
    "        'FP': fp,\n",
    "        'FN': fn,\n",
    "        'TP': tp,\n",
    "        'Accuracy': accuracy\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, droprate):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.droprate = droprate\n",
    "        self.ff_dim = ff_dim\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential([layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),])\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(droprate)\n",
    "        self.dropout2 = layers.Dropout(droprate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {'embed_dim': self.embed_dim, 'num_heads': self.num_heads, 'ff_dim':self.ff_dim, 'droprate':self.droprate}\n",
    "\n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.maxlen = maxlen\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "\n",
    "    def get_config(self):\n",
    "        return {'maxlen': self.maxlen, 'vocab_size': self.vocab_size, 'embed_dim': self.embed_dim}\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different random seeds to find one with balanced distributions\n",
    "best_seed = None\n",
    "min_kl_div = float('inf')\n",
    "threshold = 0.1\n",
    "\n",
    "df = pd.read_csv('data_labels_M_N.csv')\n",
    "unique_patient_ids = df['hid'].unique()\n",
    "\n",
    "# Function to calculate KL divergence between distributions\n",
    "def kl_divergence(p, q):\n",
    "    return np.sum(p * np.log(p/q))\n",
    "\n",
    "def get_label_dist(patient_ids, df):\n",
    "    subset = df[df['hid'].isin(patient_ids)]\n",
    "    total = len(subset)\n",
    "    dist = subset['label'].value_counts(normalize=True)\n",
    "    return dist\n",
    "\n",
    "# Get original distribution as reference\n",
    "orig_dist = df['label'].value_counts(normalize=True)\n",
    "\n",
    "# Try seeds from 0 to 99\n",
    "for seed in range(1000):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Shuffle and split patients\n",
    "    shuffled_ids = unique_patient_ids.copy()\n",
    "    random.shuffle(shuffled_ids)\n",
    "    num_tval_patients = int(len(shuffled_ids) * 0.90)\n",
    "    tval_patients = shuffled_ids[:num_tval_patients]\n",
    "    test_patients = shuffled_ids[num_tval_patients:]\n",
    "\n",
    "    # Split into cross validation folds\n",
    "    num_groups = N_FOLD\n",
    "    group_size = len(tval_patients) // num_groups\n",
    "    cross_val_sets = []\n",
    "\n",
    "    for i in range(num_groups):\n",
    "        start_idx = i * group_size\n",
    "        end_idx = (i + 1) * group_size if i != num_groups - 1 else len(tval_patients)\n",
    "        valid_patients = tval_patients[start_idx:end_idx]\n",
    "        train_patients = np.concatenate([tval_patients[:start_idx], tval_patients[end_idx:]])\n",
    "        cross_val_sets.append((train_patients, valid_patients))\n",
    "    \n",
    "    # Calculate max KL divergence across all splits\n",
    "    max_kl = 0\n",
    "    reference_dist = orig_dist.values\n",
    "    \n",
    "    for train_patients, valid_patients in cross_val_sets:\n",
    "        train_dist = get_label_dist(train_patients, df).values\n",
    "        valid_dist = get_label_dist(valid_patients, df).values\n",
    "        max_kl = max(max_kl, kl_divergence(reference_dist, train_dist))\n",
    "        max_kl = max(max_kl, kl_divergence(reference_dist, valid_dist))\n",
    "    \n",
    "    test_dist = get_label_dist(test_patients, df).values\n",
    "    max_kl = max(max_kl, kl_divergence(reference_dist, test_dist))\n",
    "    \n",
    "    # Update best seed if this distribution is more balanced\n",
    "    if max_kl < min_kl_div:\n",
    "        min_kl_div = max_kl\n",
    "        best_seed = seed\n",
    "        best_cross_val_sets = cross_val_sets\n",
    "        best_test_patients = test_patients\n",
    "\n",
    "# Use the best seed found\n",
    "print(f\"Best random seed found: {best_seed} with max KL divergence: {min_kl_div:.4f}\")\n",
    "random.seed(best_seed)\n",
    "np.random.seed(best_seed)\n",
    "\n",
    "# Save the splits using best seed\n",
    "for i, (train_patients, valid_patients) in enumerate(best_cross_val_sets):\n",
    "    with open(f'train_patients_cv_{i}.pkl', 'wb') as f:\n",
    "        pickle.dump(train_patients, f)\n",
    "    with open(f'valid_patients_cv_{i}.pkl', 'wb') as f:\n",
    "        pickle.dump(valid_patients, f)\n",
    "\n",
    "with open('test_patients.pkl', 'wb') as f:\n",
    "    pickle.dump(best_test_patients, f)\n",
    "\n",
    "# Print distributions for verification\n",
    "print(\"\\nFinal distributions:\")\n",
    "print(\"Original distribution:\")\n",
    "print(orig_dist)\n",
    "\n",
    "print(\"\\nCross validation sets distributions:\")\n",
    "for i, (train_patients, valid_patients) in enumerate(best_cross_val_sets):\n",
    "    print(f\"\\nFold {i}:\")\n",
    "    print(\"Train distribution:\")\n",
    "    print(get_label_dist(train_patients, df))\n",
    "    print(\"Validation distribution:\")\n",
    "    print(get_label_dist(valid_patients, df))\n",
    "\n",
    "print(\"\\nTest set distribution:\")\n",
    "print(get_label_dist(best_test_patients, df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_labels_M_N.csv')\n",
    "\n",
    "# Load the train dataset\n",
    "with open('test_patients.pkl', 'rb') as f:\n",
    "    test_patients = pickle.load(f)\n",
    "\n",
    "test= dataset(header_files=df.loc[df['hid'].isin(test_patients),'filename'].to_list())\n",
    "test.num_leads = 12\n",
    "test.sample = False\n",
    "test.files.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIAGNOSIS = 'MACCE'\n",
    "# Make study directory\n",
    "study_date = datetime.now().strftime('%y%m%d')\n",
    "study_folder_prefix = f'{study_date}_lead12_{DIAGNOSIS}'\n",
    "study_num = 0\n",
    "for folder in sorted(os.listdir('model'), reverse=True):\n",
    "    if folder.startswith(study_folder_prefix):\n",
    "        ch_idx = len(study_folder_prefix) + 1\n",
    "        study_num = int(folder[ch_idx:ch_idx + 2]) + 1\n",
    "        break\n",
    "STUDY_DIR = f'model/{study_folder_prefix}_{study_num:02d}'\n",
    "os.makedirs(STUDY_DIR, exist_ok=True)\n",
    "# Save configuration (Hyperparam search space)\n",
    "search_space_train = {\n",
    "    'BATCH_SIZE_LIST'       : BATCH_SIZE_LIST_GLOBAL,\n",
    "    'LR_INITIAL_LO'         : LR_INITIAL_LO,\n",
    "    'LR_INITIAL_HI'         : LR_INITIAL_HI,\n",
    "    'LR_STEP_SIZE_LO'       : LR_STEP_SIZE_LO,\n",
    "    'LR_STEP_SIZE_HI'       : LR_STEP_SIZE_HI,\n",
    "    'LR_STEP_GAMMA_LO'      : LR_STEP_GAMMA_LO,\n",
    "    'LR_STEP_GAMMA_HI'      : LR_STEP_GAMMA_HI,\n",
    "    'EARLY_STOP_PATIENCE_LO'  : EARLY_STOP_PATIENCE_LO,\n",
    "    'EARLY_STOP_PATIENCE_HI'  : EARLY_STOP_PATIENCE_HI\n",
    "}\n",
    "search_space_arch = {\n",
    "    'BLOCK_SIZE'          : BLOCK_SIZE_GLOBAL,\n",
    "    'BLOCK_DEPTH'        : BLOCK_DEPTH_GLOBAL,\n",
    "    'BLOCK_LAYERS'       : BLOCK_LAYERS_GLOBAL,\n",
    "    'HIDDEN_SIZE'        : HIDDEN_SIZE_GLOBAL,\n",
    "    'KERNEL_NUM'         : KERNEL_NUM_GLOBAL,  \n",
    "}\n",
    "\n",
    "SEARCH_SPACE_ARCH_FILENAME  = f'{STUDY_DIR}/search_space_arch.pkl'\n",
    "SEARCH_SPACE_TRAIN_FILENAME = f'{STUDY_DIR}/search_space_train.pkl'\n",
    "\n",
    "pickle.dump(search_space_arch, open(SEARCH_SPACE_ARCH_FILENAME, 'wb'))\n",
    "pickle.dump(search_space_train, open(SEARCH_SPACE_TRAIN_FILENAME, 'wb'))\n",
    "\n",
    "for i_trial in range(N_TRIAL):\n",
    "    search_space_arch = pickle.load(open(SEARCH_SPACE_ARCH_FILENAME, 'rb'))\n",
    "    search_space_train = pickle.load(open(SEARCH_SPACE_TRAIN_FILENAME, 'rb'))\n",
    "    # Train hparams\n",
    "    batch_size          = np.random.choice(search_space_train['BATCH_SIZE_LIST'], size=1)[0]\n",
    "    lr_initial          = loguniform(low=search_space_train['LR_INITIAL_LO'], high=search_space_train['LR_INITIAL_HI'], size=1)[0]\n",
    "    lr_step_size        = np.random.randint(low=search_space_train['LR_STEP_SIZE_LO'], high=search_space_train['LR_STEP_SIZE_HI'], size=1)[0]\n",
    "    lr_step_gamma       = np.random.uniform(low=search_space_train['LR_STEP_GAMMA_LO'], high=search_space_train['LR_STEP_GAMMA_HI'], size=1)[0]\n",
    "    early_stop_pat      = np.random.randint(low=search_space_train['EARLY_STOP_PATIENCE_LO'], high=search_space_train['EARLY_STOP_PATIENCE_HI'], size=1)[0]\n",
    "\n",
    "    # Architecture hparams\n",
    "    block_size             = np.random.choice(search_space_arch['BLOCK_SIZE'], size=1)[0]\n",
    "    block_depth             = np.random.choice(search_space_arch['BLOCK_DEPTH'], size=1)[0]\n",
    "    block_layers            = np.random.choice(search_space_arch['BLOCK_LAYERS'], size=1)[0]\n",
    "    hidden_size             = np.random.choice(search_space_arch['HIDDEN_SIZE'], size=1)[0]\n",
    "    kernel_num              = np.random.choice(search_space_arch['KERNEL_NUM'], size=1)[0]\n",
    "    # Trial directory\n",
    "    trial_folder = ''\n",
    "    trial_folder += f'batch={batch_size}_'\n",
    "    trial_folder += f'lr={lr_initial:.5f}_step={lr_step_size}_gam={lr_step_gamma:.3f}_pat={early_stop_pat}_'\n",
    "    trial_folder += f'block_size={block_size}_block_depth={block_depth}_hidden_size={hidden_size}_block_layers={block_layers}_kernel_num={kernel_num}'\n",
    "    \n",
    "    trial_dir = f'{STUDY_DIR}/{trial_folder}'\n",
    "    os.makedirs(trial_dir, exist_ok=True)\n",
    "\n",
    "    # Save configurations\n",
    "    hparams_train = {\n",
    "        'N_FOLD'            : N_FOLD,\n",
    "        'N_EPOCH'           : N_EPOCH,\n",
    "        'BATCH_SIZE'        : batch_size,\n",
    "        'LR_INITIAL'        : lr_initial,\n",
    "        'LR_STEP_SIZE'      : lr_step_size,\n",
    "        'LR_STEP_GAMMA'     : lr_step_gamma,\n",
    "        'EARLY_STOP_PAT'    : early_stop_pat,\n",
    "    }\n",
    "    hparams_arch = {\n",
    "        'BLOCK_SIZE'        : block_size,\n",
    "        'BLOCK_DEPTH'       : block_depth,\n",
    "        'BLOCK_LAYERS'      : block_layers,\n",
    "        'HIDDEN_SIZE'       : hidden_size,\n",
    "        'KERNEL_NUM'        : kernel_num,\n",
    "    }\n",
    "    dataset_config = {\n",
    "        'DIAGNOSIS'         : DIAGNOSIS,\n",
    "    }\n",
    "\n",
    "    print('='*100)\n",
    "    print(f'{DIAGNOSIS} RANDOM SEARCH TRIAL {1 + i_trial}/{N_TRIAL}')\n",
    "    print('='*100)\n",
    "    print()\n",
    "\n",
    "    HPARAMS_TRAIN_FILENAME  = f'{trial_dir}/hparams_train.pkl'\n",
    "    HPARAMS_ARCH_FILENAME   = f'{trial_dir}/hparams_arch.pkl'\n",
    "    DATASET_CONFIG_FILENAME = f'{trial_dir}/dataset_config.pkl'\n",
    "\n",
    "    pickle.dump(hparams_train, open(HPARAMS_TRAIN_FILENAME, 'wb'))\n",
    "    pickle.dump(hparams_arch, open(HPARAMS_ARCH_FILENAME, 'wb'))\n",
    "    pickle.dump(dataset_config, open(DATASET_CONFIG_FILENAME, 'wb'))\n",
    "\n",
    "    # ========== #\n",
    "    # File Paths #\n",
    "    # ========== #\n",
    "    TRIAL_DIR                   = trial_dir\n",
    "    TRIAL_FOLDER                = TRIAL_DIR.split('/')[-1]\n",
    "    STUDY_DIR                   = TRIAL_DIR[:-(len(TRIAL_FOLDER) + 1)]\n",
    "\n",
    "    HPARAMS_ARCH_FILENAME       = f'{TRIAL_DIR}/hparams_arch.pkl'\n",
    "    HPARAMS_TRAIN_FILENAME      = f'{TRIAL_DIR}/hparams_train.pkl'\n",
    "    DATASET_CONFIG_FILENAME     = f'{TRIAL_DIR}/dataset_config.pkl'\n",
    "\n",
    "    # ==================== #\n",
    "    # Load Hyperparameters #\n",
    "    # ==================== #\n",
    "\n",
    "    # Architecture\n",
    "    hparams_arch    = pickle.load(open(HPARAMS_ARCH_FILENAME, 'rb'))\n",
    "\n",
    "    BLOCK_SIZE      = int(hparams_arch['BLOCK_SIZE'])\n",
    "    BLOCK_DEPTH     = int(hparams_arch['BLOCK_DEPTH'])\n",
    "    BLOCK_LAYERS    = int(hparams_arch['BLOCK_LAYERS'])\n",
    "    HIDDEN_SIZE     = int(hparams_arch['HIDDEN_SIZE'])\n",
    "    KERNEL_NUM      = int(hparams_arch['KERNEL_NUM'])\n",
    "    # Train\n",
    "    hparams_train   = pickle.load(open(HPARAMS_TRAIN_FILENAME, 'rb'))\n",
    "\n",
    "    N_FOLD          = int(hparams_train['N_FOLD'])\n",
    "    N_EPOCH         = int(hparams_train['N_EPOCH'])\n",
    "    BATCH_SIZE      = int(hparams_train['BATCH_SIZE'])\n",
    "    LR_INITIAL      = hparams_train['LR_INITIAL']\n",
    "    LR_STEP_SIZE    = hparams_train['LR_STEP_SIZE']\n",
    "    LR_STEP_GAMMA   = hparams_train['LR_STEP_GAMMA']\n",
    "    EARLY_STOP_PAT  = hparams_train['EARLY_STOP_PAT']\n",
    "\n",
    "    #N_WORKERS       = 8\n",
    "    #PREFETCH_FACTOR = 4\n",
    "\n",
    "    # Dataset configs\n",
    "    dataset_config  = pickle.load(open(DATASET_CONFIG_FILENAME, 'rb'))\n",
    "\n",
    "    DIAGNOSIS       = dataset_config['DIAGNOSIS']\n",
    "\n",
    "    # =========== #\n",
    "    # Train Model #\n",
    "    # =========== #\n",
    "\n",
    "    # Initialize model & save weights\n",
    "    model = FinalModel(block_size =BLOCK_SIZE, block_depth =BLOCK_DEPTH, block_layers=BLOCK_LAYERS, hidden_size=HIDDEN_SIZE, kernel_num=KERNEL_NUM).to(DEVICE)\n",
    "    torch.save(model.state_dict(), f'{TRIAL_DIR}/initial_weights.pth')\n",
    "\n",
    "    # Create test data loader\n",
    "    test_loader = DataLoader(test, collate_fn=collate_fn, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    y_pred_test_list = []\n",
    "\n",
    "    # [Main] 4-Fold Validation\n",
    "    # Create datasets & data loaders\n",
    "    for i_fold in range(N_FOLD):\n",
    "        df = pd.read_csv('data_labels_M_N.csv')\n",
    "\n",
    "        # Load the train dataset\n",
    "        with open(f'train_patients_cv_{i_fold}.pkl', 'rb') as f:\n",
    "            train_patients = pickle.load(f)\n",
    "        # Load the valid dataset\n",
    "        with open(f'valid_patients_cv_{i_fold}.pkl', 'rb') as f:\n",
    "            valid_patients = pickle.load(f)\n",
    "            \n",
    "        # Create instances of the new class\n",
    "        train = dataset(header_files=df.loc[df['hid'].isin(train_patients),'filename'].to_list())\n",
    "        train.num_leads = 12\n",
    "        train.sample = True\n",
    "\n",
    "        valid= dataset(header_files=df.loc[df['hid'].isin(valid_patients),'filename'].to_list())\n",
    "        valid.num_leads = 12\n",
    "        valid.sample = False\n",
    "        valid.files.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        train_loader = DataLoader(train, collate_fn=collate_fn, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        valid_loader = DataLoader(valid, collate_fn=collate_fn, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        # Load model weights & Initialize study\n",
    "        model.load_state_dict(torch.load(f'{TRIAL_DIR}/initial_weights.pth', weights_only=True))\n",
    "        weight_cache = f'{TRIAL_DIR}/weights_fold_{i_fold+1}.pth'\n",
    "\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        optimizer = Adam(model.parameters(), lr=LR_INITIAL)\n",
    "        scheduler = ReduceLROnPlateau(optimizer, patience=LR_STEP_SIZE, factor=LR_STEP_GAMMA, min_lr=1e-5, mode='max')\n",
    "\n",
    "        best_val_auroc = 0.0\n",
    "        early_stop_count = 0\n",
    "\n",
    "        # Model train & validation\n",
    "        for epoch in range(N_EPOCH):\n",
    "            print(f'FOLD {i_fold+1} - EPOCH {1 + epoch}/{N_EPOCH}')        \n",
    "            train_loss = train_model(train_loader, model, criterion, optimizer, scheduler)\n",
    "            y_true_valid, y_pred_valid,_ = evaluate_model(valid_loader, model)\n",
    "            valid_loss = F.binary_cross_entropy(torch.FloatTensor(y_pred_valid), torch.FloatTensor(y_true_valid))\n",
    "            valid_auroc = roc_auc_score(y_true_valid, y_pred_valid)\n",
    "            print(f'train_loss: {train_loss:.4f} - val_loss: {valid_loss:.4f} - val_auroc: {valid_auroc:.4f}')\n",
    "\n",
    "            scheduler.step(valid_auroc)\n",
    "            # Update best model\n",
    "            if valid_auroc > best_val_auroc:\n",
    "                print(f'>> val_auroc increased from {best_val_auroc:.4f} to {valid_auroc:.4f}\\n>> Saving weights to [{weight_cache}]')\n",
    "                torch.save(model.state_dict(), weight_cache)\n",
    "\n",
    "                best_val_auroc = valid_auroc\n",
    "                early_stop_count = 0\n",
    "\n",
    "            else:\n",
    "                early_stop_count += 1\n",
    "                \n",
    "            # Early stoping\n",
    "            if early_stop_count >= EARLY_STOP_PAT:\n",
    "                break\n",
    "\n",
    "            print()\n",
    "\n",
    "        # Load best weights & inference on test set\n",
    "        model.load_state_dict(torch.load(weight_cache, weights_only=True))\n",
    "        y_true_test, y_pred_test, _ = evaluate_model(test_loader, model)\n",
    "        test_auroc = roc_auc_score(y_true_test, y_pred_test)\n",
    "        y_pred_test_list.append(y_pred_test)\n",
    "\n",
    "        print()\n",
    "        print(f'>> TEST AUROC = {test_auroc:.4f}')\n",
    "        print()\n",
    "\n",
    "    # ================ #\n",
    "    # Ensemble Results #\n",
    "    # ================ #\n",
    "\n",
    "    # Inference on test set\n",
    "    y_pred_test_ensemble = np.mean(np.array(y_pred_test_list), axis=0)\n",
    "    test_auroc_ensemble = roc_auc_score(y_true_test, y_pred_test_ensemble)\n",
    "\n",
    "    # Print results\n",
    "    print('*' * 100)\n",
    "    print('TEST RESULTS (AUROC)')\n",
    "    for i in range(N_FOLD):\n",
    "        print(f'Fold #{i}  : {roc_auc_score(y_true_test, y_pred_test_list[i]):.4f}')\n",
    "    print()\n",
    "    print(f'Ensemble : {test_auroc_ensemble:.4f}')\n",
    "    print('*' * 100)\n",
    "\n",
    "    # Rename trial folder\n",
    "    TRIAL_FOLDER_NEW = f'auc={test_auroc_ensemble:.4f}_{TRIAL_FOLDER}'\n",
    "    TRIAL_DIR_NEW = f'{STUDY_DIR}/{TRIAL_FOLDER_NEW}'\n",
    "    os.rename(TRIAL_DIR, TRIAL_DIR_NEW)\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### Code for rechecking score #############\n",
    "trial_dir = \"model/241211_lead12_MACCE_00/auc=0.8772_batch=64_lr=0.00086_step=2_gam=0.164_pat=4_block_size=16_block_depth=3_hidden_size=32_block_layers=3_kernel_num=5\"\n",
    "# ========== #\n",
    "# File Paths #\n",
    "# ========== #\n",
    "TRIAL_DIR                   = trial_dir\n",
    "TRIAL_FOLDER                = TRIAL_DIR.split('/')[-1]\n",
    "STUDY_DIR                   = TRIAL_DIR[:-(len(TRIAL_FOLDER) + 1)]\n",
    "\n",
    "HPARAMS_ARCH_FILENAME       = f'{TRIAL_DIR}/hparams_arch.pkl'\n",
    "HPARAMS_TRAIN_FILENAME      = f'{TRIAL_DIR}/hparams_train.pkl'\n",
    "DATASET_CONFIG_FILENAME     = f'{TRIAL_DIR}/dataset_config.pkl'\n",
    "\n",
    "# ==================== #\n",
    "# Load Hyperparameters #\n",
    "# ==================== #\n",
    "# Architecture\n",
    "hparams_arch    = pickle.load(open(HPARAMS_ARCH_FILENAME, 'rb'))\n",
    "\n",
    "BLOCK_SIZE      = int(hparams_arch['BLOCK_SIZE'])\n",
    "BLOCK_DEPTH     = int(hparams_arch['BLOCK_DEPTH'])\n",
    "BLOCK_LAYERS    = int(hparams_arch['BLOCK_LAYERS'])\n",
    "HIDDEN_SIZE     = int(hparams_arch['HIDDEN_SIZE'])\n",
    "KERNEL_NUM      = int(hparams_arch['KERNEL_NUM'])\n",
    "# Train\n",
    "hparams_train   = pickle.load(open(HPARAMS_TRAIN_FILENAME, 'rb'))\n",
    "\n",
    "N_FOLD          = int(hparams_train['N_FOLD'])\n",
    "N_EPOCH         = int(hparams_train['N_EPOCH'])\n",
    "BATCH_SIZE      = int(hparams_train['BATCH_SIZE'])\n",
    "LR_INITIAL      = hparams_train['LR_INITIAL']\n",
    "LR_STEP_SIZE    = hparams_train['LR_STEP_SIZE']\n",
    "LR_STEP_GAMMA   = hparams_train['LR_STEP_GAMMA']\n",
    "EARLY_STOP_PAT  = hparams_train['EARLY_STOP_PAT']\n",
    "\n",
    "#N_WORKERS       = 8\n",
    "#PREFETCH_FACTOR = 4\n",
    "# Dataset configs\n",
    "dataset_config  = pickle.load(open(DATASET_CONFIG_FILENAME, 'rb'))\n",
    "\n",
    "DIAGNOSIS       = dataset_config['DIAGNOSIS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Evaluate time series model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_labels_M_N.csv')\n",
    "\n",
    "##############################################################\n",
    "# Load the train dataset\n",
    "with open('test_patients.pkl', 'rb') as f:\n",
    "    test_patients = pickle.load(f)\n",
    "\n",
    "test= dataset(header_files=df.loc[df['hid'].isin(test_patients),'filename'].to_list())\n",
    "test.num_leads = 12\n",
    "test.sample = False\n",
    "test.files.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Create test data loader\n",
    "test_loader = DataLoader(test, collate_fn=collate_fn, batch_size=BATCH_SIZE, shuffle=False)\n",
    "y_pred_test_list = []\n",
    "\n",
    "# Create datasets & data loaders\n",
    "for i_fold in range(N_FOLD):\n",
    "    # Initialize model & save weights\n",
    "    model = FinalModel(block_size =BLOCK_SIZE, block_depth =BLOCK_DEPTH, block_layers=BLOCK_LAYERS, hidden_size=HIDDEN_SIZE, kernel_num=KERNEL_NUM).to(DEVICE)\n",
    "    weight_cache = f'{TRIAL_DIR}/weights_fold_{i_fold+1}.pth'\n",
    "    # Load best weights & inference on test set\n",
    "    model.load_state_dict(torch.load(weight_cache, weights_only=True))\n",
    "    y_true_test, y_pred_test, hidden_states = evaluate_model(test_loader, model)\n",
    "    test_auroc = roc_auc_score(y_true_test, y_pred_test)\n",
    "    y_pred_test_list.append(y_pred_test)\n",
    "    print()\n",
    "    print(f'>> TEST AUROC = {test_auroc:.4f}')\n",
    "    print()\n",
    "\n",
    "y_pred_test = np.mean(np.array(y_pred_test_list), axis=0)\n",
    "y_true_test = y_true_test.ravel()  # Flatten to 1D array\n",
    "y_pred_test = y_pred_test.ravel()  # Flatten to 1D array\n",
    "##############################################################\n",
    "\n",
    "with open('train_patients_cv_0.pkl', 'rb') as f:\n",
    "    train_patients = pickle.load(f)\n",
    "with open('valid_patients_cv_0.pkl', 'rb') as f:\n",
    "    val_patients = pickle.load(f)\n",
    "tval_patients = np.concatenate([train_patients, val_patients])\n",
    "\n",
    "tval= dataset(header_files=df.loc[df['hid'].isin(tval_patients),'filename'].to_list())\n",
    "tval.num_leads = 12\n",
    "tval.sample = False\n",
    "tval.files.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Create test data loader\n",
    "tval_loader = DataLoader(tval, collate_fn=collate_fn, batch_size=BATCH_SIZE, shuffle=False)\n",
    "y_pred_tval_list = []\n",
    "\n",
    "# [Main] 4-Fold Validation\n",
    "# Create datasets & data loaders\n",
    "for i_fold in range(N_FOLD):\n",
    "    # Initialize model & save weights\n",
    "    model = FinalModel(block_size =BLOCK_SIZE, block_depth =BLOCK_DEPTH, block_layers=BLOCK_LAYERS, hidden_size=HIDDEN_SIZE, kernel_num=KERNEL_NUM).to(DEVICE)\n",
    "    weight_cache = f'{TRIAL_DIR}/weights_fold_{i_fold+1}.pth'\n",
    "    # Load best weights & inference on test set\n",
    "    model.load_state_dict(torch.load(weight_cache, weights_only=True))\n",
    "    y_true_tval, y_pred_tval, hidden_states = evaluate_model(tval_loader, model)\n",
    "    tval_auroc = roc_auc_score(y_true_tval, y_pred_tval)\n",
    "    y_pred_tval_list.append(y_pred_tval)\n",
    "    print()\n",
    "    print(f'>> TVAL AUROC = {tval_auroc:.4f}')\n",
    "    print()\n",
    "y_pred_tval = np.mean(np.array(y_pred_tval_list), axis=0)\n",
    "y_true_tval = y_true_tval.ravel()  # Flatten to 1D array\n",
    "y_pred_tval = y_pred_tval.ravel()  # Flatten to 1D array\n",
    "\n",
    "# Save train/val predictions\n",
    "with open('y_pred_tval_cnn.pkl', 'wb') as f:\n",
    "    pickle.dump(y_pred_tval, f)\n",
    "\n",
    "######################################################################\n",
    "with open('y_pred_tval_cnn.pkl', 'rb') as f:\n",
    "    y_pred_tval = pickle.load(f)\n",
    "\n",
    "# Calculate calibration metrics before linear regression\n",
    "cal_model = LinearRegression()\n",
    "cal_model.fit(y_pred_test.reshape(-1,1), y_true_test)\n",
    "cal_intercept = cal_model.intercept_\n",
    "cal_slope = cal_model.coef_[0]\n",
    "brier_before = brier_score_loss(y_true_test, y_pred_test)\n",
    "\n",
    "# Calculate unreliability index before calibration\n",
    "\n",
    "U_before, p_before = calc_unreliability(y_true_test, y_pred_test)\n",
    "\n",
    "ir = IsotonicRegression(out_of_bounds='clip')\n",
    "ir.fit(y_pred_tval, y_true_tval)\n",
    "y_pred_test_cal = ir.predict(y_pred_test)\n",
    "\n",
    "#calib = mli.SplineCalib(unity_prior=False, unity_prior_weight=100, random_state=42, max_iter =500)\n",
    "#calib.fit(y_pred_tval, y_true_tval)\n",
    "#y_pred_test_cal = calib.calibrate(y_pred_test)\n",
    "\n",
    "# Ensure predictions are not exactly 0 or 1\n",
    "y_pred_test_cal = np.clip(y_pred_test_cal, 1e-15, 1-1e-15)\n",
    "\n",
    "# Calculate calibration metrics after linear regression\n",
    "cal_model = LinearRegression()\n",
    "cal_model.fit(y_pred_test_cal.reshape(-1,1), y_true_test)\n",
    "cal_intercept_after = cal_model.intercept_\n",
    "cal_slope_after = cal_model.coef_[0]\n",
    "brier_after = brier_score_loss(y_true_test, y_pred_test_cal)\n",
    "\n",
    "# Calculate unreliability after calibration\n",
    "U_after, p_after = calc_unreliability(y_true_test, y_pred_test_cal)\n",
    "\n",
    "print(\"Before Calibration:\")\n",
    "print(f\"Calibration Intercept: {cal_intercept:.4f}\")\n",
    "print(f\"Calibration Slope: {cal_slope:.4f}\")\n",
    "print(f\"Brier Score: {brier_before:.4f}\")\n",
    "print(f\"Unreliability Index: {U_before:.4f}\")\n",
    "print(f\"Unreliability p-value: {p_before:.4f}\")\n",
    "\n",
    "print(\"\\nAfter Linear Regression:\")\n",
    "print(f\"Calibration Intercept: {cal_intercept_after:.4f}\")\n",
    "print(f\"Calibration Slope: {cal_slope_after:.4f}\")\n",
    "print(f\"Brier Score: {brier_after:.4f}\")\n",
    "print(f\"Unreliability Index: {U_after:.4f}\")\n",
    "print(f\"Unreliability p-value: {p_after:.4f}\")\n",
    "\n",
    "prob_true, prob_pred = calibration_curve(y_true_test, y_pred_test, n_bins=10)\n",
    "draw_calibration_plot(prob_true, prob_pred, y_pred_test, 10, 1.0)\n",
    "\n",
    "Youden = youden(y_true_tval, y_pred_tval)\n",
    "print(Youden)\n",
    "\n",
    "AUROC = roc_auc_score(y_true_test, y_pred_test)\n",
    "AUPRC = average_precision_score(y_true_test, y_pred_test)\n",
    "y_pred_label = [1 if value > Youden else 0 for value in y_pred_test]\n",
    "y_pred_label = np.array(y_pred_label)\n",
    "F1_score = f1_score(y_true_test, y_pred_label)\n",
    "\n",
    "print(f'AUROC : {AUROC:.4f}')\n",
    "print(f'AUROC CI(95%): ({roc_auc_ci(y_true_test, y_pred_test)[0]:.4f},{roc_auc_ci(y_true_test, y_pred_test)[1]:.4f})')\n",
    "print(f'AUPRC : {AUPRC:.4f}')\n",
    "print(f'F1 Score : {F1_score:.4f}')\n",
    "print(f'Test Accuracy : {round(100*accuracy_score(y_true_test, y_pred_label), 2)}% ')\n",
    "\n",
    "draw_roc_curve(y_true_test, y_pred_test)\n",
    "draw_confusion_matrix(y_true_test, y_pred_label)\n",
    "draw_y_test_proba(y_pred_test)\n",
    "\n",
    "# Save test results\n",
    "with open(f'{trial_dir}/y_test_cnn.pkl', 'wb') as f:\n",
    "    pickle.dump(y_true_test, f)\n",
    "with open(f'{trial_dir}/y_test_proba_cnn.pkl', 'wb') as f:\n",
    "    pickle.dump(y_pred_test, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Evaluate GBM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-1) Extract ICD 10 codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_labels_M_N.csv')\n",
    "df2 = pd.read_csv('ecg_labels_w_features_lab_asa_3mo_N.csv')\n",
    "\n",
    "df['andur'] = df['filename'].map(df2.set_index('filename')['andur'])\n",
    "df['asa'] = df['filename'].map(df2.set_index('filename')['final_asa'])\n",
    "\n",
    "df.to_csv('data_labels_gbm.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_labels_gbm.csv')\n",
    "\n",
    "all= dataset(header_files=df['filename'].to_list())\n",
    "all.num_leads = 12\n",
    "all.sample = False\n",
    "all.files.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Create test data loader\n",
    "all_loader = DataLoader(all, collate_fn=collate_fn, batch_size=128, shuffle=False)\n",
    "\n",
    "for i_fold in range(N_FOLD):\n",
    "    # Initialize model & save weights\n",
    "    model = FinalModel(block_size =BLOCK_SIZE, block_depth =BLOCK_DEPTH, block_layers=BLOCK_LAYERS, hidden_size=HIDDEN_SIZE, kernel_num=KERNEL_NUM).to(DEVICE)\n",
    "    weight_cache = f'{TRIAL_DIR}/weights_fold_{i_fold+1}.pth'\n",
    "    # Load best weights & inference on test set\n",
    "    model.load_state_dict(torch.load(weight_cache, weights_only=True))\n",
    "    y_true_test, y_pred_test, hidden_states = evaluate_model(all_loader, model)\n",
    "    hidden_states_df = pd.DataFrame(hidden_states, columns=[f'hidden_{i_fold+1}_{i}' for i in range(HIDDEN_SIZE)])\n",
    "    df = pd.concat([df, hidden_states_df], axis=1)\n",
    "\n",
    "df.to_csv('data_labels_gbm_hidden.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Imports for ICD 10 code extraction\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, droprate):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.droprate = droprate\n",
    "        self.ff_dim = ff_dim\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential([layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),])\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(droprate)\n",
    "        self.dropout2 = layers.Dropout(droprate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {'embed_dim': self.embed_dim, 'num_heads': self.num_heads, 'ff_dim':self.ff_dim, 'droprate':self.droprate}\n",
    "\n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.maxlen = maxlen\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "\n",
    "    def get_config(self):\n",
    "        return {'maxlen': self.maxlen, 'vocab_size': self.vocab_size, 'embed_dim': self.embed_dim}\n",
    "\n",
    "# SNUH data\n",
    "# system, operation, body part, approach\n",
    "df3 = pd.read_csv('data_labels_gbm.csv', dtype=str)\n",
    "df3 = df3.merge(pd.read_csv('transformer/all_data_labels_241106(asa_updated, opname).csv', usecols=['opname','filename'], dtype=str), how='left', on='filename')  # the label manually checked by hclee\n",
    "# merge the name with opname and opname_final\n",
    "df3['src'] = 'snuh'\n",
    "df3 = df3.merge(pd.read_csv('transformer/icd10_mapping_hclee.csv', usecols=['opname', 'p', 'o', 'a'], dtype=str), how='left', on='opname')\n",
    "df3['name'] = df3['opname']\n",
    "df = df3.copy()\n",
    "# remove duplicate rows\n",
    "df = df.drop_duplicates(subset=['filename'], keep='first')\n",
    "\n",
    "# shuffling\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "# Ensure all elements in x are strings\n",
    "x = np.copy(df['name'].values.astype(str))  # Convert all elements to strings\n",
    "# Process x\n",
    "vocab_size = 4000\n",
    "t_x = Tokenizer(vocab_size)\n",
    "t_x.fit_on_texts(x)\n",
    "x = t_x.texts_to_sequences(x)\n",
    "maxlen = 158  # Instead of dynamically calculating maxlen\n",
    "x = keras.preprocessing.sequence.pad_sequences(x, maxlen=maxlen)\n",
    "\n",
    "prefix = 'transformer_res'\n",
    "for target in ('p', 'o', 'a'):\n",
    "    print(f'Processing {target}...')\n",
    "    opath = f'transformer/{prefix}_{target}.csv'\n",
    "    \n",
    "    print('Loading tokenizer...')\n",
    "    t_y = Tokenizer()\n",
    "    tokenizer_config = json.loads(open(f'transformer/tokenizer_y_{target}.json').read())\n",
    "    tokenizer_config = tokenizer_config['config']\n",
    "    # Parse string representations back to dictionaries\n",
    "    t_y.word_index = json.loads(tokenizer_config['word_index']) if isinstance(tokenizer_config['word_index'], str) else tokenizer_config['word_index']\n",
    "    t_y.index_word = json.loads(tokenizer_config['index_word']) if isinstance(tokenizer_config['index_word'], str) else tokenizer_config['index_word']\n",
    "    t_y.word_counts = json.loads(tokenizer_config['word_counts']) if isinstance(tokenizer_config['word_counts'], str) else tokenizer_config['word_counts']\n",
    "    # Convert index_word keys to integers\n",
    "    t_y.index_word = {int(k): v for k, v in t_y.index_word.items()}\n",
    "    t_y.document_count = tokenizer_config['document_count']\n",
    "\n",
    "    print('Loading model...')\n",
    "    custom_objects = {\n",
    "        'TokenAndPositionEmbedding': TokenAndPositionEmbedding,\n",
    "        'TransformerBlock': TransformerBlock\n",
    "    }\n",
    "    model = keras.models.model_from_json(\n",
    "        open(f'transformer/model_{target}.json').read(),\n",
    "        custom_objects=custom_objects\n",
    "    )\n",
    "    model.load_weights(f'transformer/tuned_weights_{target}.h5')\n",
    "\n",
    "    print('Running inference...')\n",
    "    pred = model.predict(x, verbose=1)\n",
    "    \n",
    "    print('Post-processing predictions...')\n",
    "    df['pred'] = pd.Series(t_y.sequences_to_texts(np.argmax(pred, axis=1)[...,None] + 1)).str.upper()\n",
    "    df['conf'] = pred.max(axis=1)\n",
    "    df['matched'] = (df['pred'] == df[target]).astype(int)\n",
    "\n",
    "    print(f'Saving results to {opath}...')\n",
    "    df[df['src'] == 'snuh'].drop(columns='src').to_csv(opath, index=False, encoding='utf-8-sig')\n",
    "    print(f'Finished processing {target}\\n')\n",
    "\n",
    "# merge\n",
    "df_a = pd.read_csv(f'transformer/{prefix}_a.csv', dtype=str, usecols=['filename', 'pred', 'matched', 'conf'])\n",
    "df_o = pd.read_csv(f'transformer/{prefix}_o.csv', dtype=str, usecols=['filename', 'pred', 'matched', 'conf'])\n",
    "df = pd.read_csv(f'transformer/{prefix}_p.csv', dtype=str)\n",
    "df = df.merge(df_a, how='left', on='filename', suffixes=('', '_a'))\n",
    "df = df.merge(df_o, how='left', on='filename', suffixes=('', '_o'))\n",
    "df.rename(columns={'pred':'pred_p', 'matched':'matched_p'}, inplace=True)\n",
    "\n",
    "# Fill null values in p,o,a columns with predicted values\n",
    "df['p'] = df['p'].fillna(df['pred_p'])\n",
    "df['o'] = df['o'].fillna(df['pred_o']) \n",
    "df['a'] = df['a'].fillna(df['pred_a'])\n",
    "\n",
    "# Keep only columns up to 'a' \n",
    "df = df[['filename', 'hid', 'age', 'gender', 'asa', 'andur', 'label', 'p', 'o', 'a']]\n",
    "\n",
    "df2 = pd.read_csv('data_labels_gbm_hidden.csv', dtype=str)\n",
    "df2 = df2.merge(df[['filename', 'p', 'o', 'a']], how='left', on='filename')\n",
    "df2.to_csv('data_labels_gbm_hidden_icd.csv', index=False, encoding='utf-8-sig')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-2) Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### OPID ICD Matching\n",
    "'''\n",
    "df2 = pd.read_csv('df_data.csv')\n",
    "df2['AcquisitionDate'] = pd.to_datetime(df2['AcquisitionDate'])\n",
    "\n",
    "df3 = pd.read_csv('opid_macce_orig.csv')\n",
    "df3['opid'] = df3['opid'].astype(str).str.zfill(9)\n",
    "df3['op_date'] = pd.to_datetime(df3['opid'].str[:6].apply(lambda x: f'20{x[:2]}-{x[2:4]}-{x[4:6]}'))\n",
    "\n",
    "opid_map = {}\n",
    "for _, row in df3.iterrows():\n",
    "    hid = row['hid']\n",
    "    opdate = row['op_date']\n",
    "    opid = row['opid']\n",
    "    \n",
    "    # Create key as tuple of hid and opdate\n",
    "    key = (hid, opdate)\n",
    "    if key not in opid_map:\n",
    "        opid_map[key] = []\n",
    "    opid_map[key].append(opid)\n",
    "\n",
    "def find_opid(row):\n",
    "    hid = row['hid']\n",
    "    acq_date = row['AcquisitionDate']\n",
    "    end_date = acq_date + pd.DateOffset(months=3)\n",
    "    # Check all dates in range\n",
    "    for op_date in pd.date_range(acq_date, end_date):\n",
    "        key = (hid, op_date)\n",
    "        if key in opid_map:\n",
    "            return opid_map[key][0] # Return first matching opid\n",
    "    return None\n",
    "\n",
    "df2['opid'] = df2.apply(find_opid, axis=1)\n",
    "\n",
    "df = pd.read_csv('data_labels_gbm_hidden_icd.csv')\n",
    "filename_to_opid = df2.set_index('filename')['opid'].to_dict()\n",
    "df['opid'] = df['filename'].map(filename_to_opid)\n",
    "\n",
    "# Create new dataframe without 'hidden' columns\n",
    "df_x = df.loc[:, ~df.columns.str.contains('hidden', case=False)]\n",
    "df_x.to_csv('opid_icd_matching.csv', index=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_labels_gbm_hidden_icd.csv')\n",
    "df_catcol = ['gender','p','o','a'] #'gender','asa','p','o','a'\n",
    "df_numcol = ['age'] #'age','andur'\n",
    "\n",
    "for i_fold in range(N_FOLD):\n",
    "   df_numcol.extend([f'hidden_{i_fold+1}_{i}' for i in range(HIDDEN_SIZE)])\n",
    "labels = 'label'\n",
    "model_name = 'xgb_demo_icd' # change model name according to df_catcol, df_numcol combination\n",
    "\n",
    "df[df_catcol] = df[df_catcol].astype('category')\n",
    "# Filter dataframe to only include specified columns\n",
    "df = df[df_catcol + df_numcol +['label','hid']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Get unique hospital IDs and split into train/test\n",
    "# Load the train dataset\n",
    "i_fold =0\n",
    "with open('test_patients.pkl', 'rb') as f:\n",
    "    test_hids = pickle.load(f)\n",
    "with open(f'train_patients_cv_{i_fold}.pkl', 'rb') as f:\n",
    "    train_hids = pickle.load(f)\n",
    "with open(f'valid_patients_cv_{i_fold}.pkl', 'rb') as f:\n",
    "    valid_hids = pickle.load(f)\n",
    "\n",
    "train_val_hids = np.concatenate((train_hids, valid_hids))\n",
    "\n",
    "# Save all HID groups to pickle file\n",
    "hid_splits = {\n",
    "    'test_hids': test_hids,\n",
    "    'tval_hids': train_val_hids\n",
    "}\n",
    "\n",
    "with open(f'hid_splits.pkl', 'wb') as f:\n",
    "    pickle.dump(hid_splits, f)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert label 3 to 1, keeping 1 as 1 and 0 as 0\n",
    "df['label'] = df['label'].replace({3: 1})\n",
    "\n",
    "with open(f'hid_splits.pkl', 'rb') as f:\n",
    "    hid_splits= pickle.load(f)\n",
    "df_train = df[df['hid'].isin(hid_splits['tval_hids'])]\n",
    "df_test = df[df['hid'].isin(hid_splits['test_hids'])]\n",
    "\n",
    "#df.to_csv(f'df_MACCE.csv', index=False)\n",
    "#df_train.to_csv(f'df_train_MACCE.csv', index=False)\n",
    "#df_test.to_csv(f'df_test_MACCE.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model directory if it doesn't exist\n",
    "if not os.path.exists(f'model/{model_name}'):\n",
    "    os.makedirs(f'model/{model_name}')\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\", study_name=\"XGB Classifier\")\n",
    "func = lambda trial: objective(trial, df_train, df_test, model_name)\n",
    "#func = lambda trial: objective(trial, pd.DataFrame(X_train, columns=feature_names), pd.DataFrame(y_train, columns=['label']), pd.DataFrame(h_train, columns=['hid']))\n",
    "study.optimize(func, n_trials=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm test results\n",
    "X_test = df_test.drop(['label', 'hid'], axis=1)\n",
    "y_test = df_test['label']\n",
    "\n",
    "y_pred_proba = []\n",
    "for idx in range(N_FOLD):\n",
    "    # Load the best model from saved file\n",
    "    best_model = xgb.XGBClassifier()\n",
    "    best_model.load_model(f\"model/{model_name}/best_model_fold_{idx}_{model_name}.ubj\")\n",
    "    # Make predictions on test set\n",
    "    y_pred_proba.append(best_model.predict_proba(X_test)[:,1])\n",
    "y_test_proba = np.mean(y_pred_proba, axis=0)\n",
    "\n",
    "y_test = y_test.ravel()  # Flatten to 1D array\n",
    "y_test_proba = y_test_proba.ravel()  # Flatten to 1D array\n",
    "\n",
    "##########################\n",
    "X_train = df_train.drop(['label', 'hid'], axis=1)\n",
    "y_train = df_train['label']\n",
    "\n",
    "y_pred_proba = []\n",
    "for idx in range(N_FOLD):\n",
    "    # Load the best model from saved file\n",
    "    best_model = xgb.XGBClassifier()\n",
    "    best_model.load_model(f\"model/{model_name}/best_model_fold_{idx}_{model_name}.ubj\")\n",
    "    # Make predictions on test set\n",
    "    y_pred_proba.append(best_model.predict_proba(X_train)[:,1])\n",
    "y_train_proba = np.mean(y_pred_proba, axis=0)\n",
    "\n",
    "y_train = y_train.ravel()  # Flatten to 1D array\n",
    "y_train_proba = y_train_proba.ravel()  # Flatten to 1D array\n",
    "###########################\n",
    "\n",
    "# Calculate calibration metrics before linear regression\n",
    "cal_model = LinearRegression()\n",
    "cal_model.fit(y_test_proba.reshape(-1,1), y_test)\n",
    "cal_intercept = cal_model.intercept_\n",
    "cal_slope = cal_model.coef_[0]\n",
    "brier_before = brier_score_loss(y_test, y_test_proba)\n",
    "\n",
    "U_before, p_before = calc_unreliability(y_test, y_test_proba)\n",
    "\n",
    "# Apply isotonic regression\n",
    "ir = IsotonicRegression(out_of_bounds='clip')\n",
    "ir.fit(y_train_proba, y_train)\n",
    "y_test_proba_cal = ir.predict(y_test_proba)\n",
    "\n",
    "# Save isotonic regression model\n",
    "with open(f\"isotonic_calibration.pkl\", 'wb') as f:\n",
    "    pickle.dump(ir, f)\n",
    "    \n",
    "#calib = mli.SplineCalib(unity_prior=False, unity_prior_weight=100,random_state=42, max_iter =500)\n",
    "#calib.fit(y_train_proba, y_train)\n",
    "#y_test_proba_cal = calib.calibrate(y_test_proba)\n",
    "\n",
    "# Ensure predictions are not exactly 0 or 1\n",
    "#y_test_proba_cal = np.clip(y_test_proba_cal, 1e-15, 1-1e-15)\n",
    "\n",
    "# Calculate calibration metrics after linear regression\n",
    "cal_model = LinearRegression()\n",
    "cal_model.fit(y_test_proba_cal.reshape(-1,1), y_test)\n",
    "cal_intercept_after = cal_model.intercept_\n",
    "cal_slope_after = cal_model.coef_[0]\n",
    "brier_after = brier_score_loss(y_test, y_test_proba_cal)\n",
    "\n",
    "# Calculate unreliability after calibration\n",
    "U_after, p_after = calc_unreliability(y_test, y_test_proba_cal)\n",
    "\n",
    "print(\"Before Calibration:\")\n",
    "print(f\"Calibration Intercept: {cal_intercept:.4f}\")\n",
    "print(f\"Calibration Slope: {cal_slope:.4f}\")\n",
    "print(f\"Brier Score: {brier_before:.4f}\")\n",
    "print(f\"Unreliability Index: {U_before:.4f}\")\n",
    "print(f\"Unreliability p-value: {p_before:.4f}\")\n",
    "\n",
    "print(\"\\nAfter Linear Regression:\")\n",
    "print(f\"Calibration Intercept: {cal_intercept_after:.4f}\")\n",
    "print(f\"Calibration Slope: {cal_slope_after:.4f}\")\n",
    "print(f\"Brier Score: {brier_after:.4f}\")\n",
    "print(f\"Unreliability Index: {U_after:.4f}\")\n",
    "print(f\"Unreliability p-value: {p_after:.4f}\")\n",
    "\n",
    "# Calculate and plot calibration curve\n",
    "prob_true, prob_pred = calibration_curve(y_test, y_test_proba_cal, n_bins=10)\n",
    "draw_calibration_plot(prob_true, prob_pred, y_test_proba_cal, 10, 1.0)\n",
    "########################\n",
    "\n",
    "Youden = youden(y_train, y_train_proba)\n",
    "print(f'Youden Index : {Youden:.4f}')\n",
    "\n",
    "AUROC = roc_auc_score(y_test, y_test_proba_cal)\n",
    "AUPRC = average_precision_score(y_test, y_test_proba_cal)\n",
    "y_pred_test = [1 if value > Youden else 0 for value in y_test_proba_cal]\n",
    "F1_score = f1_score(y_test, y_pred_test)\n",
    "\n",
    "print(f'AUROC : {AUROC:.4f}')\n",
    "print(f'AUROC CI(95%): ({roc_auc_ci(y_test, y_test_proba_cal)[0]:.4f},{roc_auc_ci(y_test, y_test_proba_cal)[1]:.4f})')\n",
    "print(f'AUPRC : {AUPRC:.4f}')\n",
    "print(f'F1 Score : {F1_score:.4f}')\n",
    "print(f'Test Accuracy : {round(100*accuracy_score(y_test, y_pred_test), 2)}% ')\n",
    "\n",
    "draw_roc_curve(y_test, y_test_proba_cal)\n",
    "draw_confusion_matrix(y_test, y_pred_test)\n",
    "draw_y_test_proba(y_test_proba_cal)\n",
    "\n",
    "# Save test results\n",
    "with open(f'model/{model_name}/y_test_{model_name}.pkl', 'wb') as f:\n",
    "    pickle.dump(y_test, f)\n",
    "with open(f'model/{model_name}/y_test_proba_{model_name}.pkl', 'wb') as f:\n",
    "    pickle.dump(y_test_proba_cal, f)\n",
    "\n",
    "# Get thresholds using Youden's index\n",
    "threshold_model = youden(y_test, y_test_proba_cal)\n",
    "\n",
    "# Calculate metrics for each model\n",
    "metrics_model = get_metrics(y_test, y_test_proba_cal, threshold_model)\n",
    "\n",
    "# Print results\n",
    "models = {\n",
    "    'Model': metrics_model,\n",
    "}\n",
    "\n",
    "for model_name, metrics in models.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"Threshold: {metrics['Threshold']:.4f}\")\n",
    "    print(f\"AUROC: {metrics['AUROC']:.4f} (95% CI: {metrics['AUROC CI'][0]:.4f}-{metrics['AUROC CI'][1]:.4f})\")\n",
    "    print(f\"AUPRC: {metrics['AUPRC']:.4f} (95% CI: {metrics['AUPRC CI'][0]:.4f}-{metrics['AUPRC CI'][1]:.4f})\")\n",
    "    print(f\"F1-Score: {metrics['F1-Score']:.4f} (95% CI: {metrics['F1-Score CI'][0]:.4f}-{metrics['F1-Score CI'][1]:.4f})\")\n",
    "    print(f\"Sensitivity: {metrics['Sensitivity']:.4f} (95% CI: {metrics['Sensitivity CI'][0]:.4f}-{metrics['Sensitivity CI'][1]:.4f})\")\n",
    "    print(f\"Specificity: {metrics['Specificity']:.4f} (95% CI: {metrics['Specificity CI'][0]:.4f}-{metrics['Specificity CI'][1]:.4f})\")\n",
    "    print(f\"Precision: {metrics['Precision']:.4f} (95% CI: {metrics['Precision CI'][0]:.4f}-{metrics['Precision CI'][1]:.4f})\")\n",
    "    print(f\"TN: {metrics['TN']}, FP: {metrics['FP']}, FN: {metrics['FN']}, TP: {metrics['TP']}\")\n",
    "    print(f\"Accuracy: {metrics['Accuracy']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_1 = 'xgb_demo_icd'\n",
    "model_name_2 = 'xgb'\n",
    "\n",
    "# Save test results\n",
    "with open(f'model/{model_name_1}/y_test_{model_name_1}.pkl', 'rb') as f:\n",
    "    y_test = pickle.load(f)\n",
    "with open(f'model/{model_name_1}/y_test_proba_{model_name_1}.pkl', 'rb') as f:\n",
    "    y_test_proba_1 = pickle.load(f)\n",
    "with open(f'model/{model_name_2}/y_test_proba_{model_name_2}.pkl', 'rb') as f:\n",
    "    y_test_proba_2 = pickle.load(f)\n",
    "\n",
    "#with open(f'{trial_dir}/y_test_proba_cnn.pkl', 'rb') as f:\n",
    "#    y_test_proba_2 = pickle.load(f)\n",
    "\n",
    "#Delong test p-value\n",
    "pvalue = delong_roc_test(y_test, y_test_proba_1, y_test_proba_2)\n",
    "print(f'p_value: {pvalue[0][0]:.9f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with RCRI and Troponin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "FILTER_MONTHS=3\n",
    "df_1 = pd.read_csv('opid_macce_orig_lab_asa.csv')\n",
    "df_2 = pd.read_csv('opid_macce_'+str(FILTER_MONTHS)+'mo_N.csv')\n",
    "df_3 = pd.read_csv('ecg_labels_w_features_lab_asa_'+str(FILTER_MONTHS)+'mo_N.csv')\n",
    "\n",
    "df_1= df_1.merge(df_2[['opid','hid']], on ='opid')\n",
    "df_1= df_1[pd.notnull(df_1['hid'])]\n",
    "df_1.drop(columns=['dept'], inplace=True)\n",
    "\n",
    "df_1.rename(columns={'age': 'real_PatientAge', 'sex': 'real_Gender'}, inplace=True)\n",
    "df_1.replace({'real_Gender': {'F': 0, 'M': 1}}, inplace=True)\n",
    "\n",
    "df_1['orin']= pd.to_datetime(df_1['opid'].astype(str).str.zfill(9).str[:2] + '/' + df_1['opid'].astype(str).str.zfill(9).str[2:4] + '/' + df_1['opid'].astype(str).str.zfill(9).str[4:6], format='%y/%m/%d')\n",
    "\n",
    "df_3['AcquisitionDate']= pd.to_datetime(df_3['AcquisitionDate'])\n",
    "for i in tqdm(df_3.index):\n",
    "    cond1 = (df_1['hid']==df_3.loc[i,'hid'])\n",
    "    cond2 = (df_1['orin']>=df_3.loc[i,'AcquisitionDate'])\n",
    "    cond3 = (df_1['orin']<=df_3.loc[i,'AcquisitionDate']+pd.DateOffset(months=FILTER_MONTHS))\n",
    "    if len(df_1[cond1 & cond2 & cond3])>0:\n",
    "        df_3.loc[i, 'opid'] = df_1[cond1&cond2&cond3].sort_values(by='orin').iloc[0]['opid']\n",
    "df_1.drop(columns=['hid'], inplace=True)\n",
    "\n",
    "df_3 = df_3.merge(df_1, on ='opid')\n",
    "#df_3.drop(columns=['PatientAge','Gender'], inplace=True)\n",
    "df_3.to_csv('all_data_labels_w_troponin.csv')\n",
    "\n",
    "df_4 = pd.read_csv('df_test_MACCE.csv')\n",
    "\n",
    "for i in tqdm(df_4.index):\n",
    "    matching_opids = df_3.loc[df_3['filename']==df_4.loc[i, 'filename'], 'opid']\n",
    "    if len(matching_opids) > 0:\n",
    "        df_4.loc[i, 'opid'] = matching_opids.values[0]\n",
    "    else:\n",
    "        print(df_4.loc[i, 'filename'])\n",
    "\n",
    "df_5= pd.read_csv('opid_rcri_orig.csv')\n",
    "\n",
    "for i in tqdm(df_4.index):\n",
    "    matching_rcri = df_5.loc[df_5['opid']==df_4.loc[i, 'opid'], 'rcri_score'].values\n",
    "    if len(matching_rcri) > 0:\n",
    "        df_4.loc[i, 'rcri'] = matching_rcri[0]/6\n",
    "\n",
    "df_6= pd.read_csv('opid_troponin_orig.csv')\n",
    "\n",
    "for i in tqdm(df_4.index):\n",
    "    matching_tni = df_6.loc[df_6['opid']==df_4.loc[i, 'opid'], 'pre_troponin_I']\n",
    "    if len(matching_tni) == 0:\n",
    "        continue\n",
    "    df_4.loc[i, 'TnI'] = matching_tni.values[0]\n",
    "\n",
    "df_4.to_csv('df_test_MACCE_w_rcri_troponin.csv', index=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df_3 = pd.read_csv('all_data_labels_w_troponin.csv')\n",
    "df_4 = pd.read_csv('df_MACCE.csv')\n",
    "\n",
    "for i in tqdm(df_4.index):\n",
    "    matching_opids = df_3.loc[df_3['filename']==df_4.loc[i, 'filename'], 'opid']\n",
    "    if len(matching_opids) > 0:\n",
    "        df_4.loc[i, 'opid'] = matching_opids.values[0]\n",
    "\n",
    "df_5= pd.read_csv('opid_rcri_orig.csv')\n",
    "\n",
    "for i in tqdm(df_4.index):\n",
    "    matching_rcri = df_5.loc[df_5['opid']==df_4.loc[i, 'opid'], 'rcri_score'].values\n",
    "    if len(matching_rcri) > 0:\n",
    "        df_4.loc[i, 'rcri'] = matching_rcri[0]/6\n",
    "\n",
    "df_6= pd.read_csv('opid_troponin_orig.csv')\n",
    "\n",
    "for i in tqdm(df_4.index):\n",
    "    matching_tni = df_6.loc[df_6['opid']==df_4.loc[i, 'opid'], 'pre_troponin_I']\n",
    "    if len(matching_tni) == 0:\n",
    "        continue\n",
    "    df_4.loc[i, 'TnI'] = matching_tni.values[0]\n",
    "    #df_4.loc[i, 'TnT'] = df_6.loc[df_6['opid']==df_4.loc[i, 'opid'], 'pre_troponin_T'].values[0]\n",
    "\n",
    "df_4.to_csv('df_MACCE_w_rcri_troponin.csv', index=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RCRI\n",
    "df_4 = pd.read_csv('df_test_MACCE_w_rcri_troponin.csv')\n",
    "# Remove rows with NaN values in 'label' or 'rcri' columns\n",
    "x= pd.read_csv('rcri_missing.csv')\n",
    "\n",
    "# Map rcri_score from x to df_4 based on matching opids\n",
    "for i in tqdm(df_4.index):\n",
    "    matching_rcri = x.loc[x['opid']==df_4.loc[i, 'opid'], 'rcri_score']\n",
    "    if len(matching_rcri) > 0:\n",
    "        df_4.loc[i, 'rcri'] = matching_rcri.values[0]/6\n",
    "\n",
    "model_name_1 = 'xgb_demo_icd'\n",
    "with open(f'model/{model_name_1}/y_test_proba_{model_name_1}.pkl', 'rb') as f:\n",
    "    y_test_proba_1 = pickle.load(f)\n",
    "df_4['proba']= y_test_proba_1\n",
    "\n",
    "model_name_2 = 'xgb_wo_cnn_demo_icd'\n",
    "with open(f'model/{model_name_2}/y_test_proba_{model_name_2}.pkl', 'rb') as f:\n",
    "    y_test_proba_2 = pickle.load(f)\n",
    "df_4['proba_wo_cnn']= y_test_proba_2\n",
    "\n",
    "'''\n",
    "test= dataset(header_files=df_4['filename'].to_list())\n",
    "test.num_leads = 12\n",
    "test.sample = False\n",
    "test.files.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Create test data loader\n",
    "test_loader = DataLoader(test, collate_fn=collate_fn, batch_size=BATCH_SIZE, shuffle=False)\n",
    "y_pred_test_list = []\n",
    "\n",
    "# [Main] 4-Fold Validation\n",
    "# Create datasets & data loaders\n",
    "for i_fold in range(N_FOLD):\n",
    "    # Initialize model & save weights\n",
    "    model = FinalModel(block_size =BLOCK_SIZE, block_depth =BLOCK_DEPTH, block_layers=BLOCK_LAYERS, hidden_size=HIDDEN_SIZE, kernel_num=KERNEL_NUM).to(DEVICE)\n",
    "    weight_cache = f'{TRIAL_DIR}/weights_fold_{i_fold+1}.pth'\n",
    "    # Load best weights & inference on test set\n",
    "    model.load_state_dict(torch.load(weight_cache, weights_only=True))\n",
    "    y_true_test, y_pred_test, hidden_states = evaluate_model(test_loader, model)\n",
    "    test_auroc = roc_auc_score(y_true_test, y_pred_test)\n",
    "    y_pred_test_list.append(y_pred_test)\n",
    "    print()\n",
    "\n",
    "y_pred_test = np.mean(np.array(y_pred_test_list), axis=0)\n",
    "y_test_proba_cnn = y_pred_test.ravel()  # Flatten to 1D array\n",
    "'''\n",
    "model_name_3 = 'xgb'\n",
    "with open(f'model/{model_name_3}/y_test_proba_{model_name_3}.pkl', 'rb') as f:\n",
    "    y_test_proba_cnn = pickle.load(f)\n",
    "\n",
    "y_test = df_4['label']\n",
    "y_test_proba_rcri = df_4['rcri']\n",
    "y_test_proba_model = df_4['proba']\n",
    "y_test_proba_model_wo_cnn = df_4['proba_wo_cnn']\n",
    "y_test_proba_asa = df_4['asa']\n",
    "\n",
    "# Calculate metrics for RCRI\n",
    "AUROC_rcri = roc_auc_score(y_test, y_test_proba_rcri)\n",
    "AUPRC_rcri = average_precision_score(y_test, y_test_proba_rcri)\n",
    "\n",
    "# Calculate metrics for model\n",
    "AUROC_model = roc_auc_score(y_test, y_test_proba_model)\n",
    "AUPRC_model = average_precision_score(y_test, y_test_proba_model)\n",
    "\n",
    "AUROC_cnn = roc_auc_score(y_test, y_test_proba_cnn)\n",
    "AUPRC_cnn = average_precision_score(y_test, y_test_proba_cnn)\n",
    "\n",
    "AUROC_model_wo_cnn = roc_auc_score(y_test, y_test_proba_model_wo_cnn)\n",
    "\n",
    "AUROC_asa = roc_auc_score(y_test, y_test_proba_asa)\n",
    "\n",
    "print(f'RCRI AUROC : {AUROC_rcri:.4f}')\n",
    "print(f'RCRI AUROC CI(95%): ({roc_auc_ci(y_test, y_test_proba_rcri)[0]:.4f},{roc_auc_ci(y_test, y_test_proba_rcri)[1]:.4f})')\n",
    "print(f'RCRI AUPRC : {AUPRC_rcri:.4f}')\n",
    "\n",
    "print(f'\\nModel AUROC : {AUROC_model:.4f}')\n",
    "print(f'Model AUROC CI(95%): ({roc_auc_ci(y_test, y_test_proba_model)[0]:.4f},{roc_auc_ci(y_test, y_test_proba_model)[1]:.4f})')\n",
    "print(f'Model AUPRC : {AUPRC_model:.4f}')\n",
    "\n",
    "# Plot ROC curves\n",
    "fpr_rcri, tpr_rcri, _ = roc_curve(y_test, y_test_proba_rcri)\n",
    "fpr_model, tpr_model, _ = roc_curve(y_test, y_test_proba_model)\n",
    "fpr_cnn, tpr_cnn, _ = roc_curve(y_test, y_test_proba_cnn)\n",
    "fpr_model_wo_cnn, tpr_model_wo_cnn, _ = roc_curve(y_test, y_test_proba_model_wo_cnn)\n",
    "fpr_asa, tpr_asa, _ = roc_curve(y_test, y_test_proba_asa)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_model, tpr_model, color='blue',\n",
    "         label=f'Multimodal GBM (AUC = {AUROC_model:.3f}, 95% CI: {roc_auc_ci(y_test, y_test_proba_model)[0]:.3f}-{roc_auc_ci(y_test, y_test_proba_model)[1]:.3f})')\n",
    "plt.plot(fpr_model_wo_cnn, tpr_model_wo_cnn, color='brown', \n",
    "         label=f'Baseline GBM (AUC = {AUROC_model_wo_cnn:.3f}, 95% CI: {roc_auc_ci(y_test, y_test_proba_model_wo_cnn)[0]:.3f}-{roc_auc_ci(y_test, y_test_proba_model_wo_cnn)[1]:.3f})')\n",
    "plt.plot(fpr_cnn, tpr_cnn, color='green', \n",
    "         label=f'GBM with only ECG (AUC = {AUROC_cnn:.3f}, 95% CI: {roc_auc_ci(y_test, y_test_proba_cnn)[0]:.3f}-{roc_auc_ci(y_test, y_test_proba_cnn)[1]:.3f})')\n",
    "plt.plot(fpr_rcri, tpr_rcri, color='orange', \n",
    "         label=f'RCRI (AUC = {AUROC_rcri:.3f}, 95% CI: {roc_auc_ci(y_test, y_test_proba_rcri)[0]:.3f}-{roc_auc_ci(y_test, y_test_proba_rcri)[1]:.3f})')\n",
    "plt.plot(fpr_asa, tpr_asa, color='red', \n",
    "         label=f'ASA (AUC = {AUROC_asa:.3f}, 95% CI: {roc_auc_ci(y_test, y_test_proba_asa)[0]:.3f}-{roc_auc_ci(y_test, y_test_proba_asa)[1]:.3f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "#plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "#Delong test p-value\n",
    "pvalue = delong_roc_test(df_4['label'], df_4['rcri'], df_4['proba'])\n",
    "print(f'\\nDelong p_value: {pvalue[0][0]:.9f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get thresholds using Youden's index\n",
    "threshold_model = youden(y_test, y_test_proba_model)\n",
    "threshold_cnn = youden(y_test, y_test_proba_cnn)\n",
    "threshold_model_wo_cnn = youden(y_test, y_test_proba_model_wo_cnn)\n",
    "threshold_rcri = youden(y_test, y_test_proba_rcri)\n",
    "threshold_asa = youden(y_test, y_test_proba_asa)\n",
    "\n",
    "# Calculate metrics for each model\n",
    "metrics_model = get_metrics(y_test, y_test_proba_model, threshold_model)\n",
    "metrics_cnn = get_metrics(y_test, y_test_proba_cnn, threshold_cnn)\n",
    "metrics_model_wo_cnn = get_metrics(y_test, y_test_proba_model_wo_cnn, threshold_model_wo_cnn)\n",
    "metrics_rcri = get_metrics(y_test, y_test_proba_rcri, threshold_rcri)\n",
    "metrics_asa = get_metrics(y_test, y_test_proba_asa, threshold_asa)\n",
    "\n",
    "# Print results\n",
    "models = {\n",
    "    'Baseline GBM': metrics_model,\n",
    "    'DNN': metrics_cnn,\n",
    "    'GBM without DNN features': metrics_model_wo_cnn,\n",
    "    'RCRI': metrics_rcri,\n",
    "    'ASA': metrics_asa\n",
    "}\n",
    "\n",
    "for model_name, metrics in models.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"Threshold: {metrics['Threshold']:.4f}\")\n",
    "    print(f\"AUROC: {metrics['AUROC']:.4f} (95% CI: {metrics['AUROC CI'][0]:.4f}-{metrics['AUROC CI'][1]:.4f})\")\n",
    "    print(f\"AUPRC: {metrics['AUPRC']:.4f} (95% CI: {metrics['AUPRC CI'][0]:.4f}-{metrics['AUPRC CI'][1]:.4f})\")\n",
    "    print(f\"F1-Score: {metrics['F1-Score']:.4f} (95% CI: {metrics['F1-Score CI'][0]:.4f}-{metrics['F1-Score CI'][1]:.4f})\")\n",
    "    print(f\"Sensitivity: {metrics['Sensitivity']:.4f} (95% CI: {metrics['Sensitivity CI'][0]:.4f}-{metrics['Sensitivity CI'][1]:.4f})\")\n",
    "    print(f\"Specificity: {metrics['Specificity']:.4f} (95% CI: {metrics['Specificity CI'][0]:.4f}-{metrics['Specificity CI'][1]:.4f})\")\n",
    "    print(f\"Precision: {metrics['Precision']:.4f} (95% CI: {metrics['Precision CI'][0]:.4f}-{metrics['Precision CI'][1]:.4f})\")\n",
    "    print(f\"TN: {metrics['TN']}, FP: {metrics['FP']}, FN: {metrics['FN']}, TP: {metrics['TP']}\")\n",
    "    print(f\"Accuracy: {metrics['Accuracy']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for each model\n",
    "#y_test_proba_model\n",
    "#y_test_proba_cnn\n",
    "#y_test_proba_model_wo_cnn\n",
    "#y_test_proba_rcri\n",
    "#y_test_proba_asa\n",
    "\n",
    "#Delong test p-value\n",
    "pvalue = delong_roc_test(y_test, y_test_proba_model, y_test_proba_model_wo_cnn)\n",
    "print(f'p_value: {pvalue[0][0]:.9f}')\n",
    "\n",
    "pvalue = delong_roc_test(y_test, y_test_proba_model, y_test_proba_cnn)\n",
    "print(f'p_value: {pvalue[0][0]:.9f}')\n",
    "\n",
    "pvalue = delong_roc_test(y_test, y_test_proba_model, y_test_proba_rcri)\n",
    "print(f'p_value: {pvalue[0][0]:.9f}')\n",
    "\n",
    "pvalue = delong_roc_test(y_test, y_test_proba_model, y_test_proba_asa)\n",
    "print(f'p_value: {pvalue[0][0]:.9f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply calibration to each model's predictions\n",
    "calib = mli.SplineCalib(unity_prior=False, unity_prior_weight=100, random_state=42, max_iter=500)\n",
    "#calib = IsotonicRegression(out_of_bounds='clip')\n",
    "\n",
    "# Dictionary to store calibrated probabilities\n",
    "calibrated_probas = {}\n",
    "models_data = {\n",
    "    'Baseline GBM': y_test_proba_model,\n",
    "    'DNN': y_test_proba_cnn, \n",
    "    'GBM without DNN features': y_test_proba_model_wo_cnn,\n",
    "    'RCRI': y_test_proba_rcri,\n",
    "    'ASA': y_test_proba_asa\n",
    "}\n",
    "\n",
    "print(\"\\nCalibration Metrics:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for model_name, y_proba in models_data.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    \n",
    "    # Calculate metrics before calibration\n",
    "    cal_model = LinearRegression()\n",
    "    cal_model.fit(y_proba.reshape(-1,1), y_test)\n",
    "    cal_intercept = cal_model.intercept_\n",
    "    cal_slope = cal_model.coef_[0]\n",
    "    brier_before = brier_score_loss(y_test, y_proba)\n",
    "    U_before, p_before = calc_unreliability(y_test, y_proba)\n",
    "    \n",
    "    # Apply calibration\n",
    "    calib.fit(y_proba, y_test)\n",
    "    y_proba_cal = calib.calibrate(y_proba)\n",
    "    #y_proba_cal = calib.predict(y_proba)\n",
    "    y_proba_cal = np.clip(y_proba_cal, 1e-15, 1-1e-15)\n",
    "    calibrated_probas[model_name] = y_proba_cal\n",
    "    \n",
    "    # Calculate metrics after calibration\n",
    "    cal_model = LinearRegression()\n",
    "    cal_model.fit(y_proba_cal.reshape(-1,1), y_test)\n",
    "    cal_intercept_after = cal_model.intercept_\n",
    "    cal_slope_after = cal_model.coef_[0]\n",
    "    brier_after = brier_score_loss(y_test, y_proba_cal)\n",
    "    U_after, p_after = calc_unreliability(y_test, y_proba_cal)\n",
    "    \n",
    "    print(\"Before Calibration:\")\n",
    "    print(f\"Calibration Intercept: {cal_intercept:.4f}\")\n",
    "    print(f\"Calibration Slope: {cal_slope:.4f}\")\n",
    "    print(f\"Brier Score: {brier_before:.4f}\")\n",
    "    print(f\"Unreliability Index: {U_before:.4f}\")\n",
    "    print(f\"Unreliability p-value: {p_before:.4f}\")\n",
    "    \n",
    "    print(\"\\nAfter Calibration:\")\n",
    "    print(f\"Calibration Intercept: {cal_intercept_after:.4f}\")\n",
    "    print(f\"Calibration Slope: {cal_slope_after:.4f}\")\n",
    "    print(f\"Brier Score: {brier_after:.4f}\")\n",
    "    print(f\"Unreliability Index: {U_after:.4f}\")\n",
    "    print(f\"Unreliability p-value: {p_after:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TnI\n",
    "FILTER_MONTHS=3\n",
    "df_1 = pd.read_csv('opid_macce_orig_lab_asa.csv')\n",
    "df_2 = pd.read_csv('opid_macce_'+str(FILTER_MONTHS)+'mo.csv')\n",
    "df_3 = pd.read_csv('ecg_labels_w_features_'+str(FILTER_MONTHS)+'mo.csv')\n",
    "\n",
    "df_1= df_1.merge(df_2[['opid','hid']], on ='opid')\n",
    "df_1= df_1[pd.notnull(df_1['hid'])]\n",
    "df_1.drop(columns=['dept'], inplace=True)\n",
    "\n",
    "df_1.rename(columns={'age': 'real_PatientAge', 'sex': 'real_Gender'}, inplace=True)\n",
    "df_1.replace({'real_Gender': {'F': 0, 'M': 1}}, inplace=True)\n",
    "\n",
    "df_1['orin']= pd.to_datetime(df_1['opid'].astype(str).str.zfill(9).str[:2] + '/' + df_1['opid'].astype(str).str.zfill(9).str[2:4] + '/' + df_1['opid'].astype(str).str.zfill(9).str[4:6], format='%y/%m/%d')\n",
    "\n",
    "df_3['AcquisitionDate']= pd.to_datetime(df_3['AcquisitionDate'])\n",
    "for i in tqdm(df_3.index):\n",
    "    cond1 = (df_1['hid']==df_3.loc[i,'hid'])\n",
    "    cond2 = (df_1['orin']>df_3.loc[i,'AcquisitionDate'])\n",
    "    cond3 = (df_1['orin']<df_3.loc[i,'AcquisitionDate']+pd.DateOffset(months=FILTER_MONTHS))\n",
    "    if len(df_1[cond1 & cond2 & cond3])>0:\n",
    "        df_3.loc[i, 'opid'] = df_1[cond1&cond2&cond3].sort_values(by='orin').iloc[0]['opid']\n",
    "df_1.drop(columns=['hid'], inplace=True)\n",
    "\n",
    "df_3 = df_3.merge(df_1, on ='opid')\n",
    "df_3.drop(columns=['PatientAge','Gender'], inplace=True)\n",
    "\n",
    "df_6= pd.read_csv('opid_troponin_orig.csv')\n",
    "\n",
    "for i in tqdm(df_3.index):\n",
    "    df_3.loc[i, 'TnI'] = df_6.loc[df_6['opid']==df_3.loc[i, 'opid'], 'pre_troponin_I'].values[0]\n",
    "    df_3.loc[i, 'TnT'] = df_6.loc[df_6['opid']==df_3.loc[i, 'opid'], 'pre_troponin_T'].values[0]\n",
    "\n",
    "df_3= df_3[~df_3['TnI'].isna()]\n",
    "\n",
    "df_3.to_csv('all_data_labels_w_troponin.csv')\n",
    "\n",
    "y_test = df_3['label']\n",
    "y_test_proba = df_3['TnI']\n",
    "\n",
    "AUROC = roc_auc_score(y_test, y_test_proba)\n",
    "AUPRC = average_precision_score(y_test, y_test_proba)\n",
    "\n",
    "print(f'AUROC : {AUROC:.4f}')\n",
    "print(f'AUROC CI(95%): ({roc_auc_ci(y_test, y_test_proba)[0]:.4f},{roc_auc_ci(y_test, y_test_proba)[1]:.4f})')\n",
    "print(f'AUPRC : {AUPRC:.4f}')\n",
    "\n",
    "draw_roc_curve(y_test, y_test_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No TnI vs TnI GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('df_MACCE_w_rcri_troponin.csv')\n",
    "df = df[~df['TnI'].isna()]\n",
    "with open('exclude_files.pkl', 'rb') as f:\n",
    "    exclude_files = pickle.load(f)  \n",
    "df = df[~df['filename'].isin(exclude_files)]\n",
    "df2= pd.read_csv('data_labels_gbm_hidden_icd.csv')\n",
    "df2 = pd.merge(df2, df[['filename', 'TnI']], on='filename', how='left')\n",
    "df = df2[~df2['TnI'].isna()]\n",
    "\n",
    "df_catcol = ['gender','p','o','a'] #'gender','p','o','a'\n",
    "df_numcol = ['age','TnI'] #'age', 'TnI'\n",
    "for i_fold in range(N_FOLD):\n",
    "    df_numcol.extend([f'hidden_{i_fold+1}_{i}' for i in range(HIDDEN_SIZE)])\n",
    "labels = 'label'\n",
    "model_name = 'xgb_tni_subset_demo_icd_tni' #'xgb_tni_subset_demo_icd_tni', 'xgb_demo_icd'\n",
    "\n",
    "df[df_catcol] = df[df_catcol].astype('category')\n",
    "# Filter dataframe to only include specified columns\n",
    "df = df[df_catcol + df_numcol +['label','hid']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Get unique hospital IDs and shuffle them\n",
    "unique_hids = df['hid'].unique()\n",
    "np.random.shuffle(unique_hids)\n",
    "\n",
    "# Calculate split index for 90-10 split\n",
    "split_idx = int(len(unique_hids) * 0.8)\n",
    "\n",
    "# Split hospital IDs into train/val and test sets\n",
    "tval_hids = unique_hids[:split_idx]\n",
    "test_hids = unique_hids[split_idx:]\n",
    "\n",
    "# Save hospital ID splits for TnI subset\n",
    "hid_tni_subset_splits = {\n",
    "    'tval_hids': tval_hids,\n",
    "    'test_hids': test_hids\n",
    "}\n",
    "\n",
    "with open('hid_tni_subset_splits.pkl', 'wb') as f:\n",
    "    pickle.dump(hid_tni_subset_splits, f)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "with open(f'hid_splits.pkl', 'rb') as f:\n",
    "    hid_splits= pickle.load(f)\n",
    "df_train = df[df['hid'].isin(hid_splits['tval_hids'])]\n",
    "df_test = df[df['hid'].isin(hid_splits['test_hids'])]\n",
    "print(len(df_train), len(df_test))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hid_tni_subset_splits.pkl', 'rb') as f:\n",
    "    hid_tni_subset_splits = pickle.load(f)\n",
    "\n",
    "tval_hids = hid_tni_subset_splits['tval_hids']\n",
    "test_hids = hid_tni_subset_splits['test_hids']\n",
    "\n",
    "# Create dataframes based on hospital ID splits\n",
    "df_train = df[df['hid'].isin(tval_hids)]\n",
    "df_test = df[df['hid'].isin(test_hids)]\n",
    "\n",
    "print(len(df_train), len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model directory if it doesn't exist\n",
    "if not os.path.exists(f'model/{model_name}'):\n",
    "    os.makedirs(f'model/{model_name}')\n",
    "study = optuna.create_study(direction=\"maximize\", study_name=\"XGB Classifier\")\n",
    "func = lambda trial: objective(trial, df_train, df_test, model_name)\n",
    "#func = lambda trial: objective(trial, pd.DataFrame(X_train, columns=feature_names), pd.DataFrame(y_train, columns=['label']), pd.DataFrame(h_train, columns=['hid']))\n",
    "study.optimize(func, n_trials=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm test results\n",
    "X_test = df_test.drop(['label', 'hid', 'TnI'], axis=1)\n",
    "X_train = df_train.drop(['label', 'hid','TnI'], axis=1)\n",
    "\n",
    "model_name= 'xgb_tni_subset_demo_icd'\n",
    "##########################\n",
    "y_test = df_test['label']\n",
    "y_pred_proba = []\n",
    "for idx in range(N_FOLD):\n",
    "    # Load the best model from saved file\n",
    "    best_model = xgb.XGBClassifier()\n",
    "    best_model.load_model(f\"model/{model_name}/best_model_fold_{idx}_{model_name}.ubj\")\n",
    "    # Make predictions on test set\n",
    "    y_pred_proba.append(best_model.predict_proba(X_test)[:,1])\n",
    "y_test_proba = np.mean(y_pred_proba, axis=0)\n",
    "\n",
    "y_test = y_test.ravel()  # Flatten to 1D array\n",
    "y_test_proba = y_test_proba.ravel()  # Flatten to 1D array\n",
    "########################\n",
    "y_train = df_train['label']\n",
    "y_pred_proba = []\n",
    "for idx in range(N_FOLD):\n",
    "    # Load the best model from saved file\n",
    "    best_model = xgb.XGBClassifier()\n",
    "    best_model.load_model(f\"model/{model_name}/best_model_fold_{idx}_{model_name}.ubj\")\n",
    "    # Make predictions on test set\n",
    "    y_pred_proba.append(best_model.predict_proba(X_train)[:,1])\n",
    "y_train_proba = np.mean(y_pred_proba, axis=0)\n",
    "\n",
    "y_train = y_train.ravel()  # Flatten to 1D array\n",
    "y_train_proba = y_train_proba.ravel()  # Flatten to 1D array\n",
    "#############################\n",
    "\n",
    "ir = IsotonicRegression(out_of_bounds='clip')\n",
    "ir.fit(y_train_proba, y_train)\n",
    "y_test_proba = ir.predict(y_test_proba)\n",
    "\n",
    "# Calculate and plot calibration curve\n",
    "#prob_true, prob_pred = calibration_curve(y_test, y_test_proba , n_bins=10)\n",
    "#draw_calibration_plot(prob_true, prob_pred, y_test_proba, 10, 1.0)\n",
    "\n",
    "Youden = youden(y_train, y_train_proba)\n",
    "'''\n",
    "with open(f'model/{model_name}/youden_{model_name}.pkl', 'wb') as f:\n",
    "    pickle.dump(Youden, f)\n",
    "'''\n",
    "print(f'Youden Index : {Youden:.4f}')\n",
    "\n",
    "AUROC = roc_auc_score(y_test, y_test_proba)\n",
    "AUPRC = average_precision_score(y_test, y_test_proba)\n",
    "y_pred_test = [1 if value > Youden else 0 for value in y_test_proba]\n",
    "F1_score = f1_score(y_test, y_pred_test)\n",
    "\n",
    "print(f'AUROC : {AUROC:.4f}')\n",
    "print(f'AUROC CI(95%): ({roc_auc_ci(y_test, y_test_proba)[0]:.4f},{roc_auc_ci(y_test, y_test_proba)[1]:.4f})')\n",
    "print(f'AUPRC : {AUPRC:.4f}')\n",
    "print(f'F1 Score : {F1_score:.4f}')\n",
    "print(f'Test Accuracy : {round(100*accuracy_score(y_test, y_pred_test), 2)}% ')\n",
    "\n",
    "draw_roc_curve(y_test, y_test_proba)\n",
    "draw_confusion_matrix(y_test, y_pred_test)\n",
    "draw_y_test_proba(y_test_proba)\n",
    "\n",
    "'''\n",
    "# Save test results\n",
    "with open(f'model/{model_name}/y_test_{model_name}_tni.pkl', 'wb') as f:\n",
    "    pickle.dump(y_test, f)\n",
    "with open(f'model/{model_name}/y_test_proba_{model_name}_tni.pkl', 'wb') as f:\n",
    "    pickle.dump(y_test_proba, f)\n",
    "'''\n",
    "\n",
    "X_test_2 = df_test.drop(['label', 'hid'], axis=1)\n",
    "X_train_2 = df_train.drop(['label', 'hid'], axis=1)\n",
    "model_name_2= 'xgb_tni_subset_demo_icd_tni'\n",
    "##########################\n",
    "y_test = df_test['label']\n",
    "y_pred_proba_2 = []\n",
    "for idx in range(N_FOLD):\n",
    "    # Load the best model from saved file\n",
    "    best_model_2 = xgb.XGBClassifier()\n",
    "    best_model_2.load_model(f\"model/{model_name_2}/best_model_fold_{idx}_{model_name_2}.ubj\")\n",
    "    # Make predictions on test set\n",
    "    y_pred_proba_2.append(best_model_2.predict_proba(X_test_2)[:,1])\n",
    "y_test_proba_2 = np.mean(y_pred_proba_2, axis=0)\n",
    "\n",
    "y_test = y_test.ravel()  # Flatten to 1D array\n",
    "y_test_proba_2 = y_test_proba_2.ravel()  # Flatten to 1D array\n",
    "########################\n",
    "y_train = df_train['label']\n",
    "y_pred_proba_2 = []\n",
    "for idx in range(N_FOLD):\n",
    "    # Load the best model from saved file\n",
    "    best_model_2 = xgb.XGBClassifier()\n",
    "    best_model_2.load_model(f\"model/{model_name_2}/best_model_fold_{idx}_{model_name_2}.ubj\")\n",
    "    # Make predictions on test set\n",
    "    y_pred_proba_2.append(best_model_2.predict_proba(X_train_2)[:,1])\n",
    "y_train_proba_2 = np.mean(y_pred_proba_2, axis=0)\n",
    "\n",
    "y_train = y_train.ravel()  # Flatten to 1D array\n",
    "y_train_proba_2 = y_train_proba_2.ravel()  # Flatten to 1D array\n",
    "#############################\n",
    "ir = IsotonicRegression(out_of_bounds='clip')\n",
    "ir.fit(y_train_proba_2, y_train)\n",
    "y_test_proba_2 = ir.predict(y_test_proba_2)\n",
    "\n",
    "# Calculate ROC curve for both models\n",
    "fpr_model, tpr_model, _ = roc_curve(y_test, y_test_proba)\n",
    "fpr_model_2, tpr_model_2, _ = roc_curve(y_test, y_test_proba_2)\n",
    "fpr_tni, tpr_tni, _ = roc_curve(y_test, df_test['TnI'])\n",
    "\n",
    "# Calculate AUROCs\n",
    "AUROC_model = roc_auc_score(y_test, y_test_proba)\n",
    "AUROC_model_2 = roc_auc_score(y_test, y_test_proba_2)\n",
    "AUROC_tni = roc_auc_score(y_test, df_test['TnI'])\n",
    "\n",
    "# Plot ROC curves\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_model, tpr_model, color='blue',\n",
    "         label=f'Multimodal GBM (AUC = {AUROC_model:.3f}, 95% CI: {roc_auc_ci(y_test, y_test_proba)[0]:.3f}-{roc_auc_ci(y_test, y_test_proba)[1]:.3f})')\n",
    "#plt.plot(fpr_model_2, tpr_model_2, color='green',\n",
    "#         label=f'TnI-GBM (AUC = {AUROC_model_2:.3f}, 95% CI: {roc_auc_ci(y_test, y_test_proba_2)[0]:.3f}-{roc_auc_ci(y_test, y_test_proba_2)[1]:.3f})')\n",
    "plt.plot(fpr_tni, tpr_tni, color='orange', \n",
    "         label=f'TnI (AUC = {AUROC_tni:.3f}, 95% CI: {roc_auc_ci(y_test, df_test[\"TnI\"])[0]:.3f}-{roc_auc_ci(y_test, df_test[\"TnI\"])[1]:.3f})')\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "#plt.title('ROC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "#Delong test p-value\n",
    "pvalue = delong_roc_test(y_test, df_test[\"TnI\"], y_test_proba)\n",
    "print(f'\\nDelong p_value: {pvalue[0][0]:.9f}')\n",
    "pvalue = delong_roc_test(y_test, df_test[\"TnI\"], y_test_proba_2)\n",
    "print(f'\\nDelong p_value: {pvalue[0][0]:.9f}')\n",
    "pvalue = delong_roc_test(y_test, y_test_proba, y_test_proba_2)\n",
    "print(f'\\nDelong p_value: {pvalue[0][0]:.9f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_1 = 'xgb_demo_icd'\n",
    "model_name_2 = 'xgb_tni_subset_demo_icd_tni'\n",
    "\n",
    "# Save test results\n",
    "with open(f'model/{model_name_1}/y_test_{model_name_1}_tni.pkl', 'rb') as f:\n",
    "    y_test = pickle.load(f)\n",
    "with open(f'model/{model_name_1}/y_test_proba_{model_name_1}_tni.pkl', 'rb') as f:\n",
    "    y_test_proba_1 = pickle.load(f)\n",
    "with open(f'model/{model_name_2}/y_test_proba_{model_name_2}_tni.pkl', 'rb') as f:\n",
    "    y_test_proba_2 = pickle.load(f)\n",
    "\n",
    "#with open(f'{trial_dir}/y_test_proba_cnn.pkl', 'rb') as f:\n",
    "#    y_test_proba_2 = pickle.load(f)\n",
    "\n",
    "#Delong test p-value\n",
    "pvalue = delong_roc_test(y_test, y_test_proba_1, y_test_proba_2)\n",
    "print(f'p_value: {pvalue[0][0]:.9f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "240305",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
